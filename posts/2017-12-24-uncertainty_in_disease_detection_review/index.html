<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nick Strayer">
<meta name="dcterms.date" content="2017-12-24">
<meta name="description" content="I was recently sent this fantastic paper on using uncertainty in deep neural networks. In it the authors demonstrate a practical use of approximate bayesian inference by dropout in the context of massively complicated computer vision models for diagnosing disease. The paper, while well written, is very long. Here I summarize it into its main points and comment on their impactfulness.">

<title>Leveraging uncertainty information from deep neural networks for disease detection - a summary</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/4.9.1/d3.min.js"></script>

<style>
  #custom-header {
    height: 30vh;
    width: 100%;
    position:relative;
    z-index:1;
    margin:0 auto;
  }
</style>


</head><body class="nav-fixed fullcontent"><header id="custom-header">
  <div id="header_viz"></div>
</header>

<script>
  $( document ).ready(() => {

    //javascript for intro loader

    //Function to immitate R's seq
    function seq(start, end, increment) {
      var length = Math.round((end - start)/increment);
      return Array(length).fill().map((_, idx) => start + idx*increment)
    }


    //define the pdf of the distribution.
    var logistic = function(x, theta, i) {
        var mu = 0.1;
        sign = 1
        var y = sign * (1 / (Math.sqrt(2 * Math.PI) * theta)) * (1 / x) *
            Math.exp(-Math.pow((Math.log(x) - mu), 2) / (2 * Math.pow(theta, 2)))
        return y;
    }

    var width   = $("#custom-header").width() ,
        height  = $("#custom-header").outerHeight(),
        padding = 0,
        numOfLines = 20,
        xs = seq(0.01, 5, .01),
        colors = ['rgb(165,0,38)', 'rgb(215,48,39)', 'rgb(244,109,67)', 'rgb(253,174,97)', 'rgb(254,224,144)',
            'rgb(224,243,248)', 'rgb(171,217,233)', 'rgb(116,173,209)', 'rgb(69,117,180)', 'rgb(49,54,149)'
        ];
  
    console.log("trying to run this shit.")
    //define the svg.
    var svg = d3.select("#header_viz").append("svg")
        .attr("width", width)
        .attr("height", height + 2 * padding)
        .append("g")


    var animatelines = function(whichline) {
        d3.selectAll(".line").style("opacity","0.5");

        //Select All of the lines and process them one by one
        d3.selectAll(".line").each(function(d,i){
            // Get the length of each line in turn
            var totalLength = d3.select("#line" + i).node().getTotalLength();

            d3.selectAll("#line" + i).attr("stroke-dasharray", totalLength + " " + totalLength)
              .attr("stroke-dashoffset", totalLength)
              .transition()
              .duration(5000)
              .delay(100*i)
              .attr("stroke-dashoffset", 0)
              .style("stroke-width",2)
        })

        writeGreeting()
    }



    // The Scales:
    var thetaMap = d3.scaleLinear() //name the values from 0 to 20 and make their values from .1-.7
        .domain([0, numOfLines])
        .range([0.8, 0.085])

    var yPos = d3.scaleLinear() //scalling for creating horizontal lines
        .domain([0, numOfLines])
        .range([0, 4])

    var x = d3.scaleLinear()
        .domain([0, 5])
        .range([0, width]);

    var y = d3.scaleLinear()
        .domain([0, 4.5])
        .range([height, 0]);

    // The line functions:
    var logistic = d3.range(numOfLines).map(function(i) {
        var odd = true
        var toReturn = xs.map(function(num) {
            return {
                "x": num,
                "y": logistic(num, thetaMap(i), i)
            }
        })
        return toReturn;
    })

    //make a greeting message for after the line animation.
    function writeGreeting(){

        var text_scale = width < 500 ? 1 : 0.65;
        console.log(text_scale);
        var intro_text = "Live Free or Dichotomize";

        var title = svg.append("text")
            .attr("font-size", 30)
            .attr("font-family", "optima")
            .attr("text-anchor", "end")
            .attr("fill-opacity", 0.65)
            .attr("x", width < 500 ? x(4.8) : x(4.2))
            .attr("y", width < 500 ? y(3.12) : y(2.1) )
            .attr("opacity", 0)
            .text(intro_text)
            .attr("font-size", function(d) { return ( text_scale*width - 8) / this.getComputedTextLength() * 24 + "px"; })
            .text("")
            .attr("opacity", 1);

        (function drawGreeting (i, start, greeting) {
            setTimeout(function () {

                //add next letter to the greeting in progress
                start += greeting[i];

                title.html(start) //append this to the html

                if (start.length < greeting.length) { //if the in progress greeting is less than the full, keep going.
                    drawGreeting(i+1,start,greeting);      //  increment i and call again.
                };
            }, 100)
        })(0, "", intro_text);
    }


    // The d3 stuff
    var line = d3.line()
        // .interpolate("basis")
        .x(function(d) { return x(d.x); })
        .y(function(d) { return y(d.y); });


    svg.selectAll(".line")
        .data(logistic)
        .enter().append("path")
        .attr("class", "line")
        .attr("id" , function(d, i){ return "line" + i;})
        .attr("d", line)
        .style("stroke-width", 2)
        .style("stroke", function(d, i) { return colors[i % 10] })
        .style("opacity", 0)
        .style("fill", "none")
        .on("mouseover", function(d){
            d3.select(this).style("stroke", "black")
        })
        .on("mouseout", function(d,i){
            d3.select(this).style("stroke", colors[i % 10])
        })

    // var introMessage = isMobile ? "tap" : "click"
    // var introMessage = "click"
    //
    // var intro = svg.append("text")
    //     .text(introMessage)
    //     .attr("font-size", 35)
    //     .attr("font-family", "optima")
    //     .attr("text-anchor", "middle")
    //     .attr("x", x(2.5))
    //     .attr("y", y(2.01))

    //kick it off on a click. (or tap)
    animatelines(2)
    // d3.select("svg").on("click", function() { animatelines(2) })

})

</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>





<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Leveraging uncertainty information from deep neural networks for disease detection - a summary</h1>
                  <div>
        <div class="description">
          I was recently sent this fantastic paper on using uncertainty in deep neural networks. In it the authors demonstrate a practical use of approximate bayesian inference by dropout in the context of massively complicated computer vision models for diagnosing disease. The paper, while well written, is very long. Here I summarize it into its main points and comment on their impactfulness.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">deep learning</div>
                <div class="quarto-category">algorithms</div>
                <div class="quarto-category">uncertainty</div>
                <div class="quarto-category">bayesian</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Nick Strayer </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 24, 2017</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>As a biostatistician in the deep learning world I have the awkward task of balancing the dogma of statistics (<em>everything is uncertain</em>) along with the alluring success of some of the newest crazy complex neural network architectures. Going onto any Kaggle competition or new paper <label for="tufte-mn-" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-" class="margin-toggle"><span class="marginnote">Such as the popular but <a href="https://lukeoakdenrayner.wordpress.com/2017/11/18/quick-thoughts-on-chestxray14-performance-claims-and-clinical-tasks/">arguably flawed</a> paper on <a href="https://arxiv.org/abs/1711.05225">diagnosing from radiological screens</a> from Andrew Ng et al.</span> you will see models with millions of parameters performing seemingly magical tasks on data of all kinds.</p>
<p>These models however, almost never report uncertainty in their predictions. Why would they? Besides the fact that in the “just predict as many correct as possible” world of Kaggle uncertainty doesn’t win you money, turning their model into a Bayesian one by putting priors on their weights takes the already computationally intensive task of training a neural net and makes it even more burdensome.</p>
<p>The paper <a href="https://www.nature.com/articles/s41598-017-17876-z">Leveraging uncertainty information from deep neural networks for disease detection</a> does two very powerful things. First it shows that a recent method of getting uncertainty in neural networks by exploiting the regularization technique of dropout works well in the context of complex disease diagnosis models, and second it shows the value of this uncertainty knowledge in a biomedical context in number of clever ways.</p>
<section id="context" class="level3">
<h3 class="anchored" data-anchor-id="context">Context</h3>
<p><label for="tufte-mn-" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-" class="margin-toggle"><span class="marginnote">If you understand dropout and have a basic understanding of the bayes by dropout concept, you can skip this and go straight to <strong>Main Point</strong></span> The new(ish) paper, <a href="https://arxiv.org/abs/1506.02142">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</a>, shows that you can approximate a Bayesian posterior in an arbitrary neural network by training with dropout, and then leaving that dropout on at test time.</p>
<p><strong>Dropout</strong></p>
<p>Neural networks trained with dropout will randomly drop connections in a neural network at training time with probability <span class="math inline">\(\alpha\)</span> in an effort to force the model to learn robust prediction pathways (i.e.&nbsp;never learning to rely on a single feature that could make its predictions fragile). Traditionally after the model is trained, all the connections are left in the model but the weights are multiplied by (<span class="math inline">\(1 - \alpha\)</span>) to account for the fact that the model had less connections when the weights were learned. This technique has proven extremely good at fighting overfitting.</p>
<p><strong>Getting a posterior from dropout</strong></p>
<p>The bayes-by-dropout paper points out that if you simply leave the dropout on at test time and sample a ton of predictions from your model, <label for="tufte-mn-" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-" class="margin-toggle"><span class="marginnote">The original paper explains this in much better words and also shows that, while this relationship is intuitively appealing, it really is mathematically true. It’s great work.</span> you are effectively sampling from a distribution of different models. Thus the results from that sampling reflect an approximate posterior of your model’s predictions.</p>
<p><img src="../../media/uncertainty_paper/dropout_distributions.png" class="img-fluid"> Example output of 100 passes through a convolutional network trained to recognize MNIST digits. You can see the model’s uncertainty by the scatter of a given classification over the passes. Taken from <a href="https://arxiv.org/pdf/1506.02142.pdf">the bayes-by-droput paper</a>.</p>
<p>This is great because it lowers the boundary to Bayesian neural networks by an immense amount. You don’t need to break out a fancy probabilistic programming language and setup all your priors, you can simply train with dropout and leave it on. In addition, you can leverage the substantial prior information stored in previously trained models via transfer learning.</p>
</section>
<section id="main-point" class="level2">
<h2 class="anchored" data-anchor-id="main-point">Main Point</h2>
<p>By using (approximately) Bayesian methods to ascertain prediction uncertainty in deep neural networks (but really any model) we can more intelligently decide when to trust the prediction or to send the observation to a more definitive look. In the context of biomedical applications this means we can know when to trust the model and when to get a doctor to look at the data manually.</p>
<p><strong>Side Points</strong></p>
<p>The paper also dives in bit into the use of Bayesian uncertainty to try and diagnose data from an unobserved input distribution of output class. It comes to the conclusion that the Bayesian approach can not separate the causes of uncertainty and thus it doesn’t work well for this task. They do, however, show that you can fit an autoencoder on top of the trained layers and do decently well. This is interesting but not what I consider the main or most impactful parts of the paper.</p>
</section>
<section id="what-they-did" class="level2">
<h2 class="anchored" data-anchor-id="what-they-did">What they did</h2>
<p>Took standard convolutional architectures for image prediction and trained them with dropout to predict the presence of the disease <a href="http://wjscottmd.com/diabetic-retinopathy/">Diabetic Retinopathy</a>. At testing time they kept the dropout on and assembled a monte-carlo approximation of the posterior of the predictions.</p>
<p><img src="../../media/uncertainty_paper/DR_explain.png" class="img-fluid"></p>
<p>Demonstration of how Diabetic Retinopahy manifests itself in an individuals eye. Figure taken from <a href="http://wjscottmd.com/diabetic-retinopathy/">Dr.&nbsp;Winston J. Scott’s website.</a></p>
<p>The mean of this posterior was used as the final prediction value (p(disease|image) ) and the width of the distribution was used as a measure of how uncertain the model is about its prediction.</p>
<section id="why-cant-we-just-use-the-probability-as-a-measure-of-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="why-cant-we-just-use-the-probability-as-a-measure-of-uncertainty">Why can’t we just use the probability as a measure of uncertainty?</h3>
<p>There is a tricky and subtle difference between prediction uncertainty and model uncertainty. Prediction uncertainty (or simply the output probability of a class) says: “assuming I am a correct model (I have determined the correct separating hyperplane for these data) , this is the probability that the observation is a given class”, where as model uncertainty says: “I recognize that my separating hyperplane is not exact, and taking that into account, this is how confident I am in my predicted probability.”</p>
<p>If you make the assumption that the data is truly Bernoulli after being conditioned on the inputs, you can calculate the variance of the prediction by doing the classic <span class="math inline">\((1-p)*p\)</span> variance estimate, but the paper shows that this is too simplistic and under performs the dropout uncertainty measurement in all scenarios. <label for="tufte-mn-" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-" class="margin-toggle"><span class="marginnote">And even a random baseline! See figure 4.</span></p>
</section>
</section>
<section id="how-did-they-test-their-models" class="level2">
<h2 class="anchored" data-anchor-id="how-did-they-test-their-models">How did they test their models?</h2>
<p>At first glance it seems hard to actually compare the traditional no-uncertainty models and the Bayesian models as they are fundamentally different. The paper does a rather smart thing to deal with this and simulates how they envision the models being used in real life.</p>
<p>First they run predictions from the Bayesian model and the standard model on the entire test set, they then discard (or let an expert review) the predictions that the model was the most uncertain about. This discarding was done in multiple ways:</p>
<ol type="1">
<li>They set a boundary for uncertainty (any prediction that had an associated uncertainty above was discarded), they then computed model performance measures (AUC etc) on the retained data.</li>
<li>They set a boundary on the amount of observations retained: i.e.&nbsp;only the top % of the predictions are kept. This allowed them to compare their model to a standard model (using the aforementioned Bernoulli variance as a stand in for uncertainty) and also by just randomly discarding %.</li>
</ol>
</section>
<section id="what-relationships-did-they-find" class="level2">
<h2 class="anchored" data-anchor-id="what-relationships-did-they-find">What relationships did they find?</h2>
<p>Using the first uncertainty cutoff boundary they saw that, as they made their threshold more picky, the model performance monotonically increased. This shows that the model was correctly labeling the observations that it was getting wrong as uncertain more often than not. They showed this as well by simply looking at the distributions of uncertainty for the correctly classified observations and the incorrectly classified ones, with the incorrectly classified ones having a higher average uncertainty.</p>
<p><img src="https://media.springernature.com/lw900/springer-static/image/art%3A10.1038%2Fs41598-017-17876-z/MediaObjects/41598_2017_17876_Fig3_HTML.jpg" class="img-fluid"></p>
<p>Results of sliding the level of uncertainty tollerated for a prediction by the model (a) and sliding the proportion of the predictions that are not predicted (compared to a random baseline) (b).</p>
<p>Their second test setup, where they compared the standard model with no dropout at test time to the model with dropout at test time they saw that the dropout uncertainty always outperformed the random discarding of cases, where as the Bernoulli-based uncertainty actually performed worse than the random subset approach for a large range of the thresholds.</p>
<p><img src="../../media/uncertainty_paper/auc_performance.png" class="img-fluid"> Showing the effect of sliding the proportion of data predictions are performed on on the AUC of different models. Shows that the bayes by dropout approach is always better than the random baseline, and even is better than a deep gaussian process approach. Surprisingly the nieve uncertainty approach (‘standard dropout’) is often worse than random.</p>
</section>
<section id="what-implications-does-this-have" class="level2">
<h2 class="anchored" data-anchor-id="what-implications-does-this-have">What implications does this have?</h2>
<p>This paper shows that, using the recently published finding that dropout can be used to approximate a posterior, it is incredibly simple to turn a deep neural network (of any architecture) trained on complex medical data into one that can be used to not only predict diseases but also say when you should not rely on the model and seek other input for diagnosis.</p>
<p>Purely as a real-world demo that the idea that bayes as dropout works this paper is great, but the combination of well throughout experiments demonstrating the value of uncertainty in neural network prediction and demonstrating the power of bayes-by-dropout compared to other options for uncertainty, it becomes excellent.</p>
</section>
<section id="my-hesitations" class="level2">
<h2 class="anchored" data-anchor-id="my-hesitations">My hesitations</h2>
<p><strong>Data Balance</strong></p>
<p>It was not clear from the paper if they properly accounted for the unbalance of the data. The data is split 70-30 between no-disease and disease. They properly use AUC for their performance measure but they also make statements about the distribution of certainty in their model over the true status of the observation. Some of these seem to be a result of simply the fact that there are many more no-disease examples than disease ones. For instance taking a look at the following figure where they look at the two-dimensional density of model prediction to uncertainty to predictions they got correct vs those they got incorrect…</p>
<p><img src="../../media/uncertainty_paper/uncertainty_distributions.png" class="img-fluid"></p>
<p>We see a large blob of density in the bottom left corner. This seems to be to be a simple fact that there were many more non-diseased cases than diseased cases. I think the figure would be more informative if they used a balanced test set. If anything I think it would make their point stronger, but I think it would help elucidate if the model was just relying on the distribution of the training data to be a good predictor.</p>
<p><strong>Repurposing a softmax model</strong></p>
<p>They are collapsing an ordinal outcome (disease severity) into a binary disease not-disease indicator. For one of their models they took a high-performing kaggle model and simply summed the softmax outputs for the disease-corresponding classes to get their probability of disease.</p>
<blockquote class="blockquote">
<p>The publicly available network architecture and weights provided by the participant who scored very well in the Kaggle DR competition, which we will call JFnet…We recast the original model’s five output units (trained for Kaggle DR’s level discrimination task) to our binary tasks by summing the output of respective units.</p>
</blockquote>
<p>This makes me uneasy as the model was trained to minimize the categorical cross entropy, but then they treat it like it was trained to minimize binary cross entropy. It feels like there may be some unexplored side effects of the model dedicating more of its training to optimizing the parts corresponding to disease classification than to non-disease classification.</p>
<p>Also, they are dichotomizing, and what is the name of this blog?</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="lfod/real-blog" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>