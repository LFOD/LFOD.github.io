{
  "hash": "6b743f6db6b9a1ad11d4f6fa31867244",
  "result": {
    "markdown": "---\ntitle: \"Shinyviewr: camera input for shiny\"\nauthor: \"Nick Strayer\"\ndate: '2018-07-22'\ncategories: [\"shiny\", \"images\", \"deeplearning\"]\ntags: [\"shiny\", \"images\", \"deeplearning\"]\ndescription: \"A brief intro to, and tutorial for, the new function in the shinysense packages: shinyviewr. This function allows you to take photos using the camera on your computer or phone and directly send them into your shiny applications.\"\n---\n\n\n\n\n\n## Motivation\n\nMy package `shinysense` has been around for more than a year now. It started as a package to add swiping via touch screens to shiny for our app [Papr](https://jhubiostatistics.shinyapps.io/papr), but then slowly got built to include functions for hearing (`shinyearr`), movement (`shinymovr`), and drawing (`shinydrawr`). However one major sense was missing: vision. \n\nI had it on the to-do list for the package for a while but never got around to it. Then last week [Dean Attali](https://deanattali.com/) [pinged me on github](https://github.com/nstrayer/shinysense/issues/21) about the status of the camera-functionality to `shinysense`.\n\n\nThis, along with my recent dive into deep learning, spurred a renewed effort to add vision to shiny's senses. The result is the new function `shinyviewr`, which, like the rest of `shinysense`, comes in the form of a shiny module that can be easily added to your app to endow it with the ability to sense the outside world. \n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Coming soon to a shiny app near you: webcam input! <a href=\"https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw\">#rstats</a> <br>(Spurred on by <a href=\"https://twitter.com/daattali?ref_src=twsrc%5Etfw\">@daattali</a>) <a href=\"https://t.co/1Qav8ftDJs\">pic.twitter.com/1Qav8ftDJs</a></p>&mdash; Nick Strayer (@NicholasStrayer) <a href=\"https://twitter.com/NicholasStrayer/status/1020358336527708160?ref_src=twsrc%5Etfw\">July 20, 2018</a></blockquote>\n<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n\n## How to use\n\nIn this code I will supply demo code in the form of single page shiny apps. This means the UI and Server code are all contained in a single `.R` file. This makes reproducing everything easier for you. Just copy and paste the block into an RStudio console and run! \n\n__Setup__\n\nBefore you can run these examples you will need to have the latest version of `shinysense` downloaded from github. In addition, we will be running an example with a deep learning model obtained via the library Keras. Run these commands before to make sure you're all setup. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Installs latest version of shinysense.\ndevtools::install_github('nstrayer/shinysense')\n\n# Installs latest version of keras. \ndevtools::install_github('rstudio/keras')\n\n# Make sure you have the python versions of keras/tensorflow installed on your machine.\nkeras::install_keras()\n```\n:::\n\n\nNow that all that is out of the way let's start with the most basic example. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(shiny)\nlibrary(shinysense)\nlibrary(tidyverse)\n\nui <- fluidPage(\n  shinyviewrUI(\"myCamera\", height = '200px'),\n  imageOutput(\"snapshot\")\n)\n\nserver <- function(input, output) {\n  #server side call of the drawr module\n  myCamera <- callModule(\n    shinyviewr, \"myCamera\", \n    outputHeight = 300,\n    outputWidth = 400)\n\n  # logic for what happens after a user has drawn their values. \n  observeEvent(myCamera(), {\n    photo <- myCamera() \n    print(photo) #print to console for inspection\n    output$snapshot <- renderPlot({\n      plot(as.raster(photo)) # plot photo\n    })\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n```\n:::\n\n\n__Result:__\n\n<img src=\"../../media/viewr_post/basic_viewr_app.png\" height=400px />\n\nWe have a working app! \n\n## Output format\n\nA fair question at this point would be \"in what form does the image show up in shiny?\" If you run the above example and look at the console you will see the image object printed. It is a rather simple thing: just a 3D array with the dimensions `(height, width, channels)`, with `channels` being red, green, and blue quantities on a scale from 0-1. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(photo)\n> [1] 300 400   3\n```\n:::\n\n\n\nTo plot, you can do what we did in the example, just send it to the `as.raster` function and then plot the resultant raster object. \n\n## Usage on phones\n\nHaving a webcam view is interesting but ultimately rather limiting. One of the more powerful features of shiny is the ability of the apps to run anywhere, including on a phone. \n\n`shinyviewr` can automatically detect that it is running on a phone with a rear camera and will append to its UI a drop down menu to choose between which camera you use to take a photo. \n\n<div style = 'display: flex; flex-wrap: wrap'>\n<div>\nFront camera:\n<img src=\"../../media/viewr_post/front_camera.png\" height=200px width=200px />\n</div>\n<div>\nRear camera:\n<img src=\"../../media/viewr_post/rear_camera.png\" height=200px width=200px />\n</div>\n</div>\n\nThe ability to utilize rear-facing cameras on mobile devices hopefully opens up opportunities for researchers to collect data and run models right in the field, without having to awkwardly plug in an SD card and run a script later. Collect your data and run your model right in the moment!\n\n\n## Example app\n\nTaking a photo and just plotting it is... kind of lame. The real power of image input into shiny is, in my opinion, the easy interfacing with powerful machine learning libraries. To demonstrate this, let's build an app that attempts to classify whatever is in the photo you just took. \n\nThis is where Keras comes in from the setup script. We will use a pre-trained model that has been trained to recognize 1,000 different image classes from a dataset called 'imagenet.' Keras includes easy helpers to access multiple different models that have been already trained on this massive dataset for you, vastly simplifying your workload. \nWe will load the model `vgg50`. <label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">In order to speed up your app, before you run the example, make sure to run the line loading the model. The first time this line is run it will go and fetch the (rather hefty) file containing the weights. After it's run the first time it doesn't need to be downloaded again though.</span>\n\n__Image classification example__\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(shiny)\nlibrary(shinysense)\nlibrary(tidyverse)\nlibrary(keras)\n\n# instantiate the model\nmodel <- application_resnet50(weights = 'imagenet')\n\nui <- fluidPage(\n  titlePanel(\"Image Classifier\"),\n  fluidRow(\n    column(\n      width = 7,\n      h3(\"Webcam\"),\n      shinyviewrUI(\"myCamera\", height = '250px')\n    ),\n    column(\n      width = 4, offset = 1,\n      h3('Last Photo'),\n      imageOutput(\"snapshot\", height = '250px')\n    )\n  ),\n  h3(\"Predictions\"),\n  plotOutput(\"predPlot\")\n)\n\n\nserver <- function(input, output) {\n  #server side call of the drawr module\n  myCamera <- callModule(\n    shinyviewr, \"myCamera\", \n    outputWidth = 500, \n    outputHeight = 500\n  )\n  \n  observeEvent(myCamera(), {\n    \n    photo <- myCamera() \n    photo_processed <- photo %>% \n      image_array_resize(224, 224) %>% \n      {.*255} %>%  #get image to 0-255 instead of 0-1\n      array_reshape(c(1, dim(.))) %>% \n      imagenet_preprocess_input()\n    \n    # make predictions then decode and print them\n    preds <- model %>% \n      predict(photo_processed) %>% \n      imagenet_decode_predictions(top = 20) %>% \n      .[[1]]\n    \n    output$predPlot <- renderPlot({\n        ggplot(preds, \n          aes(x = reorder(class_description, score), y = score)\n        ) +\n        geom_pointrange(aes(ymin = 0, ymax = score)) +\n        coord_flip()\n    })\n    output$snapshot <- renderPlot({\n      plot(as.raster(photo))\n    })\n  })\n}\nshinyApp(ui = ui, server = server)\n```\n:::\n\n\nWhile that may seem like a lot of code to look at. Keep in mind that it is everything needed to spin up an app that uses the deep learning to classify images coming from the camera of your phone or computer. It's pretty crazy how few lines it is in that context!\n\n<label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">The demo app linked will look a tad bit different from what you get if you run the code above. This is because I modified it with some stylistic changes that just added more code to the already long chunk above. If you want to see the exact code generating the demo app check out [the repo.](https://github.com/nstrayer/viewr_imagenet_demo/blob/master/basic_demo.R)</span>\nHere's a snapshot of the results when running on my phone. \n\n<img src=\"../../media/viewr_post/beer_classify.png\" height = 400px />\n\n\n[Here's a link](https://nstrayer.shinyapps.io/viewr_imagenet) to a live demo app for you to check out the app without having to run the code above. Beware, the app is hosted on the free tier of shinyapps.io so there's a decent chance it will not work by the time you read this. If this is the case, just running the code above will accomplish the same thing!\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}