{
  "hash": "e467829ad4b4cd54a4607f3b71cd83c9",
  "result": {
    "markdown": "---\ntitle: \"The United States of Seasons\"\nauthor: \"Nick Strayer\"\ndate: '2018-02-12'\ncategories: [\"visualization\", \"maps\", \"climate\"]\ntags: [\"visualization\", \"maps\", \"climate\"]\ndescription: \"How different is the warmest day from the coldest day all around the country? Using readings from 7,000+ NOAA weather stations across the country we can find out.\"\n---\n\n\n\n\n---\n\n<div class = 'fullwidth'>\n\n::: {.cell fig.fullwidth='true' class='fullwidth'}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=1152}\n:::\n:::\n\n</div>\n\n<label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">I think my favorite detail about this map is the little splotch that is the Smoky Mountains on the western edge of North Carolina. Having spent a good amount of time there they really do have a different climate than the surrounding area.</span>\n\nThe other day my girlfriend and I were talking about places we would like to live after grad school and one of the things that got brought up was how 'seasonal' the location is. She grew up in Long Beach, California, which essentially has no seasons, whereas I grew up near Ann Arbor, Michigan which very much has seasons. This got me to thinking: what does the country look like in the context of its seasonality? By 'seasonality' we mean how big of a shift in the weather is there during the year.\n\nBeing into data and maps I figured I would try and investigate this in a data-driven way. The final result is above this, but here I will walk you through the process that got me from question to map.\n\n### The data\n\nThe data that I wanted were the average (high) temperature over the year by different locations around the US. \n<label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">Turns out there are some nice APIs for getting this such as [Wunderground's](https://www.wunderground.com/weather/api?MR=1) but they all ended up requiring me to pay if I wanted to get anything close to the level of detail I wanted.</span>\nLuckily, we have a federal government agency that likes to track things related to climate/weather in the National Oceanic and Atmospheric Administration (NOAA) and they have [oodles of data.](https://www.ncdc.noaa.gov/) \n\nAfter some digging I discovered what I was looking for: [U.S. Climate Normals](https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/climate-normals/1981-2010-normals-data), a dataset for ~7,000 stations around the country that contains 'normal' or average measures of temperature, wind, etc for the dates from 1981-2010. \n\nLooking in their \"https\" datasource they had a nice `readme.txt`. After skimming this document I found that what I was looking for was in `products/auxiliary/station`, there was a problem though. It is briefly alluded to by the line in the `readme` corresponding to these files: `Users wanting to visually inspect normals for just one or a few stations should use the files in this subdirectory.` \"Visually inspect\"? I don't want that I want to read it in R! Let's see what the problem could be....\n\n__products/auxiliary/station/CQC00914801.normals.txt__\n```\nStation Name: MP ROTA AP\nGHCN Daily ID: CQC00914801\nLatitude: 14.1717\nLongitude: 145.2428\nElevation: 179.2m\n \nTemperature-Related Normals\n \nMonthly                   JAN    FEB    MAR    APR    MAY    JUN    JUL    AUG    SEP    OCT    NOV    DEC\n-----------------------------------------------------------------------------------------------------------\n        mly-tmax-normal   836R   832R   839R   852R   865R   873R   867R   865R   867R   865R   859R   848R\n        mly-tavg-normal   788R   782R   790R   801R   811R   817R   811R   808R   808R   808R   806R   800R\n        mly-tmin-normal   740R   733R   741R   750R   756R   761R   754R   750R   748R   751R   753R   752R\n        mly-dutr-normal    96R    99R    98R   102R   108R   112R   113R   115R   119R   114R   105R    95R\n        mly-cldd-normal   428R   371R   434R   453R   498R   501R   498R   488R   473R   490R   468R   465R\n        mly-htdd-normal     0R     0R     0R     0R     0R     0R     0R     0R     0R     0R     0R     0R\n        mly-tmax-stddev     8R    14R    13R     8R     6R     8R     6R     5R    14R     7R     7R     8R\n        mly-tavg-stddev     5R    11R    10R     5R     4R     6R     4R     5R     9R     5R     5R     6R\n        mly-htdd-base60     0R     0R     0R     0R     0R     0R     0R     0R     0R     0R     0R     0R\n...\n```\n\nTurns out these files are just a bunch of tables concatinated into a single text file. Not exactly able to be read by `read_csv`. All I want though is that top line: `mly-tmax-normal...`. So we can go old school and read the file line by line and extract just that line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_station_data <- function(station_id){\n  station_url <- paste0(\n    \"https://www1.ncdc.noaa.gov/pub/data/normals/1981-2010/products/auxiliary/station/\",\n    station_id,\n    \".normals.txt\"\n  )\n  file_lines <- readLines(url(station_url))\n  \n  # find the line that has monthly temp maxes\n  loc_of_temp <- file_lines %>% str_detect('mly-tmax-normal' ) %>% which()\n  \n  months <- c('Jan', 'Feb', \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n  \n  # strip out white space and split into a vector of 12 months\n  file_lines[loc_of_temp] %>% \n    strsplit('\\\\s+') %>% \n    .[[1]] %>% \n    .[-c(1,2)] %>% {\n      data_frame(temp = ., month = months, station_id = station_id)\n    }\n}\n```\n:::\n\n\nThis function will take a given station ID, download the text file corresponding to that station, and extract the monthly temperatures into a nice clean dataframe. Let's test it out on the station we looked at two seconds ago. \n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_591800df989f0454064f573d37f80569'}\n\n```{.r .cell-code}\nget_station_data('CQC00914801') %>% \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|temp |month |station_id  |\n|:----|:-----|:-----------|\n|836R |Jan   |CQC00914801 |\n|832R |Feb   |CQC00914801 |\n|839R |Mar   |CQC00914801 |\n|852R |Apr   |CQC00914801 |\n|865R |May   |CQC00914801 |\n|873R |Jun   |CQC00914801 |\n|867R |Jul   |CQC00914801 |\n|865R |Aug   |CQC00914801 |\n|867R |Sep   |CQC00914801 |\n|865R |Oct   |CQC00914801 |\n|859R |Nov   |CQC00914801 |\n|848R |Dec   |CQC00914801 |\n:::\n:::\n\n\n<label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">The weird letters in the temp columns correspond to the quality of the measurement. Some stations hadn't been up for all the years so their averages were not as exact. We ignore this here but in a more rigerous analysis we should probably look into them a bit more.</span>\n\n\nWonderful! Now all we need is a list of the unique station ids and we're good to go! Luckily that is in the data as well. `station-inventories/allstations.txt` contained all the info on each station along with the geographic position of the station. Let's grab it so we can get scraping!\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_8668e79e1c5ddb9a13333e919db852ee'}\n\n```{.r .cell-code}\ninfo_loc <- \"https://www1.ncdc.noaa.gov/pub/data/normals/1981-2010/station-inventories/allstations.txt\"\n\n# taken from readme.txt\ncolumn_names <- c(\n  'station_id','lat','lon','elevation',    \n  'state','name','gsnflag', 'hcnflag',      \n  'wmoid', 'method' )\n\nstation_info <- read_table(info_loc, col_names = column_names) %>% \n  select(station_id, lat, lon, state)\n\nstation_info %>% \n  head() %>% \n  knitr::kable(align = c('l','r', 'r', 'r'))\n```\n\n::: {.cell-output-display}\n|station_id  |      lat|       lon| state|\n|:-----------|--------:|---------:|-----:|\n|AQC00914000 | -14.3167| -170.7667|    AS|\n|AQW00061705 | -14.3306| -170.7136|    AS|\n|CAW00064757 |  44.2325|  -79.7811|    ON|\n|CQC00914080 |  15.2136|  145.7497|    MP|\n|CQC00914801 |  14.1717|  145.2428|    MP|\n|CQC00914855 |  15.1189|  145.7294|    MP|\n:::\n:::\n\n\nNow we just need to loop through all the ids and amass a nice dataset of their temperatures over the year. We are going to do this in a loop rather than something like `purrr::map_df` because we can more easily handle errors and log progress. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nstations <- station_info$station_id\n\n# initialize dataframe for first station\nall_station_data <- get_station_data(stations[1])\n# vector to hold the stations who we failed to get data for\nbad_stations <- c()\n\n# go!\nfor(station in stations[-1]){\n  print(station)\n  tryCatch({\n    all_station_data <- all_station_data %>% \n      bind_rows(get_station_data(station))\n  },error = function(e) {\n    bad_stations <- c(bad_stations, station)\n  })\n}\n```\n:::\n\n\nThis loop takes a long time to run. With each query taking ~1-2 seconds and there being ~7,000 unique stations. I let it sit overnight. <label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">Amazingly not a single station failed to download. When dealing with long running scripts like this hitting things on the internet that is practically unheard of.</span>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Exactly 90,000 rows.\nall_station_data %>% dim()\n## [1] 90000     3\n```\n:::\n\n\nSo we want a measure of seasonality. We will define this as how big of a delta there is between the warmest and coldest months for each station.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-8_8d37d94b6d5d8a3bc9c549b9668de0ff'}\n\n```{.r .cell-code}\nswing_by_station <- all_station_data %>% \n  mutate(\n    quality = str_extract(temp, '[A-Z]'),\n    temp = as.numeric(str_replace(temp, '[A-Z]', ''))\n  ) %>% \n  group_by(station_id) %>% \n  summarise(\n    warmest_month = month[which.max(temp)],\n    coldest_month = month[which.min(temp)],\n    swing = (max(temp) - min(temp))/10\n  ) %>% \n  right_join(station_info, by = 'station_id') %>% \n  filter((lon < -60 & lon > -130 & lat > 20) & !is.na(swing)) # we only want mainland us\n\nswing_by_station %>% \n  head() %>% \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|station_id  |warmest_month |coldest_month | swing|     lat|      lon|state |\n|:-----------|:-------------|:-------------|-----:|-------:|--------:|:-----|\n|CAW00064757 |Jul           |Jan           |  51.9| 44.2325| -79.7811|ON    |\n|USC00010063 |Aug           |Jan           |  39.2| 34.2553| -87.1814|AL    |\n|USC00010160 |Jul           |Jan           |  35.5| 32.9453| -85.9481|AL    |\n|USC00010178 |Jul           |Jan           |  36.6| 33.1272| -88.1550|AL    |\n|USC00010252 |Jul           |Jan           |  30.8| 31.3072| -86.5225|AL    |\n|USC00010260 |Jul           |Jan           |  40.4| 34.9092| -87.2747|AL    |\n:::\n:::\n\n\nWe're basically done now! Let's just plot this and call it a day... Unfortnuately it doesn't work that easily. Let's see why. \n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-9_70d34bfeb62c9913671aa4970212b18c'}\n\n```{.r .cell-code}\nggplot(swing_by_station, aes(x = lon, y = lat, color = swing)) + \n  geom_point(alpha = 0.2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nUnsurprisingly, our data are unevenly distributed around the country, we want a nice smooth plot of the swing so we need to do some interpolation. Enter the geospatial side of R. \n\n## Geospatial interpolation\n\nWhat we need to do is fit a model that can predict average swing given a lat-lon pair, along with generating a high-density evenly-spaced grid to run that model on. \n\n### Gridding it\n\nTo get the grid we will download a US shapefile from the [Census' Tiger repository](https://www.census.gov/cgi-bin/geo/shapefiles/index.php) and use the library `rgdal` to read it in and exclude the given locations we don't want to look at.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-10_fb053b89bf007f9d2119a0e47cd2e2b8'}\n\n```{.r .cell-code}\nlibrary(sp)\nlibrary(rgdal)\nlibrary(gstat)\nlibrary(maptools)\n\nnot_wanted <- c(\n  'Alaska', 'American Samoa', 'Puerto Rico', 'Guam', \n  'Commonwealth of the Northern Mariana Islands United States Virgin Islands', \n  'Commonwealth of the Northern Mariana Islands', \n  'United States Virgin Islands', 'Hawaii')\n\nus <- rgdal::readOGR(\"../../media/seasons/us_shapefile/\", \"cb_2016_us_state_500k\") %>% \n  subset(!(NAME %in% not_wanted))\n## OGR data source with driver: ESRI Shapefile \n## Source: \"/Users/lucymcgowan/Dropbox (Wake Forest University)/wonderland-db/lfod/media/seasons/us_shapefile\", layer: \"cb_2016_us_state_500k\"\n## with 56 features\n## It has 9 fields\n## Integer64 fields read as strings:  ALAND AWATER\n\nplot(us)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nLooks right to me! Now let's calculate a grid of points over this. \n\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-11_a1f4174a13b1a2cf988137476da93191'}\n\n```{.r .cell-code}\n# make a big grid that corresponds to the bounding box around out shapefile\ngrid_us <- makegrid(us, n = 5000) %>% \n  SpatialPoints(proj4string = CRS(proj4string(us))) %>% \n  .[us,] # subset the grid such that it only has points that fall inside of our states.\n\nplot(grid_us, type = 'p')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n<label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">In the final product I bumped up the resolution (n) way higher to make the result smoother</span>\n\nNext we need to fit a model for interpolation. Here I use one of the most common techniques for this: [inverse distance weighting](https://en.wikipedia.org/wiki/Inverse_distance_weighting). <label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">The task of fitting a model to these data is actually a fascinating one that I am not giving nearly enough weight in this post. Here we are just having as inputs lat and lon but we could easily make it more realistic by adding in elevation etc. Some models use 2-d gaussian processes to fit flexible models, some use splines, it's a vast world and one I would love to dig into more!</span>\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-12_02a6b2cfc61dddfa4cdce059dd3e044e'}\n\n```{.r .cell-code}\n# convert the data to a spacial dataframe.\nsp::coordinates(swing_by_station) = ~lon + lat\n\n# make sure that the projection matches the grid we've built.\nproj4string(swing_by_station) <- CRS(proj4string(us)) \n\n# fit basic inverse distance model\nidw_model <- gstat::idw(\n  formula = swing ~ 1, \n  locations = swing_by_station, \n  newdata = grid_us,\n  idp = 2) \n## [inverse distance weighted interpolation]\n\n# extract predictions from the interpolation model\ninterpolated_results = as.data.frame(idw_model) %>% {# output is defined as a data table\n    names(.)[1:3] <- c(\"lon\", \"lat\", \"swing\")  # give names to the modeled variables\n    . } %>% \n  select(lon, lat, swing)\n\ninterpolated_results %>% \n  head() %>% \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|    lon|   lat|    swing|\n|------:|-----:|--------:|\n| -80.80| 25.34| 18.65760|\n| -80.26| 25.34| 18.35323|\n| -81.34| 25.88| 16.24864|\n| -80.80| 25.88| 18.56805|\n| -80.26| 25.88| 15.06180|\n| -98.62| 26.42| 31.26166|\n:::\n:::\n\n\nWe're in business! We've now got an evenly spaced grid of points we can plot in a standard raster way!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(interpolated_results, aes(x = lon, y = lat)) + \n  geom_raster( aes(fill = swing)) +\n  xlim(-125, -65) + ylim(24, 51) + \n  theme_void() +\n  labs(fill = \"Temp swing in degrees\") +\n  borders('state', alpha = 0.1, size = 0.1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nLooks good but could use some improvement. The gradiations are rather smooth and thus it's hard to see. Let's bin the swings to 5 degree intervals and change the color pallet to be a bit more appropriate. <label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">I chose 5 degree intervals here because I figured that was about the limit of sensitivity for the average person. Totally arbitrary though.</span>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninterpolated_results %>% \n  mutate(swing_in_5s = round(swing/5)*5) %>% \n  ggplot(aes(x = lon, y = lat)) + \n  geom_raster( aes(fill = swing_in_5s)) +\n  scale_fill_distiller(palette = 'YlGn', direction = 1) +\n  xlim(-125, -65) + ylim(24, 51) + \n  theme_void() +\n  labs(fill = \"Temp swing in degrees\") +\n  borders('state', alpha = 0.1, size = 0.1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nThere it is! In the top plot I have rerun the grid with a much higher `n` and also tweaked some settings in the legend/scales to make the plot look a bit better. Hopefully now when you have some interesting geospatial data that is not evenly distributed you will know how to deal with it!\n\n### Code for top figure. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nguide_tinker =  guide_legend(\n  title.position = \"top\",\n  label.position=\"bottom\",\n  label.hjust = 0.5,\n  direction = \"horizontal\",\n  keywidth = 1,\n  nrow = 1 )\n\ncolourCount = interpolated_swings$swing_in_5s %>% unique() %>% length()\npalette = colorRampPalette(brewer.pal(9, \"YlGnBu\"))(colourCount)\n\ninterpolated_swings %>% \n  mutate(swing_in_5s = factor(round(swing_in_5s)))%>% \n  ggplot(aes(x = lon, y = lat)) + \n  geom_raster( aes(fill = swing_in_5s)) +\n  scale_fill_manual(values = palette, \n                    guide = guide_tinker)  +\n  xlim(-130,-67) + ylim(24,50) +\n  theme_void() +\n  theme(\n    text = element_text(family = 'Montserrat'),\n    legend.justification = c(0,0),\n    legend.position = c(0,0.02),\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 8),\n    legend.box.background = element_rect(fill = '#f0f0f0', color = NA)\n  ) + \n  labs(\n    title = \"The United States of Seasons\",\n    subtitle = \"Difference between the hottest and coldest days of the year\",\n    fill = \"Temp swing in degrees\") +\n  borders('state', alpha = 0.1, size = 0.1)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}