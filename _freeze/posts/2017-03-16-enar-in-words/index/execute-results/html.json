{
  "hash": "79b3fd819e07fdfc8e3f882cbd4dd65f",
  "result": {
    "markdown": "---\nauthor: \"Lucy D'Agostino McGowan\"\ndate: 2017-03-16T14:56:30-04:00\ndraft: false\nimage: \"\"\nshare: true\ncategories: [\"ENAR\", \"tidytext\", \"conferences\", \"rstats\"]\ntags:\n- ENAR\n- tidytext\n- conferences\n- rstats\ntitle: \"ENAR in words\"\ndescription: \"I had an absolutely delightful time at ENAR this year. Lots of talk about the intersection between data science & statistics, diversity, and great advancements in statistical methods. Since there was quite a bit of twitter action, I thought I'd do a quick tutorial in scraping twitter data in R.\"\n---\n\n\nI had an absolutely delightful time at [ENAR](http://www.enar.org) this year. Lots of talk about the intersection between data science & statistics, diversity, and **exceptional** advancements in statistical methods. \n\n<!-- My conference began with the [diversity workshop](https://www.enar.org/meetings/FosteringDiversity/) where we heard from my former adviser, [Dr. Melody Goodman](http://twitter.com/goodmanthebrain), about her journey to biostatistics & her work in community advocacy and health disparities, a career panel, and a graduate student panel. [Dr. Emma Benn](https://twitter.com/EKTBenn) pointed out that Miguel de Cervantes (author of Don Quixote) would have been a biostatistian in another life: -->\n\n<!-- <span style=\"color:#EB6864; font-size: 20pt\">\"By a small sample we may judge of the whole piece\" -->\n<!-- </span> -->\n\n\nI **loved** it, but let's see what others were saying! Check out this word cloud of the most commonly tweeted words.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThis certainly sums up my experience. Some of my favorites that make a big appearance:\n\n * methods\n * causal inference\n * resources\n * diversity\n * data\n * learning\n * loving\n \nSince there was quite a bit of twitter action, I thought I'd do a quick tutorial in scraping twitter data in `R`.\n\n## Get twitter credentials\n\nGo [here](https://apps.twitter.com) and create an app - this will give you a **Consumer key**, **Consumer secret**. \n\n\n::: column-margin\nPro Tip: be sure to enter `http://127.0.0.1:1410` \nas your `Callback URL`. If you get lost, there is a \ngreat tutorial on this process [here](https://mkearney.github.io/rtweet/articles/auth.html)\n:::\n\n\n## Scrape tweets\n\nWe will use the `rtweet` package to scrape the tweets using the `search_tweets` function.\n\n::: column-margin\nMy original tutorial used `twitteR`, but [MaÃ«lle](https://twitter.com/ma_salmon) \nkindly pointed out that it is on the way out and\n`rtweet` is the better option, so it's been updated!\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('rtweet')\n\ntwitter_token <- create_token(\n  app = \"PASTE_YOUR_APP_NAME_HERE\",\n  consumer_key = \"PASTE_YOUR_CONSUMER_KEY_HERE\",\n  consumer_secret = \"PASTE_YOUR_CONSUMER_SECRET_HERE\")\n\ndat <- search_tweets('#ENAR2017', n = 1e4, since = '2017-03-10', token = twitter_token)\n```\n:::\n\n\nIf you would like to practice with the ENAR tweet data, you can load mine in with the following code & continue with the example. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(url(\"https://github.com/LFOD/real-blog/raw/master/data/enar_data.rda\"))\n```\n:::\n\n\n## Wrangle tweets\n\nNow we need to corral these tweets into something we can analyze. We are going to use some data-wrangling packages (`dplyr`, `purrr`ðŸ˜º, and `stringr`) as well as Julia & David's `tidytext`.\n\n::: column-margin\nFor more details on how to analyze text,\ncheck out their book [Text Mining with R](http://tidytextmining.com), \nthe code below is modified from one of  their examples.\n:::\n\nWe will then use the `wordcloud` package to display our results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load packages\nlibrary('dplyr')\nlibrary('purrr')\nlibrary('stringr')\nlibrary('tidytext')\nlibrary('wordcloud')\n```\n:::\n\n\nWe are going to get rid of unwanted symbols and links, split the tweets into individual words, and filter out some stop words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#this will drop links & symbols\ndrop_pattern <- \"https://t.co/[A-Za-z\\\\d]+|http://[A-Za-z\\\\d]+|&amp;|&lt;|&gt;|RT|https|ht\"\n#this pattern is great for twitter, includes # and @ symbols\nunnest_pattern <- \"([^A-Za-z_\\\\d#@']|'(?![A-Za-z_\\\\d#@]))\"\n\ntweets <- dat %>% \n  filter( !grepl(\"#OTORRINO\", text)) %>% # we have one tweeter with our hashtag that wasn't at our conference\n  mutate(text = str_replace_all(text, drop_pattern, \"\")) %>%\n  unnest_tokens(word, \n                text, \n                token = \"regex\", \n                pattern = unnest_pattern) %>%\n  filter(!(word %in% stop_words$word),\n         str_detect(word, \"[a-z]\"),\n         !grepl(\"@\", word )) \n```\n:::\n\n\nNow it's plotting time!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncols <- c(brewer.pal(8,\"Dark2\"), rep(brewer.pal(8,\"Dark2\"), each = 5) ) #make some colors for our plot\n\ntweets %>%\n  count(word) %>%\n  with(wordcloud(word, \n                 n,\n                 min.freq = 5,\n                 random.order = FALSE,\n                 colors = cols))\n```\n:::\n\n\n\nYou did it! Easy as [Ï€](https://potpieshop.files.wordpress.com/2016/03/pi-day.jpg?w=665).\n\n![](https://media.giphy.com/media/fBZXu9v0qjjTq/giphy.gif)\n\nFor giggles, let's try another word cloud package [`wordcloud2`](https://github.com/lchiffon/wordcloud2). This one is interactive (but not on CRAN, you can install using `devtools::install_github(\"lchiffon/wordcloud2\")`).\n \nFor a word cloud similar to the one above, we can use the `wordcloud2` function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('wordcloud2')\n\ntweets %>%\n  count(word) %>%\n  filter(n > 2) %>%\n  wordcloud2(size = 3, minRotation = -pi/2, maxRotation = -pi/2)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"wordcloud2 html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-a6e5e675240e97e0373d\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-a6e5e675240e97e0373d\">{\"x\":{\"word\":[\"#biostat\",\"#biostatistics\",\"#causalinference\",\"#datascience\",\"#diversity\",\"#drscarlettbellamy\",\"#enar2017\",\"#personalizedmedicine\",\"#rlady\",\"#rstats\",\"'get\",\"45pm\",\"abt\",\"academic\",\"accuracy\",\"adaptive\",\"adding\",\"address\",\"adherence\",\"advice\",\"afternoon\",\"alzheimer's\",\"amazing\",\"analysis\",\"assays\",\"association\",\"award\",\"awards\",\"ba\",\"ballroom\",\"basket\",\"bayesian\",\"bellamy\",\"bingo\",\"biomarkers\",\"biostat\",\"biostatistician\",\"biostatisticians\",\"biostatistics\",\"books\",\"bottom\",\"brilliant\",\"browse\",\"calc\",\"career\",\"chairs\",\"change\",\"chen\",\"choice\",\"cited\",\"classic\",\"clinical\",\"colleage\",\"colleague\",\"coming\",\"communities\",\"comparative\",\"competition\",\"conference\",\"confidence\",\"congrats\",\"constant\",\"contribute\",\"contributions\",\"cool\",\"create\",\"cunanan\",\"cv\",\"dance\",\"data\",\"day\",\"dc\",\"decision\",\"dependent\",\"designs\",\"diff\",\"discusses\",\"discussing\",\"discussion\",\"discussions\",\"distinguised\",\"diverse\",\"diversify\",\"doctoral\",\"doug\",\"dr\",\"driven\",\"east\",\"editor\",\"effectiveness\",\"ehr\",\"emrs\",\"enar\",\"engaging\",\"enjoyed\",\"error\",\"estimation\",\"evaluating\",\"evidence\",\"expect\",\"expensive\",\"experts\",\"favorite\",\"fay\",\"fei\",\"fellow\",\"field\",\"filling\",\"final\",\"flexible\",\"folks\",\"forward\",\"found\",\"framework\",\"free\",\"friends\",\"fun\",\"funding\",\"future\",\"gao\",\"generation\",\"github\",\"giving\",\"glickman\",\"graduate\",\"gurstelle\",\"happening\",\"hard\",\"harnessing\",\"health\",\"hear\",\"heterogeneous\",\"history\",\"hope\",\"huge\",\"hypothesis\",\"impact\",\"imposteriors\",\"including\",\"index\",\"inference\",\"info\",\"innovative\",\"inspiring\",\"integrating\",\"international\",\"intl\",\"introducing\",\"involved\",\"james\",\"jiang\",\"jin\",\"jon\",\"journals\",\"junior\",\"keynote\",\"kids\",\"kristen\",\"leaders\",\"leading\",\"learning\",\"list\",\"louise\",\"love\",\"loving\",\"machine\",\"marvin\",\"materials\",\"meeting\",\"methodological\",\"methods\",\"microbial\",\"mig\",\"minute\",\"missed\",\"model\",\"modeling\",\"models\",\"morn\",\"morning\",\"multi\",\"multiple\",\"nig\",\"nonparametric\",\"o'malley\",\"office\",\"official\",\"opportunities\",\"options\",\"outcome\",\"outstanding\",\"page\",\"paired\",\"panel\",\"paper\",\"participants\",\"party\",\"patient\",\"people\",\"phd\",\"pioneering\",\"poe\",\"policy\",\"poster\",\"power\",\"powerhouse\",\"ppl\",\"precision\",\"presentation\",\"president\",\"presidential\",\"prez\",\"program\",\"promotion\",\"puzzled\",\"question\",\"questions\",\"quick\",\"rab\",\"real\",\"recommended\",\"relevant\",\"research\",\"researcher\",\"resources\",\"rig\",\"rna\",\"robots\",\"rocks\",\"ryan\",\"sample\",\"scarlett\",\"science\",\"score\",\"scraping\",\"sd\",\"seq\",\"session\",\"sessions\",\"shaping\",\"shiny\",\"shout\",\"shrinkage\",\"similar\",\"single\",\"skills\",\"smart\",\"smartphon\",\"start\",\"statistics\",\"stats\",\"stoked\",\"story\",\"student\",\"students\",\"study\",\"talk\",\"talking\",\"talks\",\"tang\",\"test\",\"testing\",\"time\",\"tomorrow\",\"trial\",\"trials\",\"tue\",\"tutorial\",\"twitter\",\"variety\",\"voic\",\"walked\",\"wang\",\"wasteful\",\"watching\",\"web\",\"wed\",\"week\",\"wiley\",\"wilson\",\"winners\",\"wins\",\"won\",\"wonderful\",\"workshop\",\"world\",\"wrapping\",\"wrong\",\"yue\",\"zelen\"],\"freq\":[3,18,12,3,54,4,409,3,3,8,3,7,10,6,3,6,10,11,3,6,6,4,3,10,3,3,9,5,7,3,3,6,11,5,3,9,4,21,21,38,4,3,4,3,11,5,3,4,3,3,3,9,3,4,7,3,5,6,5,3,6,6,4,6,11,4,3,3,5,43,11,8,4,3,5,3,7,6,6,3,3,7,3,9,3,5,4,3,4,5,3,4,3,6,3,3,3,7,4,6,5,5,4,6,4,3,3,3,4,6,3,4,6,4,3,4,3,4,16,7,18,7,14,3,4,4,3,8,4,6,11,4,4,4,3,4,3,3,34,4,6,3,6,5,5,11,3,3,3,4,5,3,4,4,5,17,10,3,6,5,12,50,3,5,10,7,3,34,12,3,10,5,3,3,3,9,6,9,3,5,4,3,5,3,4,3,4,6,10,3,6,4,3,10,22,3,3,8,10,9,3,3,7,29,4,3,4,4,9,11,6,4,10,7,3,3,3,6,6,4,6,4,21,6,37,7,5,3,3,3,5,11,4,3,11,4,6,32,10,3,7,7,4,4,3,3,10,3,4,9,4,7,3,36,9,3,29,6,10,3,5,5,12,7,5,4,3,5,8,3,4,3,3,5,7,7,3,5,4,4,14,3,4,3,68,6,5,6,5,3],\"fontFamily\":\"Segoe UI\",\"fontWeight\":\"bold\",\"color\":\"random-dark\",\"minSize\":0,\"weightFactor\":1.320293398533007,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-1.570796326794897,\"maxRotation\":-1.570796326794897,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"circle\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":{\"render\":[{\"code\":\"function(el,x){\\n                        console.log(123);\\n                        if(!iii){\\n                          window.location.reload();\\n                          iii = False;\\n\\n                        }\\n  }\",\"data\":null}]}}</script>\n```\n\n:::\n:::\n\n\nTry the following to make an `R` shaped cloud using the `letterCloud` function!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweets %>%\n  count(word) %>%\n  filter(n > 1) %>%\n  letterCloud(size = 3, word = \"R\") \n```\n:::\n\n\nHappy scraping!",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/htmlwidgets-1.6.2/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/wordcloud2-0.0.1/wordcloud.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/wordcloud2-0.0.1/wordcloud2-all.js\"></script>\n<script src=\"../../site_libs/wordcloud2-0.0.1/hover.js\"></script>\n<script src=\"../../site_libs/wordcloud2-binding-0.2.1/wordcloud2.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}