{
  "hash": "197dc935c7bc910055209dfca01b10e5",
  "result": {
    "markdown": "---\ntitle: 'p-value thoughts: A twitter follow up'\nauthor: Lucy D'Agostino McGowan\ndate: '2018-08-21'\nslug: p-value-thoughts-a-twitter-follow-up\ncategories: [\"rstats\", \"p-values\"]\ntags: [\"rstats\", \"p-values\"]\ndescription: \"A conversation about how \\\"convincing\\\" various studies were based on sample size and p-values led me to post a poll on twitter. Here I discuss some thoughts that came up based on these results. tl;dr: p-values are hard, twitter is a fun way to spur stats convos!\"\n---\n\n\nA conversation about how \"convincing\" various studies were based on sample size and p-values led me to post the following poll on twitter.\n\n<label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">Fun fact: This was my _first ever_ Twitter poll! I've avoided them because they seem hard to do well, but it led to fun conversations and thinking about the results was actually quite enjoyable!</span>\n`{{% tweet \"1029534591760195584\" %}}`{=html}\n\nThe results definitely surprised me - while I _somewhat_ expected the large study to win out, I definitely didn't think it would by such a large margin. _For what it's worth, I would have voted for the small study if I could have voted in my own poll._\n\n<label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">You can find Royall's [\"The Effect of Sample Size on the Meaning of Significance Tests\"](https://www.jstor.org/stable/2684616) here. Unfortunately it is behind a paywall, but the first page is available has a lot of good discussion.</span>My initial intention was to post this question, and follow up by posting an article by Royall titled   \"The Effect of Sample Size on the Meaning of Significance Tests\" where this conundrum is discussed. DaniÃ«l Lakens anticipated this and responded with a link to the same article as well!\n\n`{{% tweet \"1029734016474390528\" %}}`{=html}\n\nWhile this paper doesn't necessarily tell you under which circumstance you ought to be more _convinced_, when I initially read it in Jeffrey Blume's Advanced Topics course at Vandy, I walked away with two main impressions: \n\n1. I would find a small study with the same p-value as a large study more convincing, since the effect size must be larger for this to be (_there are some assumptions here that I will address below_).  \n2. Since I am convinced differently by two studies with the same p-value, but different sample sizes, p-values must not be a measure of evidence.\n\nI think I still stand by 2, but I want to delve a bit deeper into 1, since it seems most of the people in my biased twitter-sphere disagree with me. To do this, I am going to explore three scenarios. In all cases, there is a small study and a large study, and the p-values from both studies are the same.\n\n1. You have two studies measuring _different interventions_ on the _same population_.  \n2. You have two studies measuring the _same intervention_ on the _same population_.  \n3. You have two studies measuring the _same intervention_ on _different populations_.\n\n## Different intervention, same population\n\nThis first scenario is the one I was considering when I posted the poll (and when I initially read Royall's paper). This definitely demonstrates one way twitter polls are so delightfully flawed -- they often don't leave enough room to state all assumptions -- but in this case I actually think this lead to really neat discussion, since differing views came up in the replies!\n\nIn order to demonstrate what I mean here, I am going to set up a small simulation. We have two studies, **Study A** and **Study B**. **Study A** is \"small\", it has 16 participants. **Study B** is \"large\", it has 10,000 participants. Both are examining new blood pressure drugs, drug **A** and drug **B**, and comparing them to \"standard of care\". The study populations are the same, let's say the distribution of systolic blood pressure in this population is Normal with a mean of 145 and a standard deviation of 30. I can generate this population in R with the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nset.seed(924)\npopulation <- tibble(\n  baseline_bp = rnorm(10^6, mean = 145, sd = 30)\n)\n\nggplot(population, aes(baseline_bp)) + \n  geom_histogram(bins = 30)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nUnder these circumstances, in order for the two p-values to be the same, the effect size for the larger study, **Study B**, must be smaller. For example, let's say the true effect of Drug A is -2.5, and the effect of Drug B is -0.1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npopulation <- population %>%\n  mutate(bp_drug_a = baseline_bp - 2.5 + rnorm(10^6),\n         bp_drug_b = baseline_bp - 0.1 + rnorm(10^6),\n         bp_soc = baseline_bp + rnorm(10^6))\n```\n:::\n\n\nLet's create the sample for **Study A** and see what happens.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n# sample 16 from the large population\nstudy_a <- population[sample(1:10^6, 16), ]\n\n# update so the first half receive drug A and the second receive standard of care\nstudy_a <- study_a %>%\n  mutate(drug_a = case_when(\n    row_number() %in% 1:8 ~ 1,\n    TRUE ~ 0\n  ),\n  outcome = case_when(\n    drug_a == 1 ~ bp_drug_a,\n    TRUE ~ bp_soc\n  )) %>%\n  select(baseline_bp, outcome, drug_a)\n\n# test association between drug A and blood pressure\nlm(outcome ~ baseline_bp + drug_a, data = study_a) %>%\n  broom::tidy() %>%\n  filter(term == \"drug_a\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 5\n  term   estimate std.error statistic  p.value\n  <chr>     <dbl>     <dbl>     <dbl>    <dbl>\n1 drug_a    -2.84     0.558     -5.08 0.000210\n```\n\n\n:::\n:::\n\n\nAnd now the sample for **Study B**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(62)\n# sample 10000 from the large population\nstudy_b <- population[sample(1:10^6, 10000), ]\n\n# update so the first half receive drug B and the second receive standard of care\nstudy_b <- study_b %>%\n  mutate(drug_b = case_when(\n    row_number() %in% 1:5000 ~ 1,\n    TRUE ~ 0\n  ),\n  outcome = case_when(\n    drug_b == 1 ~ bp_drug_b,\n    TRUE ~ bp_soc\n  )) %>%\n  select(baseline_bp, outcome, drug_b)\n\n# test association between drug B and blood pressure\nlm(outcome ~ baseline_bp + drug_b, data = study_b) %>%\n  broom::tidy() %>%\n  filter(term == \"drug_b\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 5\n  term   estimate std.error statistic     p.value\n  <chr>     <dbl>     <dbl>     <dbl>       <dbl>\n1 drug_b   -0.104    0.0201     -5.19 0.000000220\n```\n\n\n:::\n:::\n\n\nBoth of these have approximately the same p-value (0.0005), but the effect sizes are very different! Perhaps we can see this relationship a bit more clearly using confidence intervals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_a <- lm(outcome ~ baseline_bp + drug_a, data = study_a)\nconfint_a <- broom::tidy(mod_a) %>%\n  bind_cols(broom::confint_tidy(mod_a)) %>%\n  filter(term == \"drug_a\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: confint_tidy is now deprecated and will be removed\nfrom a future release of broom. Please use the applicable\nconfint method.\n```\n\n\n:::\n\n```{.r .cell-code}\nmod_b <- lm(outcome ~ baseline_bp + drug_b, data = study_b)\nconfint_b <- broom::tidy(mod_b) %>%\n  bind_cols(broom::confint_tidy(mod_b)) %>%\n  filter(term == \"drug_b\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: confint_tidy is now deprecated and will be removed\nfrom a future release of broom. Please use the applicable\nconfint method.\n```\n\n\n:::\n\n```{.r .cell-code}\nconfints <- bind_rows(confint_a, confint_b)\n\nggplot(confints, aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) + \n  geom_pointrange() + \n  geom_hline(yintercept = 0, lty = 2) + \n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nLooking at these results, if you were to ask me whether I would prefer to take Drug A or Drug B to lower my blood pressure, I'd say Drug A. Drug B is essentially detecting a clinically meaningless result. This came up a few times in the poll's replies, for example here:\n\n`{{% tweet \"1029588551162245120\" %}}`{=html}\n\nIf you require a little more convincing, let's introduce a third study that falls in the middle, **Study C**. This study has 100 participants and an effect size of -1. <label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">How am I picking these effect sizes? We know the p-value is a function of the effect size and standard error, so I've chosen each effect size such that the ratio of the effect size to standard error would be equal. For example, in the first case, this is proportional to $-2.5\\sqrt{16}$, and in the second case, this is proportional to $-0.1\\sqrt{10000}$, both of which equal -10.</span>\n\n\n::: {.cell}\n\n```{.r .cell-code}\npopulation <- population %>%\n  mutate(bp_drug_c = baseline_bp - 1 + rnorm(10^6))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(68)\n\n# sample 100 from the large population\nstudy_c <- population[sample(1:10^6, 100), ]\n\n# update so the first half receive drug C and the second receive standard of care\nstudy_c <- study_c %>%\n  mutate(drug_c = case_when(\n    row_number() %in% 1:50 ~ 1,\n    TRUE ~ 0\n  ),\n  outcome = case_when(\n    drug_c == 1 ~ bp_drug_c,\n    TRUE ~ bp_soc\n  )) %>%\n  select(baseline_bp, outcome, drug_c)\n\nmod_c <- lm(outcome ~ baseline_bp + drug_c, data = study_c)\nconfint_c <- broom::tidy(mod_c) %>%\n  bind_cols(broom::confint_tidy(mod_c)) %>%\n  filter(term == \"drug_c\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: confint_tidy is now deprecated and will be removed\nfrom a future release of broom. Please use the applicable\nconfint method.\n```\n\n\n:::\n\n```{.r .cell-code}\nconfints <- bind_rows(confints, confint_c)\n\nggplot(confints, \n       aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) + \n  geom_pointrange() + \n  geom_hline(yintercept = 0, lty = 2) + \n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nAs expected, **Study C**, which has approximately the same p-value as **Study A** and **Study B**, has an effect size that falls between the two, since the sample size falls between as well. This phenomenon was also explored in Peter Freeman's [The role of P-values in analysing trial results](https://doi.org/10.1002/sim.4780121510), as pointed out by [Anupam Singh](https://twitter.com/@anupampom) in the replies.\n\n\n`{{% twitter \"1029620344410255360\" %}}`{=html}\n\nOkay, so now I've shown you why *I* would have chosen the small study in my poll. Note here the small study that I have simulated *is* truly better (in other words in the large population, Drug A really does have a much larger effect size when compared to B or C). If we were to sample 10,000 participants from the Drug A group from the large population, we would see the same effect size (-2.5) with a much tighter confidence interval; this isn't just a spurious result. \n\n## Same intervention, same population\n\nLet's now explore what would happen if we found the same p-value for the same intervention in the same population. I hadn't really considered this case when I was posting the poll, but I think it is an important one to discuss! It was brought up a few times in the replies, for example:\n\n`{{% twitter \"1029648273894727680\" %}}`{=html}\n\nLet's use my same population as above, but just examine Drug B. We have two studies, one with 16 participants analyzing the efficacy of Drug B, and one with 10,000 participants. The \"true\" underlying effect here is still -0.1, the p-value will be 0.0005, as we obtained in the section above. There are a few different ways I could imagine this happening. \n\n1. The small study has a design flaw that biases the result\n2. The small study gets a crazy large effect size by chance\n\nFor the purposes of this example, I am going to assume the study designs are the same (except for the sample size), and target the second cause. I can imagine there is _some_ sample of 16 from our large population of a million that would result in this spurious result. Let's see if we can find one.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseed <- 0\np_val <- 1\nwhile (p_val != 0.0005) {\n  seed <- seed + 1\n  set.seed(seed)\n  # sample 16 from the large population\n  study_b <- population[sample(1:10^6, 16), ]\n  \n  # update so the first half receive drug B and the second receive standard of care\n  study_b <- study_b %>%\n    mutate(drug_b = case_when(\n      row_number() %in% 1:8 ~ 1,\n      TRUE ~ 0\n    ),\n    outcome = case_when(\n      drug_b == 1 ~ bp_drug_b,\n      TRUE ~ bp_soc\n    )) %>%\n    select(baseline_bp, outcome, drug_b)\n  \n  # test association between drug B and blood pressure\n  p_val <- lm(outcome ~ baseline_bp + drug_b, data = study_b) %>%\n    broom::tidy() %>%\n    filter(term == \"drug_b\") %>%\n    pull(p.value) %>%\n    round(4)\n}\n```\n:::\n\n\nThis took a bit of time, but after 5124 tries, it did find a seed where it happens! \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(5125)\n# sample 16 from the large population\nstudy_b <- population[sample(1:10^6, 16), ]\n\n# update so the first half receive drug B and the second receive standard of care\nstudy_b <- study_b %>%\n  mutate(drug_b = case_when(\n    row_number() %in% 1:8 ~ 1,\n    TRUE ~ 0\n  ),\n  outcome = case_when(\n    drug_b == 1 ~ bp_drug_b,\n    TRUE ~ bp_soc\n  )) %>%\n  select(baseline_bp, outcome, drug_b)\n\n# test association between drug B and blood pressure\nlm(outcome ~ baseline_bp + drug_b, data = study_b) %>%\n  broom::tidy() %>%\n  filter(term == \"drug_b\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 5\n  term   estimate std.error statistic p.value\n  <chr>     <dbl>     <dbl>     <dbl>   <dbl>\n1 drug_b   -0.268     0.613    -0.437   0.669\n```\n\n\n:::\n:::\n\n\nHere, the effect size is super exaggerated. We know the _true_ effect size is -0.1, and yet here we are seeing an effect of -1.4. We are definitely seeing this by chance. Let's look at the distribution of p-values in this small sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_p_val <- function() {\n  study_b <- population[sample(1:10^6, 16), ]\n  \n  # update so the first half receive drug B and the second receive standard of care\n  study_b <- study_b %>%\n    mutate(drug_b = case_when(\n      row_number() %in% 1:8 ~ 1,\n      TRUE ~ 0\n    ),\n    outcome = case_when(\n      drug_b == 1 ~ bp_drug_b,\n      TRUE ~ bp_soc\n    )) %>%\n    select(baseline_bp, outcome, drug_b)\n  \n  # test association between drug B and blood pressure\n  lm(outcome ~ baseline_bp + drug_b, data = study_b) %>%\n    broom::tidy() %>%\n    filter(term == \"drug_b\") %>%\n    pull(p.value)\n}\np_vals <- tibble(\n  p_val = map_dbl(1:1000, ~get_p_val())\n)\nggplot(p_vals, aes(p_val)) + \n  geom_histogram(bins = 100) + \n  geom_vline(xintercept = 0.0005, color = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe red line denotes 0.0005. In this run of 1,000 simulations, we only had 0 where the p-value was less than 0.0005. \n\nAll this is to say that if I were to observe two studies _measuring the same intervention_ with the _same study design_, one was small and one was large, I would definitely trust the large study. Perhaps this is what three quarters of my twitter-sphere were thinking! This seems akin to Gelman's [\"What doesn't kill my significance makes it stronger\"](http://andrewgelman.com/2017/02/06/not-kill-statistical-significance-makes-stronger-fallacy/) fallacy, brought up in the replies:\n\n`{{% twitter \"1029777778806280193\" %}}`{=html}\n\nIf a small and large study are evaluating the exact same thing, but show different results, I would certainly trust the large study more.\n\n## Same intervention, different populations\n\n<label for=\"tufte-mn-\" class=\"margin-toggle\">&#8853;</label><input type=\"checkbox\" id=\"tufte-mn-\" class=\"margin-toggle\"><span class=\"marginnote\">PHEW this post seems really long, sorry, dear readers! Here is a cute gif of  a cat covered in chicks for your troubles. ![](https://media.giphy.com/media/aiOKZWnTwDdxC/giphy.gif)</span>This brings us to our final scenario, measuring the same intervention on different populations. This is another scenario that I didn't consider prior to posting this poll! A few people brought up the possibility by asking about the effect size, for example [Miles McBain](https://twitter.com/MilesMcBain) said:\n\n`{{% twitter \"1029555525648703489\" %}}`{=html}\n\nTo which I initially replied that I didn't think that was possible. [Nick](https://twitter.com/NicholasStrayer) then chimed in to set me straight, and after much back and forth I _finally_ understood how that was possible. Essentially if the larger study ALSO had a larger standard deviation, proportional to the ratio of the square root of the sample sizes, this would happen. Here I'm not sure exactly which I would find more convincing, as they are really looking at different things.\n\n## Wrap up\n\nSO there we have it! This poll generated a lot of fun discussion, please feel free to let me know if you think I've left off an important scenario to consider. There are so many more fun things to discuss when thinking about what makes studies \"convincing\". None of these simulations take into account systematic bias, for example, from which a large sample size will never save you! Perhaps that can be left for a post for another day on the trade-off between bias and power. After all of this, my real conclusions remain similar to how I felt previously, \n\nðŸ¤” p-values are hard  \nðŸ•¯ confidence intervals shed more light on the situation  \nðŸŽ‰ twitter is a fun way to spur good discussion!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}