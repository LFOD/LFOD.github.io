[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "We are Lucy and Nick, we have things to say and we think people should be tricked into reading them, thus we created this blog.\nNick is a sofware engineer at RStudio. He can be found online, on twitter, and on GitHub\nLucy is an assistant professor at Wake Forest University. She can be found online, on twitter, and on GitHub"
  },
  {
    "objectID": "posts/2017-07-18-the-making-of-we-r-ladies/index.html",
    "href": "posts/2017-07-18-the-making-of-we-r-ladies/index.html",
    "title": "The making of ‚ÄúWe R-Ladies‚Äù",
    "section": "",
    "text": "Last March Ma√´lle wrote a blog post ‚ÄúFaces of #rstats Twitter‚Äù, a great tutorial on scraping twitter photos and compiling them in a collage. This inspired a similar adventure, but for #RLadies!\nWe brainstormed different ways we could make this collage unique, but ultimilately landed on creating a mosaic with the R-Ladies logo.\nWe first wanted to use Twitter profile pics but even after launching a small campaign we realized we‚Äôd identify too few R-Ladies to fill a collage. Therefore we mixed two information sources:"
  },
  {
    "objectID": "posts/2017-07-18-the-making-of-we-r-ladies/index.html#pull-in-meetup-data",
    "href": "posts/2017-07-18-the-making-of-we-r-ladies/index.html#pull-in-meetup-data",
    "title": "The making of ‚ÄúWe R-Ladies‚Äù",
    "section": "Pull in Meetup data",
    "text": "Pull in Meetup data\nTo get all of our current Meetups, we scrape our R-Ladies GitHub data.\ndoc.raw &lt;- RCurl::getURL(\"https://raw.githubusercontent.com/rladies/starter-kit/master/Current-Chapters.md\")\nmeetups &lt;- stringr::str_match_all(doc.raw, \"www.meetup.com/(.*?)/\")[[1]][,2]\nmeetups &lt;- unique(meetups)\nWe use our meetupr package to pull profile pictures of all of our members.\n\n\nIf you would like to follow along, you can grab a Meetup API Key.\napi_key &lt;- \"YOUR_MEETUP_API_KEY\"\nHere are a few small functions to get and save the Meetup photos.\nget_photos &lt;- function(i,meetup){\n  members &lt;- try(get_members(meetup[i], api_key), silent = TRUE)\n  if(class(members)[1] != \"try-error\"){\n    members %&gt;%\n      purrr::map(.,\"photo\", .null = NA) %&gt;%\n      purrr::map_chr(., \"photo_link\", .null = NA)\n  }\n}\n\nsave_image &lt;- function(photo){\n  image &lt;- try(image_read(photo), silent = TRUE)\n  if(class(image)[1] != \"try-error\"){\n    image &lt;- image %&gt;%\n      image_scale(\"100x100!\") %&gt;%\n      image_write(\n        paste0(\n          \"meetup-data/\",\n          stringr::str_match(photo[1], \"member_(.*?)\\\\.\")[, 2],\n          \".jpg\"\n        ),\n        format = \"jpeg\"\n      )\n  }\n  \n}\nGrab the photos for each Meetup group.\npics &lt;- purrr::map(1:length(meetups),\n                       get_photos,\n                       meetup = meetups\n                       ) %&gt;%\n                       unlist()\nNow walk it out üíÉ.\npurrr::walk(pics, save_image)"
  },
  {
    "objectID": "posts/2017-07-18-the-making-of-we-r-ladies/index.html#remove-the-default-avatars",
    "href": "posts/2017-07-18-the-making-of-we-r-ladies/index.html#remove-the-default-avatars",
    "title": "The making of ‚ÄúWe R-Ladies‚Äù",
    "section": "Remove the default avatars",
    "text": "Remove the default avatars\nThere are a few photos that are the default avatar. These have a grey color with a white center. To check if this is the case, test whether the first column of pixels is fully grey.\n\n\nThis removed ~200 photos.\n\ndir &lt;- dir(\"meetup-data\", full.names = TRUE)\n\ncheck_image &lt;- function(pic) {\n  img &lt;- jpeg::readJPEG(pic)\n  if (any(dim(img) == 3)) {\n    if (length(unique(img[,1,1])) == 1) {\n      file.remove(pic)\n    }\n  }\n}\n\npurrr::walk(dir, check_image)"
  },
  {
    "objectID": "posts/2017-07-18-the-making-of-we-r-ladies/index.html#make-into-a-mosaic",
    "href": "posts/2017-07-18-the-making-of-we-r-ladies/index.html#make-into-a-mosaic",
    "title": "The making of ‚ÄúWe R-Ladies‚Äù",
    "section": "Make into a mosaic!",
    "text": "Make into a mosaic!\nWe use RsimMosaic, but had to tweak a few things (there were a few pictures causing errors making the whole thing break, so I added some error catching as well as the abilitity to specify the input tile size).\n## devtools::install_github(\"LucyMcGowan/RsimMosaic\")\nlibrary(\"RsimMosaicLDM\")\nset.seed(108)\ncomposeMosaicFromImageRandomOptim(\n  originalImageFileName = \"RLadies_tofill.jpeg\",\n  outputImageFileName = \"art-ladies.jpg\",\n  inputTileSize = 100,\n  imagesToUseInMosaic = \"meetup-data\", \n  removeTiles = TRUE,\n  fracLibSizeThreshold = 0.01\n  )\nWe used 9106 unique tiles to create the image using K-nearest neighbors to select the optimal match."
  },
  {
    "objectID": "posts/2017-07-18-the-making-of-we-r-ladies/index.html#pull-in-the-twitter-data",
    "href": "posts/2017-07-18-the-making-of-we-r-ladies/index.html#pull-in-the-twitter-data",
    "title": "The making of ‚ÄúWe R-Ladies‚Äù",
    "section": "Pull in the Twitter data",
    "text": "Pull in the Twitter data\ndat &lt;- search_tweets(q = '#RLadies', n = 1000, include_rts = FALSE)\ntweets &lt;- users_data(dat) %&gt;%\n  select(screen_name, profile_image_url)\nusers &lt;- search_users(\"#RLadies\", n = 1000) %&gt;%\n  select(screen_name, profile_image_url)\n\nprof &lt;- unique(rbind(tweets, users))\n\n\nWe removed a few accounts that we know are not actually (our version of) R-Ladies accounts.\nprof &lt;- prof[!(prof$screen_name %in%\n  c(\"Junior_RLadies\",\n  \"RLadies_LF\",\n  \"WomenRLadies\",\n  \"Rstn_RLadies13\",\n  \"RLadies\",\n  \"RnRladies\")\n  ), ]\nHere is a function to save the images. We are using the twitter images as the frame, so they are larger than the Meetup images.\nsave_image &lt;- function(df){\n  image &lt;- try(image_read(sub(\"_normal\", \"\", df$profile_image_url)), silent = TRUE)\n  if(class(image)[1] != \"try-error\"){\n    image %&gt;%\n      image_scale(\"500x500\") %&gt;%\n      image_write(paste0(\"tweet-data/\", df$screen_name, \".jpg\"), format = \"jpeg\")\n  }\n  \n}\n\nusers &lt;- split(prof, 1:nrow(prof))\npurrr::walk(users, save_image)\nNow let‚Äôs randomly read them in!\nset.seed(525)\npics &lt;- dir(\"tweet-data\", full.names = TRUE)\npics &lt;- sample(pics, length(pics))\n\n\nWe removed a few of the chapter accounts to ensure an even number.\n## we need a multiple of 4\nrladies_chapters &lt;- which(grepl(\"RLadies\", pics)) \npics &lt;- pics[-rladies_chapters[1:2]]"
  },
  {
    "objectID": "posts/2017-07-18-the-making-of-we-r-ladies/index.html#create-the-frame",
    "href": "posts/2017-07-18-the-making-of-we-r-ladies/index.html#create-the-frame",
    "title": "The making of ‚ÄúWe R-Ladies‚Äù",
    "section": "Create the frame",
    "text": "Create the frame\nCreate the top of the frame üéâ.\nlength_size &lt;- length(pics)/4\npics[1:length_size] %&gt;%\n  image_read() %&gt;%\n  image_append(stack = FALSE) %&gt;%\n  image_write(\"frame/top.jpg\")\nCreate the bottom of the frame üëá.\npics[(length_size + 1):(length_size * 2)] %&gt;%\n  image_read() %&gt;%\n  image_append(stack = FALSE) %&gt;%\n  image_write(\"frame/bottom.jpg\")\nCreate the left side of the frame üëà.\npics[(length_size * 2 + 1):(length_size * 3)] %&gt;%\n  image_read() %&gt;%\n  image_append(stack = TRUE) %&gt;%\n  image_write(\"frame/left.jpg\")\nCreate the right side of the frame üëâ.\npics[(length_size * 3 + 1):length(pics)] %&gt;%\n  image_read() %&gt;%\n  image_append(stack = TRUE) %&gt;%\n  image_write(\"frame/right.jpg\")\nWe add small R-Ladies logos to the sides of the frame!\nlogo_url &lt;- \"https://raw.githubusercontent.com/rladies/starter-kit/master/logo/R-LadiesGlobal_RBG_online_LogoOnly.png\"\n\nimage_read(logo_url) %&gt;%\n  image_scale(\"500x500+0+0\") %&gt;%\n  image_write(\"frame/logo_small.jpg\")\n\nht &lt;- image_read(\"art-ladies.jpg\") %&gt;% \n  image_info %&gt;% \n  select(height)\n  \ndims &lt;- paste0(ht, \"x\", ht, \"+0+0\")\nPut it all together! üé®\nc(\"frame/bottom.jpg\") %&gt;%\n  image_read() %&gt;%\n  image_append(stack = FALSE) %&gt;%\n  image_scale(geometry = dims) %&gt;%\n  image_write(\"frame/good_bottom.jpg\")\n\nc(\"frame/top.jpg\") %&gt;%\n  image_read() %&gt;%\n  image_append(stack = FALSE) %&gt;%\n  image_scale(geometry = dims) %&gt;%\n  image_write(\"frame/good_top.jpg\")\n\nc(\"frame/good_top.jpg\", \"art-ladies.jpg\",\n  \"frame/good_bottom.jpg\") %&gt;%\n  image_read() %&gt;%\n  image_append(stack = TRUE) %&gt;%\n  image_write(\"test_topbottom.jpg\")\n\nht &lt;- image_read(\"test_topbottom.jpg\") %&gt;% \n  image_info %&gt;% \n  select(height)\n  \ndims &lt;- paste0(ht, \"x\", ht, \"+0+0\")\n\nc(\"frame/logo_small.jpg\", \n  \"frame/left.jpg\",\n  \"frame/logo_small.jpg\") %&gt;%\n  image_read() %&gt;%\n  image_append(stack = TRUE) %&gt;%\n  image_scale(geometry = dims) %&gt;%\n  image_write(\"frame/good_left.jpg\")\n\nc(\"frame/logo_small.jpg\", \n  \"frame/right.jpg\",\n  \"frame/logo_small.jpg\") %&gt;%\n  image_read() %&gt;%\n  image_append(stack = TRUE) %&gt;%\n  image_scale(geometry = dims) %&gt;%\n  image_write(\"frame/good_right.jpg\")\n\nc(\"frame/good_left.jpg\", \"test_topbottom.jpg\",\n  \"frame/good_right.jpg\") %&gt;%\n  image_read() %&gt;%\n  image_append(stack = FALSE) %&gt;%\n  image_write(\"we-r-ladies.png\")\n  \nAttending JSM? Come see our work on display at the Data Art Show! And since participating makes us real data artists now, find below the ‚Äúartsy‚Äù description of our work!\nR-Ladies is a global organization focused on improving gender diversity in the R community. R is an open-source statistics programming language. We used an R-Ladies developed package, meetupr to pull several thousand profile pictures from 33 R-Ladies meetups. We then used k-nearest neighbors to optimally match each image to a pixel of the R logo. The frame is comprised of Twitter profile pictures of individuals and local chapters who used the hashtag #RLadies in the days prior to our data collection. Pictures were assembled thanks to rOpenSci‚Äôs magick package. This work symbolizes how the global R-Ladies community is composed of diverse individuals attending events and communicating about the organization, whose strengths add up and create a strong grassroots organization. The whole creation process was carried out using R, showing the diversity of the language itself, and making the most of a tool, meetupr, created by members of the R-Ladies community, further underlining the synergy we want to convey. All in all, We R-Ladies is a tribute to diversity in the R community."
  },
  {
    "objectID": "posts/2017-07-24-twitter-trees/index.html",
    "href": "posts/2017-07-24-twitter-trees/index.html",
    "title": "Twitter trees",
    "section": "",
    "text": "A little over a week ago, Hilary Parker tweeted out a poll about sending calendar invites that generated quite the repartee.\nIt was quite popular üíÖ ‚Äì so much so that I couldn‚Äôt possible keep up with all of the replies! I personally am quite dependent on my calender, but I was intrigued to see what others had to say. This inspired me to try out some swanky R packages for visualizing trees üå≥."
  },
  {
    "objectID": "posts/2017-07-24-twitter-trees/index.html#load-all-the-s",
    "href": "posts/2017-07-24-twitter-trees/index.html#load-all-the-s",
    "title": "Twitter trees",
    "section": "Load all the üì¶s",
    "text": "Load all the üì¶s\n\nlibrary(\"dplyr\")\nlibrary(\"ggraph\")\nlibrary(\"igraph\")\nlibrary(\"ggiraph\")"
  },
  {
    "objectID": "posts/2017-07-24-twitter-trees/index.html#grab-the-tweets",
    "href": "posts/2017-07-24-twitter-trees/index.html#grab-the-tweets",
    "title": "Twitter trees",
    "section": "Grab the tweets",
    "text": "Grab the tweets\n‚äïWe‚Äôre using Mike Kearney‚Äôs glorious rtweet package!\nI tried a few iterations, but eventually ended up searching for all tweets since the tweet just prior to Hilary‚Äôs calendar inquiry, setting q = @hspter OR to:hspter OR hspter. Allegedly to:hspter should have done the trick, but that seemed to be missing some ü§∑‚Äç‚ôÄ.\n\n\nYou can get twitter status ids from their urls like so gsub(\".*status/\", \"\", tweet_url)\n\ntweets &lt;- rtweet::search_tweets(q = \"@hspter OR to:hspter OR hspter\",\n                                sinceId = 887022381809639424,\n                                n = 2000, \n                                include_rts = FALSE)\n\ntweets &lt;- tweets %&gt;%\n  distinct()\n\nI did some adventurous iterating to grab all the correct tweets by essentially grabbing all ids that replied to the original tweet, then all ids that replied to those replies, and so on, subsetting to only tweets involved in this particular tree.\n‚äï\n\nid &lt;- \"887493806735597568\"\ndiff &lt;- 1\nwhile (diff != 0) {\nid_next &lt;- tweets %&gt;%\n  filter(in_reply_to_status_status_id %in% id) %&gt;%\n  pull(status_id)\nid_new &lt;- unique(c(id, id_next))\ndiff &lt;- length(id_new) - length(id)\nid &lt;- id_new\n}\n\nall_replies &lt;- tweets %&gt;% \n  filter(in_reply_to_status_status_id %in% id)\n\n‚äïI spent 5 minutes TOO LONG trying to decide how to spell replyee, by which I mean ‚Äúone who is replied to‚Äù."
  },
  {
    "objectID": "posts/2017-07-24-twitter-trees/index.html#pull-the-replyee-and-replier-text",
    "href": "posts/2017-07-24-twitter-trees/index.html#pull-the-replyee-and-replier-text",
    "title": "Twitter trees",
    "section": "Pull the replyee and replier text",
    "text": "Pull the replyee and replier text\n\nfrom_text &lt;- all_replies %&gt;%\n  select(in_reply_to_status_status_id) %&gt;%\n  left_join(all_replies, c(\"in_reply_to_status_status_id\" = \"status_id\")) %&gt;%\n  select(screen_name, text)\n\nSet the text for the infamous tweet_0.\n\ntweet_0 &lt;- \"@hspter: Do you like getting google calendar invites from your friends for lunches / coffees / etc.?\"\n\n\n\nFor some reason ggiraph gets very üò≠ about single quotes, so we must replace them.\n\nto_text &lt;- paste0(all_replies$screen_name, \": \", all_replies$text)\nto_text &lt;- gsub(\"'\", \"`\", to_text)\nfrom_text &lt;- paste0(from_text$screen_name, \": \", from_text$text)\nfrom_text &lt;- gsub(\"'\", \"`\", from_text)"
  },
  {
    "objectID": "posts/2017-07-24-twitter-trees/index.html#create-the-edges",
    "href": "posts/2017-07-24-twitter-trees/index.html#create-the-edges",
    "title": "Twitter trees",
    "section": "Create the edges",
    "text": "Create the edges\n\nedges &lt;- tibble::tibble(\n  from = from_text,\n  to = to_text\n) %&gt;%\n  mutate(from = ifelse(\n    from == \"NA: NA\",\n    tweet_0,\n    from)\n  )"
  },
  {
    "objectID": "posts/2017-07-24-twitter-trees/index.html#create-the-graph",
    "href": "posts/2017-07-24-twitter-trees/index.html#create-the-graph",
    "title": "Twitter trees",
    "section": "Create the graph",
    "text": "Create the graph\n\ngraph &lt;- graph_from_data_frame(edges, directed = TRUE)\nV(graph)$tooltip &lt;- V(graph)$name\n\nset.seed(525)\np &lt;- ggraph(graph, layout = \"nicely\") + \n  geom_edge_link() + \n  geom_point_interactive(aes(x, y, color = \"red\", alpha = 0.05, tooltip = tooltip)) +\n  theme_void() + \n  theme(legend.position = \"none\")\nggiraph(code = print(p),\n        width_svg = 10,\n        zoom_max = 4)\n\nFunction `ggiraph()` is replaced by `girafe()` and will be removed soon.\n\n\nWarning: Using the `size` aesthetic in this geom was deprecated in\nggplot2 3.4.0.\n‚Ñπ Please use `linewidth` in the `default_aes` field and\n  elsewhere instead.\n\n\n\n\n\n\n\nIsn‚Äôt that easier to navigate? I must admit I kind of love that it quite resembles a barely budding Cherry Blossom üíÉ."
  },
  {
    "objectID": "posts/2017-07-24-twitter-trees/index.html#p.s.-a-little-text-analysis",
    "href": "posts/2017-07-24-twitter-trees/index.html#p.s.-a-little-text-analysis",
    "title": "Twitter trees",
    "section": "P.S. A little text analysis",
    "text": "P.S. A little text analysis\nComing at you üî• from the fresh-off-the-press Text Mining with R, let‚Äôs do a little sentiment analysis to see how folks were feeling while replying to this curiosity.\n‚äïCheck out an example just like this and more examples on the Text Mining with R website\n\nlibrary(\"tidytext\")\n#this will drop links & symbols\ndrop_pattern &lt;- \"https://t.co/[A-Za-z\\\\d]+|http://[A-Za-z\\\\d]+|&amp;|&lt;|&gt;|RT|https|ht\"\nunnest_pattern &lt;- \"([^A-Za-z_\\\\d#@']|'(?![A-Za-z_\\\\d#@]))\"\n\nall_replies %&gt;% \n  mutate(text = stringr::str_replace_all(text, drop_pattern, \"\")) %&gt;%\n  unnest_tokens(word, \n                text, \n                token = \"regex\", \n                pattern = unnest_pattern) %&gt;%\n  inner_join(get_sentiments(\"bing\"), by = \"word\") %&gt;%\n  count(word, sentiment, sort = TRUE) %&gt;%\n  ungroup() %&gt;%\n  group_by(sentiment) %&gt;%\n  top_n(5) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(word, n, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(y = \"Contribution to sentiment\",\n       x = NULL) +\n  coord_flip()\n\n\n\n\nIt looks like the poll represents the people! There seems to generally be more positive sentiment, with some strongly worded negative sentiment snuck in there üòâ."
  },
  {
    "objectID": "posts/2020-04-13-prevalence-and-probability-of-covid/index.html",
    "href": "posts/2020-04-13-prevalence-and-probability-of-covid/index.html",
    "title": "Prevalence of a disease plays an important role in your probability of having COVID-19 given you tested positive",
    "section": "",
    "text": "The prevalence of a disease plays an important role in your probability of having it given you test positive.\nThis is less relevant for testing positive for SARS-CoV-2 infection since RT-PCR rarely (if ever) has false positives. However, this may be relevant for antibody tests, which are less precise. Here is a quick explainer for how this works.\n\n\nThe numbers in this explainer are made up. Are some real numbers for the sensitivity and specificity (as it stands today): r tufte::margin_note(\"1. https://www.mayoclinicproceedings.org/article/S0025-6196(20)30365-7/fulltext &lt;br&gt; 2. https://www.fda.gov/media/136151/download &lt;br&gt; 3. https://www.centerforhealthsecurity.org/resources/COVID-19/serology/Serology-based-tests-for-COVID-19.html\")\nRT-PCR:\nr emo::ji(\"point_right\") sensitivity: can be as low as 70%, maybe closer to 90%¬π\nr emo::ji(\"point_right\") specificity: ~100%¬≤\nAntibody test (depends on the test, the one approved in US¬≥):\nr emo::ji(\"point_right\") sensitivity: 93.8%\nr emo::ji(\"point_right\") specificity: 95.6%"
  },
  {
    "objectID": "posts/2017-06-03-runconf17-an-analysis-of-emoji-use/index.html",
    "href": "posts/2017-06-03-runconf17-an-analysis-of-emoji-use/index.html",
    "title": "runconf17, an analysis of emoji use",
    "section": "",
    "text": "I had such a delightful time at rOpenSci‚Äôs unconference last week.\nNot only was it extremely productive, but in between the crazy productivity was some epic community building.\nStefanie kicked the conference off with ice breakers, where we explored topics ranging from #rcatladies & #rdogfellas to impostor syndrome. It was an excellent way to get conversations starting!"
  },
  {
    "objectID": "posts/2017-06-03-runconf17-an-analysis-of-emoji-use/index.html#work",
    "href": "posts/2017-06-03-runconf17-an-analysis-of-emoji-use/index.html#work",
    "title": "runconf17, an analysis of emoji use",
    "section": "work",
    "text": "work\nKarthik and I worked on two packages:\narresteddev: a package for when your development is going awry! ::: column-margin Mostly, this was a good excuse to look up Arrested Development gifs, which, we established, is pronounced with a g like giraffe.  ::: Includes functions such as lmgtfy(), that will seamlessly google your last error message, David Robinson‚Äôs tracestack() that will query your last error message on Stack Overflow, and squirrel(), a function that will randomly send you to a distracting website - for when things are really going poorly üíÅ.\nponyexpress: a package for automating speedy emails from R - copy and paste neigh more üê¥. This package allows you to send templated emails to a list of contacts. Great for conferences, birthday parties, or karaoke invitations."
  },
  {
    "objectID": "posts/2017-06-03-runconf17-an-analysis-of-emoji-use/index.html#play",
    "href": "posts/2017-06-03-runconf17-an-analysis-of-emoji-use/index.html#play",
    "title": "runconf17, an analysis of emoji use",
    "section": "play",
    "text": "play\nBetween our package building, there were SO many opportunities to get to know some of the most talented people.\n\n\n&lt;img src = ‚Äúhttps://github.com/LFOD/real-blog/raw/master/static/images/jenny_lucy.jpg‚Äù‚Äú&gt;  Jenny & I enthusiastically working on googledrive.\nMore than anything, this was an excellent opportunity to feel like a part of a community ‚Äì and a community that certainly extends beyond the people that attended the unconference! There were so many people following along, tweeting along, and assisting along the way.\na few highlights:\n\nüç® ice cream outings\nüé§ karaoke adventures\n\nüç∏ happy hours (complete with R-themed drinks)\nüí™ Karthik attempting to lick his elbow"
  },
  {
    "objectID": "posts/2017-06-03-runconf17-an-analysis-of-emoji-use/index.html#analysis",
    "href": "posts/2017-06-03-runconf17-an-analysis-of-emoji-use/index.html#analysis",
    "title": "runconf17, an analysis of emoji use",
    "section": "analysis",
    "text": "analysis\n\n\nNote: this is not particularly statistically rigorous, but it is VERY fun.\nIn an effort to stay on brand, I decided to do a small analysis of the tweets that came out of #runconf17. I designed a small study:\n\npulled all tweets (excluding retweets) using the hashtag #runconf17 between May 24th and May 30th\nalso pulled all tweets (excluding retweets) using the hashtag #rstats during the same time period\n\n Question: Are twitter users who used the #runconf17 hashtag more likely to use emojis than those who only tweeted with the #rstats hashtag during the same time period? \n\n\nI used the rtweet package to pull the tweets, dplyr and fuzzyjoin to wrangle the data a bit, and rms to analyze it.\n\nlibrary(\"rtweet\")\nlibrary(\"dplyr\")\nlibrary(\"fuzzyjoin\")\nlibrary(\"rms\")\n\n\nrunconf &lt;- search_tweets(q = \"#runconf17 AND since:2017-05-23 AND until:2017-05-31\",\n                         n = 1e4, \n                         include_rts = FALSE)\n\nrstats &lt;- search_tweets(q = \"#rstats AND since:2017-05-23 AND until:2017-05-31\",\n                        n = 1e4,\n                        include_rts = FALSE)\n\n\n\nThe emoji dictionary was discovered by the lovely Ma√´lle!\nAfter pulling in the tweets, I categorized tweeters as either using the #runconf17 hashtag during the week or not. I then merged the tweets with an emoji dictionary, and grouped by tweeter. If the tweeter used an emoji at any point during the week, they were categorized as an emoji-user, if not, they were sad (jk, there is room for all here!).\n\n## create variable for whether tweeted about runconf\nrunconf$runconf &lt;- \"yes\"\n\nrstats &lt;- rstats %&gt;%\n  mutate(runconf = ifelse(screen_name %in% runconf$screen_name, \"yes\", \"no\"))\n\n## load in the emoji dictionary\ndico &lt;- readr::read_csv2(\"https://raw.githubusercontent.com/today-is-a-good-day/emojis/master/emDict.csv\")\n\n‚Ñπ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 842 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \";\"\nchr (4): Description, Native, Bytes, R-encoding\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n## combine datasets, keep only unique tweets\ndata &lt;- bind_rows(runconf, rstats) %&gt;%\n  distinct(text, .keep_all = TRUE)\n\n## summarize by user, did they tweet about runconf in the past week \n## & did they use an emoji in the past week?\nused_emoji &lt;- regex_left_join(data, dico, by = c(text = \"Native\")) %&gt;%\n  select(screen_name, \n         text,\n         runconf,\n         emoji = Native) %&gt;%\n  group_by(screen_name) %&gt;%\n  mutate(tot_emoji = sum(!is.na(emoji)),\n         used_emoji = ifelse(tot_emoji &gt; 0, \"yes\", \"no\"),\n         tot_tweets = n_distinct(text)) %&gt;%\n  distinct(screen_name, .keep_all = TRUE)"
  },
  {
    "objectID": "posts/2017-06-03-runconf17-an-analysis-of-emoji-use/index.html#results",
    "href": "posts/2017-06-03-runconf17-an-analysis-of-emoji-use/index.html#results",
    "title": "runconf17, an analysis of emoji use",
    "section": "results",
    "text": "results\nWe had 526 tweeters that just used the #rstats hashtag, and 107 that tweeted with the #runconf17 hashtag. ::: column-margin THESE ARE MY PEOPLE üôå ::: Among the #rstats tweeters, 5.9% used at least one emoji in their tweets, whereas among #runconf17 tweeters, 25.2% used emojis!\n\nused_emoji %&gt;%\n  group_by(`tweeted #runconf` = runconf, `used emoji` = used_emoji) %&gt;%\n  tally() %&gt;%\n  mutate(`%` = 100*prop.table(n)) %&gt;%\n  knitr::kable(digits = 1)\n\n\n\n\ntweeted #runconf\nused emoji\nn\n%\n\n\n\n\nno\nno\n495\n94.1\n\n\nno\nyes\n31\n5.9\n\n\nyes\nno\n80\n74.8\n\n\nyes\nyes\n27\n25.2\n\n\n\n\n\nAlright, that looks pretty promising, but let‚Äôs get some confidence intervals. It‚Äôs time to model it! üíÉ\n\n## modeling time!\ndd &lt;- datadist(used_emoji)\noptions(datadist = \"dd\")\n\nlrm(used_emoji~runconf, data = used_emoji) %&gt;%\n  summary() %&gt;%\n  html()\n\n\n\n\nEffects ‚ÄÉ‚ÄÉResponse: used_emoji\n\n\n\nLow\nHigh\nŒî\nEffect\nS.E.\nLower 0.95\nUpper 0.95\n\n\n\n\nrunconf --- yes:no\n1\n2\n\n1.684\n0.2895\n1.117\n2.252\n\n\n‚ÄÉOdds Ratio\n1\n2\n\n5.389\n\n3.056\n9.505\n\n\n\n\n\n\n\n \n Tweeting the #runconf17 hashtag seems undeniably associated with a higher odds of emoji use (OR: 5.4, 95% CI: 3.1, 9.5)."
  },
  {
    "objectID": "posts/2017-06-03-runconf17-an-analysis-of-emoji-use/index.html#most-popular-emojis",
    "href": "posts/2017-06-03-runconf17-an-analysis-of-emoji-use/index.html#most-popular-emojis",
    "title": "runconf17, an analysis of emoji use",
    "section": "most popular emojis",
    "text": "most popular emojis\nNow let‚Äôs checkout which emojis were most popular among #runconf17 tweeters. This time I‚Äôll allow for retweets üëØ\n\n\nFor this I used ggplot2, magick, and webshot\n\nlibrary(\"ggplot2\")\nlibrary(\"webshot\")\nlibrary(\"magick\")\n\n\nrunconf_emojis &lt;- search_tweets(q = \"#runconf17 AND since:2017-05-23 AND until:2017-05-31\",\n                                n = 1e4)\n\n\nemojis &lt;- regex_left_join(runconf_emojis, dico, by = c(text = \"Native\")) %&gt;%\n  group_by(Native) %&gt;%\n  filter(!is.na(Native)) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;%\n  head(15) %&gt;%\n  mutate(num = 1:15)\n\nThis (like many things I do) was very much inspired by Ma√´lle‚Äôs post.\n\nplot_emojis &lt;- function(limit) {\n  emojis_filter &lt;- emojis %&gt;%\n    filter(emojis$n &lt;= limit)\n  out_svg &lt;- paste0(\"file://emojis_\", limit,\".svg\")\n  out_png &lt;- paste0(\"emojis_\", limit, \".png\")\n  p &lt;- ggplot(emojis_filter, aes(num, n)) + \n    geom_col() + \n    xlim(c(0,16)) +\n    geom_text(aes(x = num, \n                  y = n + 1,\n                  label = Native), size = 5)  +\n    theme(axis.text.y=element_blank(),\n          axis.ticks=element_blank(),\n          legend.position=\"none\") + \n    ylim(c(0, max(emojis$n) + 10)) +\n    xlab(\"emoji\") + \n    ggtitle(\"#runconf17 emojis\") +\n    coord_flip() \n  print(p)\n  gridSVG::grid.export(out_svg)\n  webshot(out_svg,\n          out_png,\n          vwidth = 100,\n          vheight = 100,\n          zoom = 3)\n  out_png\n}\n\nNow let‚Äôs make them into a gif!\n\nout_png &lt;- purrr::map_chr(emojis$n, plot_emojis)\n\npurrr::map(unique(rev(out_png)), image_read) %&gt;%\n  image_join() %&gt;%\n  image_animate(fps=1) %&gt;%\n  image_write(\"runconf_emojis.gif\")\n\n \n\n\nPhew, the üêî\nThe purple heart seems to be the most popular emoji, which makes sense given 25% of us were #RLadies! I think it‚Äôs a credit to the awesome geographic diversity that we have two different globe emojis in our top 15!\nAll in all, it was an epic experience. Thank you so much to the conference organizers, attendees, and #runconf17 tweeters for such a delightful week!"
  },
  {
    "objectID": "posts/2018-01-22-a-set-seed-ggplot2-adventure/index.html",
    "href": "posts/2018-01-22-a-set-seed-ggplot2-adventure/index.html",
    "title": "A set.seed() + ggplot2 adventure",
    "section": "",
    "text": "Recently I tweeted a small piece of advice re: when to set a seed in your script:\n\n\n#rstats tip: always set.seed() AFTER loading ggplot2 pic.twitter.com/TLpdokbSlN\n\n‚Äî Lucy üåª (@LucyStats) January 19, 2018\n\n\nJenny pointed out that this may be blog post-worthy, so here we are!\n\nBackground\n‚äï A little bit about where this tweet came from: I was out to lunch with my friend Jonathan ‚Äì over our scrumptious Thai food, he mentioned he was having some simulation trouble. In particular, some of his simulations were breaking, but when he tried to run the script locally, everything seemed fine. He was using the same seed in both instances, and in fact was running the exact same script ‚Äì what could be causing this discrepancy! I recalled that ggplot2 would generate a random message about 10% the time, and asked whether he thought loading this package could somehow be affecting his simulation results. After some further investigation üïµÔ∏è‚Äç‚ôÇÔ∏è, it looked like indeed this may be the culprit!\n\n\nWhy did this happen?\nThe set.seed() function sets the starting number used to generate a sequence of random numbers ‚Äì it ensures that you get the same result if you start with that same seed each time you run the same process. For example, if I use the sample() function immediately after setting a seed, I will always get the same sample.\n\nset.seed(1)\nsample(3)\n\n[1] 1 2 3\n\n\n\nset.seed(1)\nsample(3)\n\n[1] 1 2 3\n\n\nIf I run sample() twice after setting a seed, however, I would not expect them to be the same. I‚Äôd expect the first result to match those above, and the second to be different.\n\nset.seed(1)\nsample(3)\n\n[1] 1 2 3\n\nsample(3)\n\n[1] 1 2 3\n\n\nThe second is different because I have already performed one random process, so now my starting point prior to running the latter sample() function is no longer 1.\nThere is a small function in ggplot2 that runs when the library is loaded for the first time.\n‚äïNote: This is a little different than this code looks now, it has been updated since this discussion began!\n\n.onAttach &lt;- function(...) {\n  if (!interactive() || stats::runif(1) &gt; 0.1) return()\n\n  tips &lt;- c(\n      \"RStudio Community is a great place to get help: https://community.rstudio.com/c/tidyverse.\",\n      \"Find out what's changed in ggplot2 at https://github.com/tidyverse/ggplot2/releases.\",\n      \"Use suppressPackageStartupMessages() to eliminate package startup messages.\",\n      \"Need help? Try Stackoverflow: https://stackoverflow.com/tags/ggplot2.\",\n      \"Need help getting started? Try the cookbook for R: http://www.cookbook-r.com/Graphs/\",\n      \"Want to understand how all the pieces fit together? See the R for Data Science book: http://r4ds.had.co.nz/\"\n    )\n\n  tip &lt;- sample(tips, 1)\n  packageStartupMessage(paste(strwrap(tip), collapse = \"\\n\"))\n}\n\nThis function has a sample() call, which will move the starting place of your random sequence of numbers. The main piece that caused Jonathan some üò´ is the !interactive() logic, which only runs the remainder of the code if the session is interactive. Another thing that can cause a bit of confusion is this .onAttach() function is only run the first time the library is loaded, so if I run what looks like the exact same code twice during the same session, I can get different results. For example,\n\n\nNote: If you are quite clever, you will notice that this is not an interactive document and therefore neither the first nor the second chunk should run .onAttach() ‚Äì this is true! I‚Äôve run them interactively and included the output for demonstration purposes üôä.\n\nset.seed(1)\nlibrary(ggplot2)\nsample(3)\n\n## [1] 2 3 1\n\nset.seed(1)\nlibrary(ggplot2)\nsample(3)\n\n[1] 1 2 3\n\n\nNotice the second chunk gave us the same result as above, but the first chunk was different. That is because the first chunk runs the .onAttach() function between when I set my seed and when I drew my sample.\n\n\nReproducibility crisis?\n‚äï It turns out this isn‚Äôt cause for concern re: reproducibility, thanks to Jim Hester‚Äôs new patch to ggplot2 phew! Prior to this update, non-interactive scripts will be reproducible (if always run non-interactively), however interactive scripts can cause some issues, as shown above. In general, I like to provide future Lucy with as much assistance as possible, so I will likely avoid setting seeds prior to loading packages on the off chance that it will make my debugging trickier in the future.\n\n\nWhat should I do?\n\nFor this particular issue, Jim Hester has already patched the development version of ggplot2 to preserve your seed üå± if you set it before loading ggplot2 (he did it with a slick function, withr::with_preserve_seed, love it!), so you can go download the development version and set your seed wherever you please.\n\nIn general, it seems somewhat prudent to set your seeds after loading packages üì¶, as it can be tricky to know exactly what is going on under the hood. The wise R-Lady Steph Locke advised in a conversation on this topic to generally try to set seeds as close to the random component as possible to avoid any confusion ‚Äì this seems like easy and good advice to follow üëØ!\n\n\n\nWhat have we learned?\n\nI‚Äôve learned a lot just from thinking through where to set different parts of my code and how that can affect things downstream\n\nWe now know a bit more about how seeds work!\n\nWe‚Äôve learned about the withr::with_preserve_seed() function üéâ\n\nWe‚Äôve seen the potential consequences of changing the global state in a package ‚Äì Jenny recently added this as an issue to discuss in a future version of r-pkgs, which she eloquently summarizes as ‚Äúdon‚Äôt touch things that don‚Äôt belong to you and if you have to, you need to be super careful to wipe all your sticky fingerprints off everything‚Äù\n\nThe #rstats community is so helpful and responsive! A small debugging situation led to lots of helpful advice and a quick fix from Jim üë∑"
  },
  {
    "objectID": "posts/2017-01-27-yoga-for-modeling/index.html",
    "href": "posts/2017-01-27-yoga-for-modeling/index.html",
    "title": "Yoga for modeling",
    "section": "",
    "text": "A New Year‚Äôs resolution for all of our models: get more flexible!\nAs an aside, I‚Äôm better at implementing yoga for my models than yoga for myself, most of the time I end up like this:\nAnyways, let‚Äôs make our models flexible! By flexible, we mean let‚Äôs be more intentional about fitting nonlinear parametric models."
  },
  {
    "objectID": "posts/2017-01-27-yoga-for-modeling/index.html#what-do-you-mean-by-nonlinear-modeling",
    "href": "posts/2017-01-27-yoga-for-modeling/index.html#what-do-you-mean-by-nonlinear-modeling",
    "title": "Yoga for modeling",
    "section": "What do you mean by nonlinear modeling",
    "text": "What do you mean by nonlinear modeling\nBy nonlinear modeling, we mean fitting parametric models with nonlinear terms. In particular, Professor Harrell suggests restricted cubic splines (more on this below). So by this definition, you could be fitting a ‚Äúlinear‚Äù model (as on ordinary least squares), with nonlinear terms. We brought up in class that this can cause some confusion, and setting on calling it ‚ÄúGaussian regression‚Äù So you heard it here first, folks, from now on, refer to your ordinary least squares models as ‚ÄúGaussian‚Äù.\n\nUpdate: Professor Harrell proofed this and suggested  ‚Äúwhat about just calling it OLS?‚Äù, so nevermind on  Gaussian, it had a good run‚Ä¶\n\nI spent a really long time trying to think up a whitty pun/slogan for this name change, alas, all you get is this:\n\nIf you have a particulary clever slogan, send it to me.\n\nUpdate 2: Actually! We‚Äôre putting it to a vote.  Gaussian may have another chance.  Will update in a further post.\n\nTo do this nonlinear modeling, restricted cubic splines are suggested because\n\nthey allow for flexibility without costing too many degrees of freedom (it only costs \\(k-1\\) degrees of freedom, here \\(k\\) is the total number of knots) for comparison, unrestricted cubic splines require \\(k+3\\) parameters to be estimated\nthey force linearity in the tails (that is before the first knot and after the last knot) this is good because cubic spline functions can have weird behavior in the tails\nregression coeffcients can be estimated using standard techniques\nthere are easy functions in R to do it Harrell‚Äôs rms package makes restricted cubic spines very easy to implement. Simply wrap your predictor in the rcs() function within your model statement. For example, if I were to fit a Gaussian model (see what I did there üòâ), I would run the following code to fit x flexibly with 4 knots:\n\n\nlibrary(rms)\nk &lt;- 4\nols(y ~ rcs(x, k))"
  },
  {
    "objectID": "posts/2017-01-27-yoga-for-modeling/index.html#why-should-i-care-about-nonlinear-modeling",
    "href": "posts/2017-01-27-yoga-for-modeling/index.html#why-should-i-care-about-nonlinear-modeling",
    "title": "Yoga for modeling",
    "section": "Why should I care about nonlinear modeling",
    "text": "Why should I care about nonlinear modeling\nFitting flexible models is an excellent way to avoid having to test a whole bunch of assumptions that are hard to prove/disprove! Additionally, once you‚Äôve fit a model, changing it based on ‚Äúmodel checking‚Äù procedures has the danger of inflating your type 1 error üò±. We discuss an excellent paper by Grambsch and O‚ÄôBrien demonstrating that testing for nonlinear effects & then proceeding to drop from from the model if the test is not significant has real implications for the type 1 error. Might as well just model everything flexibly from the get-go! But what about my degrees of freedom? ah yes, the next section is just for you!"
  },
  {
    "objectID": "posts/2017-01-27-yoga-for-modeling/index.html#what-if-i-dont-have-enough-degrees-of-freedom-for-nonlinear-modeling",
    "href": "posts/2017-01-27-yoga-for-modeling/index.html#what-if-i-dont-have-enough-degrees-of-freedom-for-nonlinear-modeling",
    "title": "Yoga for modeling",
    "section": "What if I don‚Äôt have enough degrees of freedom for nonlinear modeling",
    "text": "What if I don‚Äôt have enough degrees of freedom for nonlinear modeling\nIn predictive modeling, we are always concerned with overfitting, or creating models that describe the population we‚Äôve sampled very well, but are not generalizable. In the extreme case, you can imagine if you fit a model with 50 participants and included 50 covariates, one for each participant, you could perfectly predict any outcome (but that would be a pretty silly model). In order to avoid overfitting disasters, there are useful rules of thumb we try to follow. In general, we try to fit models that have around \\(m/15\\) covariates, where \\(m\\) varies depending on the type of model in the following manner (for those following along, this is on page 74 of Regression Modeling Strategies).\n\n\n\nResponse\nm\n\n\n\n\ncontinuous\nthe total sample size\n\n\nbinary\n# of events\n\n\nsurvival\n# of failures\n\n\n\nNow that we‚Äôve convinced you that degrees of freedom matter, I‚Äôm sure you‚Äôre thinking ‚Äúbut nonlinear terms add degrees of freedom!‚Äù True. Restricted cubic splines are a bit less aggressive that regular cubic splines in terms of degrees of freedom usage, but even still sometimes that is not enough. In those cases, we need to determine how to best utilize our degrees of freedom. Professor Harrell recommends the following (page 67 of RMS):\n\nLet all continuous predictors be represented as restricted cubic splines\nLet all categorical predictors retain their original categories except for pooling of very low prevalence categories (e.g., ones containing &lt; 6 observations).\nFit this general main effects model.\nCompute the partial \\(\\chi^2\\) statistic for testing the association of each predictor with the response, adjusted for all other predictors.\nSort the partial association statistics in descending order.\nAssign more degrees of freedom for variables high on the list (ie allow for nonlinear terms/more knots for highly ranked covariates)\n\n VERY IMPORTANT: Make certain that tests of nonlinearity are not revealed as this would bias the analyst.\nHere is a little snippet of R code to do just that in a logistic regression case. We are using the support2 dataset as an example.\n\nlibrary(rms)\ngetHdata(support2)\nf &lt;- lrm(hospdead ~ rcs(age,5) + rcs(temp, 5) + rcs(hrt, 5) + rcs(pafi, 5) + edu + sex, data = support2)\nplot(anova(f))\n\n\n\n\nAnd there you have it.\n meme cred: Nick Strayer"
  },
  {
    "objectID": "posts/2017-01-27-yoga-for-modeling/index.html#take-away",
    "href": "posts/2017-01-27-yoga-for-modeling/index.html#take-away",
    "title": "Yoga for modeling",
    "section": "Take Away",
    "text": "Take Away\n\nAlways fit flexible models using restricted cubic splines\nNever test for linearity & proceed to remove a nonlinear term based on the result\nIf necessary be choosy about where to spend degrees of freedom"
  },
  {
    "objectID": "posts/2017-07-13-tuftesque/index.html",
    "href": "posts/2017-07-13-tuftesque/index.html",
    "title": "Introducing the tuftesque blogdown theme",
    "section": "",
    "text": "This post will serve as a quick tutorial getting you from nothing to a customized blogdown blog using the theme built for this blog: tuftesque.\nThe tuftesque theme gives your blog a ‚Ä¶tufte-esque‚Ä¶ style that eschews some of the choices of the original and which I‚Äôve added a few splashes of my personal style. The main title page is a single column and has an easily customizable banner where as the article view is the two panel view (for large screens) to allow for sidenotes. In addition you can add little author bios at the end which is especially useful if you have more than one author on your blog.\nIt doesn‚Äôt cover the more standard details such as writing new posts as I feel those have been covered much better than I could here."
  },
  {
    "objectID": "posts/2017-07-13-tuftesque/index.html#getting-blogdown",
    "href": "posts/2017-07-13-tuftesque/index.html#getting-blogdown",
    "title": "Introducing the tuftesque blogdown theme",
    "section": "Getting Blogdown",
    "text": "Getting Blogdown\nIf you already have a blogdown site than you can skip to the next step. If you do not, open R inside a new fresh directory and run the following commands:\n\n\nR Console\n\n#downloads the blogdown package\ndevtools::install_github(\"rstudio/blogdown\")\n\n#initialize a new blog\nblogdown::new_site()\n\nBoom, now you have your very own blog! Wouldn‚Äôt it be nice if everything in life was this easy?\n\n\n\n  \n\n\nThis is what your site should look like at this point."
  },
  {
    "objectID": "posts/2017-07-13-tuftesque/index.html#adding-tuftesque",
    "href": "posts/2017-07-13-tuftesque/index.html#adding-tuftesque",
    "title": "Introducing the tuftesque blogdown theme",
    "section": "Adding tuftesque",
    "text": "Adding tuftesque\nNow that you have your blogdown blog up and running switch back to your R console (you may have to press escape to kill the running blogdown server) and run the following command to download the tuftesque theme from github.\n\n\nR Console\n\nblogdown::install_theme('nstrayer/tuftesque')\n\nAssuming this successfully worked (if it didn‚Äôt file an issue here) now run the command blogdown::serve_site() to restart the blogdown server and you should have your fancy new blog working!\n\n\n\n  \n\n\nAfter installing the tuftesque theme your blog should now look like this."
  },
  {
    "objectID": "posts/2017-07-13-tuftesque/index.html#side-notes",
    "href": "posts/2017-07-13-tuftesque/index.html#side-notes",
    "title": "Introducing the tuftesque blogdown theme",
    "section": "Side Notes:",
    "text": "Side Notes:\nWe all know why you‚Äôre here: the sidenotes. This is rather easy. Just make sure you have the tufte package installed and then, in an inline r chunk, wherever you want to put a side note call the following.\ntufte::margin_note(\"This is a margin note\")\nThe above produces the following: ‚äïThis is a margin note\nYou can place just about anything in this that you would normally put in markdown so go crazy."
  },
  {
    "objectID": "posts/2017-07-13-tuftesque/index.html#summary-text",
    "href": "posts/2017-07-13-tuftesque/index.html#summary-text",
    "title": "Introducing the tuftesque blogdown theme",
    "section": "Summary Text:",
    "text": "Summary Text:\nYou can place a description of a post on the home page to give your readers a little bit more info on what a post is about. To do this just add the field excerpt to your posts header:\n‚äïThis is the header of this article. The except shows up on the home page.\ntitle: \"Introducing the tuftesque blogdown theme\"\nauthor: \"Nick Strayer\"\ndate: '2017-07-13'\ncategories: [\"R\"]\ntags: [\"Blogdown\", \"regression\"]\nexcerpt: \"Our blog's theme started as the theme hugo-tufte but has sense been modified a lot. I've been asked to put it up as a theme for other to use, here it is!\""
  },
  {
    "objectID": "posts/2017-07-13-tuftesque/index.html#logo",
    "href": "posts/2017-07-13-tuftesque/index.html#logo",
    "title": "Introducing the tuftesque blogdown theme",
    "section": "Logo",
    "text": "Logo\nYou‚Äôre not a giant Tintin fan? Well lucky for you changes can be made to the theme to make it more ‚Äúyou‚Äù.\nYou will notice in the directory set up for you by blogdown there is a file called config.toml. Open this up real quick and locate the where it says [params.logo] (line 35 if a fresh blogdown install).\nUnder this you will see the parameter url. You can set this to be whatever your heart desires. Say you‚Äôre more of a parrot person, no problem.\n‚äï/config.toml\n[params]\n...\n  [params.logo]\n    url = \"http://bit.ly/2tNSEG4\"\n    width = 50\n    height = 50\n    alt = \"Logo\"\n\n\n\n  \n\n\nThat‚Äôs a pretty cool new logo."
  },
  {
    "objectID": "posts/2017-07-13-tuftesque/index.html#banner",
    "href": "posts/2017-07-13-tuftesque/index.html#banner",
    "title": "Introducing the tuftesque blogdown theme",
    "section": "Banner",
    "text": "Banner\nAgain, go into your config.toml, but this time you need to add a new key called [params.banner] and under that add a field url just like for the logo.\n‚äï/config.toml\n[params.logo]\n  url = \"http://bit.ly/2tNSEG4\"\n  width = 50\n  height = 50\n  alt = \"Logo\"\n  \n[params.banner]\n  url = \"http://bit.ly/2sV7B6p\"\n‚äïBanner Size: The banner image defaults to being a 3:1 ratio of width to height, so if your image is that size it will be fully shown. Otherwise some will get cut off.\n\n\n\n  \n\n\nA network diagram?! I like your style."
  },
  {
    "objectID": "posts/2017-07-13-tuftesque/index.html#adding-authors",
    "href": "posts/2017-07-13-tuftesque/index.html#adding-authors",
    "title": "Introducing the tuftesque blogdown theme",
    "section": "Adding authors",
    "text": "Adding authors\nIf you want to have your very own custom bio at the bottom of your articles, go into your blog‚Äôs data/ directory and add a file titled authors.toml. Inside this file you can fill in some information about the authors on your blog. This info will then show up automatically at the end of any article they write.\nIn this include the following info (replacing in the obvious places).\n‚äï/data/authors.toml\n‚äïIf you don‚Äôt have all of the fields no worries, just don‚Äôt enter them and they will automatically be hidden from the bio. \n[\"Party Parrot\"]\n  name =  \"Mr. Parrot\"\n  bio = \"This is my super awesome bio and stuff.\"\n  website =  \"http:/www.example.com\"\n  thumbnail = \"http://bit.ly/2sV7B6p\"\n  twitter = \"https://twitter.com/\"\n  github = \"https://github.com/nstrayer/tuftesque/\"\nMake sure that the key [\"Author Name\"] is exactly as the name appears in the author field of your post‚Äôs header.\n\n\n\n  \n\n\nAuthor bios automatically show up if the author‚Äôs info is in /data/authors.toml"
  },
  {
    "objectID": "posts/2017-07-13-tuftesque/index.html#beyond-the-basics",
    "href": "posts/2017-07-13-tuftesque/index.html#beyond-the-basics",
    "title": "Introducing the tuftesque blogdown theme",
    "section": "Beyond the basics",
    "text": "Beyond the basics\nIf the limited options included in the theme are not flexible enough for you and you feel comfortable with html and css you can easily extend any part of this theme. For instance, you may notice on our blog we have an interactive animation that plays instead of a banner image. This is accomplished by simply adding a file called banner.html (this is what it looks like) into the /layouts/partials/ directory of our blog. This works because the tuftesque theme has a file in its own directory structure with the same name (/themes/tuftesque/layouts/partials/banner.html). Hugo uses that file to draw the header section of the page, but not before checking in our blog‚Äôs own layouts/ directory for the file first. Since we have our own version of this file, that one gets used instead of the default banner image.\nSo if you want to extend tuftesque (or any other Hugo theme) you never have to muck around in the themes/ directory, you simply need to investigate what part of the theme is responsible for the view you want to change by looking at its structure (poke around the github page to see how tuftesque is laid out) and then replicating that file‚Äôs path in your blogs personal layouts/ directory and make changes to your hearts desire! This is how tuftesque was slowly built out."
  },
  {
    "objectID": "posts/2022-09-19-migrating-from-hugo-to-quarto/index.html",
    "href": "posts/2022-09-19-migrating-from-hugo-to-quarto/index.html",
    "title": "Migrating from Hugo to Quarto",
    "section": "",
    "text": "We have migrated our blog from Hugo to Quarto! Here are a few quick tips that made the transition a bit smoother."
  },
  {
    "objectID": "posts/2022-09-19-migrating-from-hugo-to-quarto/index.html#setting-up-a-quarto-website",
    "href": "posts/2022-09-19-migrating-from-hugo-to-quarto/index.html#setting-up-a-quarto-website",
    "title": "Migrating from Hugo to Quarto",
    "section": "1. Setting up a Quarto website",
    "text": "1. Setting up a Quarto website\nIt is super easy to set up a Quarto website. To get the basic template, you can run the following in your terminal:\nquarto create-project mysite --type website\nYou can find lots of details about how to customize your site in the Quarto Docs. The rest of this post will cover a few things that made the transition smooth for us."
  },
  {
    "objectID": "posts/2022-09-19-migrating-from-hugo-to-quarto/index.html#moving-.rmd-files-from-hugo-to-your-new-site",
    "href": "posts/2022-09-19-migrating-from-hugo-to-quarto/index.html#moving-.rmd-files-from-hugo-to-your-new-site",
    "title": "Migrating from Hugo to Quarto",
    "section": "2. Moving .Rmd files from Hugo to your new site",
    "text": "2. Moving .Rmd files from Hugo to your new site\nIn Hugo, my .Rmd files were in the following folder under the main project: content/post. These were often individual files, rather than nested in folders. For my Quarto site, I wanted them in a folder called post and I wanted each post to have it‚Äôs only folder with content in a file called index.qmd nested within. I wrote a quick R script to help me do this.\n\nlibrary(fs)\n\n# pull all .Rmd files from my blog\nfiles_rmd &lt;- list.files(\n  \"~/livefreeordichotomize/content/post\",\n  pattern = \"*.Rmd\")\n# remove the .Rmd for the folders\nfolders &lt;- gsub(\".Rmd\", \"\", files_rmd)\n\n# pull again, with full names\nfull_files &lt;- list.files(\n  \"~/livefreeordichotomize/content/post\",\n  pattern = \"*.Rmd\", \n  full.names = TRUE)\n\n\n# create folders\ndir_create(glue::glue(\"posts/{folders}\"))\n\n# copy the .Rmd files into a new folder, named according to the old file name\npurrr::walk2(\n  full_files, \n  folders, \n  ~file_copy(.x, glue::glue(\"posts/{.y}/index.qmd\")))"
  },
  {
    "objectID": "posts/2022-09-19-migrating-from-hugo-to-quarto/index.html#setting-up-a-404-pages-for-readers-with-old-links",
    "href": "posts/2022-09-19-migrating-from-hugo-to-quarto/index.html#setting-up-a-404-pages-for-readers-with-old-links",
    "title": "Migrating from Hugo to Quarto",
    "section": "3. Setting up a 404 pages for readers with old links",
    "text": "3. Setting up a 404 pages for readers with old links\nSince we have rerouted where many of the files are, I set up a 404 page that will allow readers to quickly find a post if they have an old link. To do this, I created a folder in the top of the project called 404.qmd containing the following:\n---\ntitle: Page Not Found\nlisting:\n  contents: posts\n  type: table\n  sort: \"date\"\n---\nThis will create a searchable table listing of all of the previous posts, allowing readers to quickly find the link they are looking for.\nUPDATE You can add the old links under an aliases parameter in each posts‚Äô YAML."
  },
  {
    "objectID": "posts/2022-09-19-migrating-from-hugo-to-quarto/index.html#comments-and-google-analytics",
    "href": "posts/2022-09-19-migrating-from-hugo-to-quarto/index.html#comments-and-google-analytics",
    "title": "Migrating from Hugo to Quarto",
    "section": "4. Comments and Google analytics",
    "text": "4. Comments and Google analytics\nIt is straightforward to incorporate comments on your blog ‚Äì our old site used utterance. Likewise, it is simple to add Google Analytics. To use this in Quarto, we can add the following to the website‚Äôs global yaml:\n\nwebsite:\n  comments: \n    utterances:\n      repo: lfod/lfod.github.io\n  google-analytics: \"UA------\"\n\nTo see our full setup, check out our GitHub repo: github.com/LFOD/LFOD.github.io\nHave other questions? Feel free to leave them in the comments!"
  },
  {
    "objectID": "posts/2019-11-14-pulling-co-authors-for-grant-docs/index.html",
    "href": "posts/2019-11-14-pulling-co-authors-for-grant-docs/index.html",
    "title": "Pulling co-authors for grant docs",
    "section": "",
    "text": "So I just submitted my first grant (yikes) and WOW are there a lot of little pieces! One of the little required pieces was a spreadsheet detailing all of my co-authors for the past 4 years with their affiliations üò±. This sounded like a total nightmare to compile ‚äïAnd I don‚Äôt have very many papers! I am on a few clinical papers that have a ton of co-authors. Sidenote to people who know grant things: Did I actually need to do this for every co-author? Please comment with your advise! and naturally I left it until the last second to do. I am mostly writing this post so I have a way to do this quickly documented for next time.\nIt turns out R has once again come to the rescue! There is a üí£ package, easyPubMed that made this easy peasy."
  },
  {
    "objectID": "posts/2019-11-14-pulling-co-authors-for-grant-docs/index.html#load-packages",
    "href": "posts/2019-11-14-pulling-co-authors-for-grant-docs/index.html#load-packages",
    "title": "Pulling co-authors for grant docs",
    "section": "Load packages",
    "text": "Load packages\n‚äïI‚Äôm loading four packages: easyPubMed, the trusty tidyverse üì¶, one of my all times favorites, glue, and the unfortunately named lubridate\n\nlibrary(easyPubMed) \nlibrary(tidyverse)\nlibrary(glue)\nlibrary(lubridate)"
  },
  {
    "objectID": "posts/2019-11-14-pulling-co-authors-for-grant-docs/index.html#pull-my-papers",
    "href": "posts/2019-11-14-pulling-co-authors-for-grant-docs/index.html#pull-my-papers",
    "title": "Pulling co-authors for grant docs",
    "section": "Pull my papers",
    "text": "Pull my papers\nHere I‚Äôm querying pubmed for the papers I‚Äôve authored. I‚Äôm creating a data frame that includes details about the papers (include all of the authors!).\n\nquery &lt;- \"Lucy D'Agostino McGowan[AU] OR LD McGowan[AU]\"\nget_pubmed_ids(query) %&gt;%\nfetch_pubmed_data(encoding = \"ASCII\") %&gt;%\ntable_articles_byAuth(included_authors = \"all\", \n                            max_chars = 100, \n                            autofill = TRUE) -&gt; my_papers"
  },
  {
    "objectID": "posts/2019-11-14-pulling-co-authors-for-grant-docs/index.html#clean-it-up",
    "href": "posts/2019-11-14-pulling-co-authors-for-grant-docs/index.html#clean-it-up",
    "title": "Pulling co-authors for grant docs",
    "section": "Clean it up",
    "text": "Clean it up\nUsing the glue package, I combine the first and lastname variables into the lastname, firstname form requested by the granting agency. Year is pulled in as a ??character?? variable, so we need to fix that. I also filter out myself and only keep the papers from the past four years. Finally I select just the co-author‚Äôs name and affiliation and filter out any duplicates.\n\nmy_papers %&gt;%\n  mutate(name = glue(\"{lastname}, {firstname}\"),\n         year = as.numeric((year))) %&gt;%\n  filter(!str_detect(name, \"D'Agostino\"), year &gt;= year(today()) - 4) %&gt;%\n  select(name, address) %&gt;%\n  distinct() -&gt; coauthors\n\nAnd there you have it! üéâ"
  },
  {
    "objectID": "posts/2017-11-12-thanksgiving-gantt-chart/index.html",
    "href": "posts/2017-11-12-thanksgiving-gantt-chart/index.html",
    "title": "Thanksgiving Gantt Chart",
    "section": "",
    "text": "Thanksgiving ü¶É is right around the corner üéâ ‚Äì this year we are hosting 17 people üò±.\nIf you too are hosting way more than your kitchen normally cooks for, perhaps this will be of interest! We decided to make a Google Sheet of the various dishes so we could plot out what will be cooking when."
  },
  {
    "objectID": "posts/2017-11-12-thanksgiving-gantt-chart/index.html#packages-well-use",
    "href": "posts/2017-11-12-thanksgiving-gantt-chart/index.html#packages-well-use",
    "title": "Thanksgiving Gantt Chart",
    "section": "Packages we‚Äôll use",
    "text": "Packages we‚Äôll use\n\nlibrary(googlesheets4)\nlibrary(lubridate)\nlibrary(plotly)"
  },
  {
    "objectID": "posts/2017-11-12-thanksgiving-gantt-chart/index.html#pull-in-the-data",
    "href": "posts/2017-11-12-thanksgiving-gantt-chart/index.html#pull-in-the-data",
    "title": "Thanksgiving Gantt Chart",
    "section": "Pull in the data",
    "text": "Pull in the data\nFirst we can pull the spreadsheet into R using the googlesheets package.\n\n## Read the sheet into R\ndishes_df &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1k-H3CjkQRQJv7Ni8SA9Ghse8Y-kuKSLVAV4R4w4xXRE/edit?usp=sharing\")"
  },
  {
    "objectID": "posts/2017-11-12-thanksgiving-gantt-chart/index.html#clean-up-a-bit",
    "href": "posts/2017-11-12-thanksgiving-gantt-chart/index.html#clean-up-a-bit",
    "title": "Thanksgiving Gantt Chart",
    "section": "Clean up a bit üõÄ",
    "text": "Clean up a bit üõÄ\n\ndishes_df$start &lt;- as_datetime(dishes_df$start,\n                                          tz = \"America/Chicago\")\ndishes_df$finish &lt;- as_datetime(dishes_df$finish,\n                                           tz = \"America/Chicago\")\ndishes_df$minutes &lt;- dishes_df$finish - dishes_df$start\n\n‚äïLet‚Äôs pick some lovely turkey-themed colors for our chart üåà brought to you by colour lovers.\n\ncols &lt;- c(\"#487878\", \"#783030\", \"#904830\", \"#A87860\", \"#D89048\")\ndishes_df$color  &lt;- factor(dishes_df$where, labels = cols)\n\n## Order for the chart\ndishes_df &lt;- dishes_df[order(dishes_df$start,\n                             decreasing = TRUE), ]"
  },
  {
    "objectID": "posts/2017-11-12-thanksgiving-gantt-chart/index.html#make-the-plot",
    "href": "posts/2017-11-12-thanksgiving-gantt-chart/index.html#make-the-plot",
    "title": "Thanksgiving Gantt Chart",
    "section": "Make the plot üíÉ",
    "text": "Make the plot üíÉ\n‚äïThis is inspired by a Plotly blog post!\n\np &lt;- plot_ly()\n\nfor (i in 1:nrow(dishes_df)) {\n  p &lt;- add_lines(p,\n                 x = c(dishes_df$start[i], dishes_df$finish[i]), \n                 y = c(i, i), \n                 line = list(color = dishes_df$color[i],\n                             width = 20),\n                 hoverinfo = \"text\",\n                 text = paste(\"Dish: \",\n                              dishes_df$dish[i],\n                              \"&lt;br&gt;\",\n                              \"Cook time: \",\n                              dishes_df$minutes[i],\n                              \"minutes&lt;br&gt;\",\n                              \"Where: \",\n                              dishes_df$where[i]),\n                 showlegend = FALSE\n  ) \n}\n\n‚äï\n\n## Add the dish names to the y-axis, remove grid\np &lt;- layout(p,\n            xaxis = list(showgrid = FALSE),\n            yaxis = list(showgrid = FALSE, \n                         tickmode = \"array\",\n                         tickvals = 1:nrow(dishes_df),\n                         ticktext = unique(dishes_df$dish)),\n            margin = list(l = 200, r = 50, b = 50, t = 50, pad = 4),\n            plot_bgcolor = \"#EBE5E5\",\n            paper_bgcolor = \"#EBE5E5\",\n            ## add a turkey because why not!\n            images = list(\n      list(source = \"https://upload.wikimedia.org/wikipedia/commons/c/c9/Twemoji2_1f983.svg\",\n           xref = \"paper\",\n           yref = \"paper\",\n           x= 0.75,\n           y= 1,\n           sizex = 0.25,\n           sizey = 0.25\n           ) )\n\n)\np\n\n\n\n\n\nHappy feasting! ü¶É‚ù§Ô∏è"
  },
  {
    "objectID": "posts/2017-03-12-shinyswipr/index.html",
    "href": "posts/2017-03-12-shinyswipr/index.html",
    "title": "Introducing shinyswipr: swipe your way to a great Shiny UI",
    "section": "",
    "text": "One day Lucy was sitting around on twitter when she spotted this tweet:\nA tinder for preprints needs swiping though‚Ä¶\nSoon she remembered that I know a bit of javascript‚Ä¶\nAs we can‚Äôt pass up an opportunity to get distracted from coursework we immediately set about trying to implement swiping in papr.\nWe were dismayed to find that no one had already designed a swipe-based input for Shiny. Surely this must be one of the most important and needed additions to the statistical app making world?!\nNever deterred, after a few hours of hacking we had it working, but it was not pretty. The code was extremely hacky, with javascript injected haphazardly here and there and elements hidden so shiny could see the changes. Since then we steadily made improvements to it, streamlining the code until it worked consistently, and more importantly, looked good.\nNow we are ready to share our super secret swiping technology with the world. shinyswipr is an R package that allows you to easily put a swipe-based interface into any of your Shiny apps. Here‚Äôs how you can start swiping immediately."
  },
  {
    "objectID": "posts/2017-03-12-shinyswipr/index.html#taking-it-farther",
    "href": "posts/2017-03-12-shinyswipr/index.html#taking-it-farther",
    "title": "Introducing shinyswipr: swipe your way to a great Shiny UI",
    "section": "Taking it farther",
    "text": "Taking it farther\nA simple static swiping tool like that is great, but most of the time you want the content of your card to be dynamic. Luckily this is as easy with shinyswipr as it is with any other UI element in Shiny. Simple pass in whatever reactive elements you want to the shinyswiprUI function and they will be rendered within the card and updated at your will.\nLet‚Äôs demo this with a simple app. QuotR (trademark/patent pending) is an app that lets you view statistician quotes from the wonderful fortunes package and rate them at your own discretion.\nClick here to see a working demo. If you don‚Äôt feel like it, that‚Äôs fine. Here‚Äôs a screenshot of it.\n\nIn this app we use the output of the swipe function to store the results and also update the contents of the card/show past swipes. The UI code to make this app is:\n\nui &lt;- fixedPage(\n  h1(\"Stats Quotes\"),\n  p(\"This is a simple demo of the R package shinyswipr. Swipe on the quote card below to store your rating. What each direction (up, down, left, right) mean is up to you. (We won't tell.)\"),\n  hr(),\n  shinyswiprUI( \"quote_swiper\",\n                h4(\"Swipe Me!\"),\n                hr(),\n                h4(\"Quote:\"),\n                textOutput(\"quote\"),\n                h4(\"Author(s):\"),\n                textOutput(\"quote_author\")\n  ),\n  hr(),\n  h4(\"Swipe History\"),\n  tableOutput(\"resultsTable\")\n)\n\nAll we had to do was pass our desired UI functions in and shinyswipr handles the rest. These functions can then be updated by the server functions just as they would in any other situation. Easy, huh?"
  },
  {
    "objectID": "posts/2017-03-12-shinyswipr/index.html#what-do-we-do-with-this",
    "href": "posts/2017-03-12-shinyswipr/index.html#what-do-we-do-with-this",
    "title": "Introducing shinyswipr: swipe your way to a great Shiny UI",
    "section": "What do we do with this?",
    "text": "What do we do with this?\nAs silly of an interface tool as swiping is (with its connotations to a dating app), it truly is a much more mobile friendly interface tool than a button. What applications can you think of for swiping? Perhaps a Shiny app could used by a doctor to quickly parse through relevant patient data while walking through the ED, or a field ecologist can easily log data while in the field. Any way as statisticians and data scientists that we can make our products more inviting the better.\nIt would be fantastic if you could try out shinyswipr. I want people to use it for things I didn‚Äôt design it for, break it, submit bug reports on the github page, request new features, etc."
  },
  {
    "objectID": "posts/2017-03-12-shinyswipr/index.html#appendix",
    "href": "posts/2017-03-12-shinyswipr/index.html#appendix",
    "title": "Introducing shinyswipr: swipe your way to a great Shiny UI",
    "section": "Appendix",
    "text": "Appendix\nHere is the full code for the quotes app from above.\n\nlibrary(shinyswipr)\nlibrary(shiny)\nlibrary(fortunes)\n\nui &lt;- fixedPage(\n  h1(\"Stats Quotes\"),\n  p(\"This is a simple demo of the R package shinyswipr. Swipe on the quote card below to store your rating. What each direction (up, down, left, right) mean is up to you. (We won't tell.)\"),\n  hr(),\n  shinyswiprUI( \"quote_swiper\",\n                h4(\"Swipe Me!\"),\n                hr(),\n                h4(\"Quote:\"),\n                textOutput(\"quote\"),\n                h4(\"Author(s):\"),\n                textOutput(\"quote_author\")\n  ),\n  hr(),\n  h4(\"Swipe History\"),\n  tableOutput(\"resultsTable\")\n)\n\nserver &lt;- function(input, output, session) {\n  card_swipe &lt;- callModule(shinyswipr, \"quote_swiper\")\n\n  appVals &lt;- reactiveValues(\n    quote = fortune(),\n    swipes = data.frame(quote = character(), author = character(), swipe = character())\n  )\n\n  our_quote &lt;- isolate(appVals$quote)\n  output$quote &lt;- renderText({ our_quote$quote })\n  output$quote_author &lt;- renderText({ paste0(\"-\",our_quote$author) })\n  output$resultsTable &lt;- renderDataTable({appVals$swipes})\n\n  observeEvent( card_swipe(),{\n    #Record our last swipe results.\n    appVals$swipes &lt;- rbind(\n      data.frame(quote = appVals$quote$quote,\n                 author = appVals$quote$author,\n                 swipe = card_swipe()\n      ), appVals$swipes\n    )\n    #send results to the output.\n    output$resultsTable &lt;- renderTable({appVals$swipes})\n\n    #update the quote\n    appVals$quote &lt;- fortune()\n\n    #send update to the ui.\n    output$quote &lt;- renderText({ appVals$quote$quote })\n\n    output$quote_author &lt;- renderText({ paste0(\"-\",appVals$quote$author) })\n  }) #close event observe.\n}\n\nshinyApp(ui, server)\n\nThat is a lot of code, but to it is a full app."
  },
  {
    "objectID": "posts/2018-02-12-seasonality/index.html",
    "href": "posts/2018-02-12-seasonality/index.html",
    "title": "The United States of Seasons",
    "section": "",
    "text": "‚äïI think my favorite detail about this map is the little splotch that is the Smoky Mountains on the western edge of North Carolina. Having spent a good amount of time there they really do have a different climate than the surrounding area.\nThe other day my girlfriend and I were talking about places we would like to live after grad school and one of the things that got brought up was how ‚Äòseasonal‚Äô the location is. She grew up in Long Beach, California, which essentially has no seasons, whereas I grew up near Ann Arbor, Michigan which very much has seasons. This got me to thinking: what does the country look like in the context of its seasonality? By ‚Äòseasonality‚Äô we mean how big of a shift in the weather is there during the year.\nBeing into data and maps I figured I would try and investigate this in a data-driven way. The final result is above this, but here I will walk you through the process that got me from question to map."
  },
  {
    "objectID": "posts/2018-02-12-seasonality/index.html#geospatial-interpolation",
    "href": "posts/2018-02-12-seasonality/index.html#geospatial-interpolation",
    "title": "The United States of Seasons",
    "section": "Geospatial interpolation",
    "text": "Geospatial interpolation\nWhat we need to do is fit a model that can predict average swing given a lat-lon pair, along with generating a high-density evenly-spaced grid to run that model on.\n\nGridding it\nTo get the grid we will download a US shapefile from the Census‚Äô Tiger repository and use the library rgdal to read it in and exclude the given locations we don‚Äôt want to look at.\n\nlibrary(sp)\nlibrary(rgdal)\nlibrary(gstat)\nlibrary(maptools)\n\nnot_wanted &lt;- c(\n  'Alaska', 'American Samoa', 'Puerto Rico', 'Guam', \n  'Commonwealth of the Northern Mariana Islands United States Virgin Islands', \n  'Commonwealth of the Northern Mariana Islands', \n  'United States Virgin Islands', 'Hawaii')\n\nus &lt;- rgdal::readOGR(\"../../media/seasons/us_shapefile/\", \"cb_2016_us_state_500k\") %&gt;% \n  subset(!(NAME %in% not_wanted))\n## OGR data source with driver: ESRI Shapefile \n## Source: \"/Users/lucymcgowan/Dropbox (Wake Forest University)/wonderland-db/lfod/media/seasons/us_shapefile\", layer: \"cb_2016_us_state_500k\"\n## with 56 features\n## It has 9 fields\n## Integer64 fields read as strings:  ALAND AWATER\n\nplot(us)\n\n\n\n\nLooks right to me! Now let‚Äôs calculate a grid of points over this.\n\n# make a big grid that corresponds to the bounding box around out shapefile\ngrid_us &lt;- makegrid(us, n = 5000) %&gt;% \n  SpatialPoints(proj4string = CRS(proj4string(us))) %&gt;% \n  .[us,] # subset the grid such that it only has points that fall inside of our states.\n\nplot(grid_us, type = 'p')\n\n\n\n\n‚äïIn the final product I bumped up the resolution (n) way higher to make the result smoother\nNext we need to fit a model for interpolation. Here I use one of the most common techniques for this: inverse distance weighting. ‚äïThe task of fitting a model to these data is actually a fascinating one that I am not giving nearly enough weight in this post. Here we are just having as inputs lat and lon but we could easily make it more realistic by adding in elevation etc. Some models use 2-d gaussian processes to fit flexible models, some use splines, it‚Äôs a vast world and one I would love to dig into more!\n\n# convert the data to a spacial dataframe.\nsp::coordinates(swing_by_station) = ~lon + lat\n\n# make sure that the projection matches the grid we've built.\nproj4string(swing_by_station) &lt;- CRS(proj4string(us)) \n\n# fit basic inverse distance model\nidw_model &lt;- gstat::idw(\n  formula = swing ~ 1, \n  locations = swing_by_station, \n  newdata = grid_us,\n  idp = 2) \n## [inverse distance weighted interpolation]\n\n# extract predictions from the interpolation model\ninterpolated_results = as.data.frame(idw_model) %&gt;% {# output is defined as a data table\n    names(.)[1:3] &lt;- c(\"lon\", \"lat\", \"swing\")  # give names to the modeled variables\n    . } %&gt;% \n  select(lon, lat, swing)\n\ninterpolated_results %&gt;% \n  head() %&gt;% \n  knitr::kable()\n\n\n\n\nlon\nlat\nswing\n\n\n\n\n-80.80\n25.34\n18.65760\n\n\n-80.26\n25.34\n18.35323\n\n\n-81.34\n25.88\n16.24864\n\n\n-80.80\n25.88\n18.56805\n\n\n-80.26\n25.88\n15.06180\n\n\n-98.62\n26.42\n31.26166\n\n\n\n\n\nWe‚Äôre in business! We‚Äôve now got an evenly spaced grid of points we can plot in a standard raster way!\n\nggplot(interpolated_results, aes(x = lon, y = lat)) + \n  geom_raster( aes(fill = swing)) +\n  xlim(-125, -65) + ylim(24, 51) + \n  theme_void() +\n  labs(fill = \"Temp swing in degrees\") +\n  borders('state', alpha = 0.1, size = 0.1)\n\n\n\n\nLooks good but could use some improvement. The gradiations are rather smooth and thus it‚Äôs hard to see. Let‚Äôs bin the swings to 5 degree intervals and change the color pallet to be a bit more appropriate. ‚äïI chose 5 degree intervals here because I figured that was about the limit of sensitivity for the average person. Totally arbitrary though.\n\ninterpolated_results %&gt;% \n  mutate(swing_in_5s = round(swing/5)*5) %&gt;% \n  ggplot(aes(x = lon, y = lat)) + \n  geom_raster( aes(fill = swing_in_5s)) +\n  scale_fill_distiller(palette = 'YlGn', direction = 1) +\n  xlim(-125, -65) + ylim(24, 51) + \n  theme_void() +\n  labs(fill = \"Temp swing in degrees\") +\n  borders('state', alpha = 0.1, size = 0.1)\n\n\n\n\nThere it is! In the top plot I have rerun the grid with a much higher n and also tweaked some settings in the legend/scales to make the plot look a bit better. Hopefully now when you have some interesting geospatial data that is not evenly distributed you will know how to deal with it!\n\n\nCode for top figure.\n\nguide_tinker =  guide_legend(\n  title.position = \"top\",\n  label.position=\"bottom\",\n  label.hjust = 0.5,\n  direction = \"horizontal\",\n  keywidth = 1,\n  nrow = 1 )\n\ncolourCount = interpolated_swings$swing_in_5s %&gt;% unique() %&gt;% length()\npalette = colorRampPalette(brewer.pal(9, \"YlGnBu\"))(colourCount)\n\ninterpolated_swings %&gt;% \n  mutate(swing_in_5s = factor(round(swing_in_5s)))%&gt;% \n  ggplot(aes(x = lon, y = lat)) + \n  geom_raster( aes(fill = swing_in_5s)) +\n  scale_fill_manual(values = palette, \n                    guide = guide_tinker)  +\n  xlim(-130,-67) + ylim(24,50) +\n  theme_void() +\n  theme(\n    text = element_text(family = 'Montserrat'),\n    legend.justification = c(0,0),\n    legend.position = c(0,0.02),\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 8),\n    legend.box.background = element_rect(fill = '#f0f0f0', color = NA)\n  ) + \n  labs(\n    title = \"The United States of Seasons\",\n    subtitle = \"Difference between the hottest and coldest days of the year\",\n    fill = \"Temp swing in degrees\") +\n  borders('state', alpha = 0.1, size = 0.1)"
  },
  {
    "objectID": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html",
    "href": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html",
    "title": "Leveraging uncertainty information from deep neural networks for disease detection - a summary",
    "section": "",
    "text": "As a biostatistician in the deep learning world I have the awkward task of balancing the dogma of statistics (everything is uncertain) along with the alluring success of some of the newest crazy complex neural network architectures. Going onto any Kaggle competition or new paper ‚äïSuch as the popular but arguably flawed paper on diagnosing from radiological screens from Andrew Ng et al. you will see models with millions of parameters performing seemingly magical tasks on data of all kinds.\nThese models however, almost never report uncertainty in their predictions. Why would they? Besides the fact that in the ‚Äújust predict as many correct as possible‚Äù world of Kaggle uncertainty doesn‚Äôt win you money, turning their model into a Bayesian one by putting priors on their weights takes the already computationally intensive task of training a neural net and makes it even more burdensome.\nThe paper Leveraging uncertainty information from deep neural networks for disease detection does two very powerful things. First it shows that a recent method of getting uncertainty in neural networks by exploiting the regularization technique of dropout works well in the context of complex disease diagnosis models, and second it shows the value of this uncertainty knowledge in a biomedical context in number of clever ways."
  },
  {
    "objectID": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html#main-point",
    "href": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html#main-point",
    "title": "Leveraging uncertainty information from deep neural networks for disease detection - a summary",
    "section": "Main Point",
    "text": "Main Point\nBy using (approximately) Bayesian methods to ascertain prediction uncertainty in deep neural networks (but really any model) we can more intelligently decide when to trust the prediction or to send the observation to a more definitive look. In the context of biomedical applications this means we can know when to trust the model and when to get a doctor to look at the data manually.\nSide Points\nThe paper also dives in bit into the use of Bayesian uncertainty to try and diagnose data from an unobserved input distribution of output class. It comes to the conclusion that the Bayesian approach can not separate the causes of uncertainty and thus it doesn‚Äôt work well for this task. They do, however, show that you can fit an autoencoder on top of the trained layers and do decently well. This is interesting but not what I consider the main or most impactful parts of the paper."
  },
  {
    "objectID": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html#what-they-did",
    "href": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html#what-they-did",
    "title": "Leveraging uncertainty information from deep neural networks for disease detection - a summary",
    "section": "What they did",
    "text": "What they did\nTook standard convolutional architectures for image prediction and trained them with dropout to predict the presence of the disease Diabetic Retinopathy. At testing time they kept the dropout on and assembled a monte-carlo approximation of the posterior of the predictions.\n\nDemonstration of how Diabetic Retinopahy manifests itself in an individuals eye. Figure taken from Dr.¬†Winston J. Scott‚Äôs website.\nThe mean of this posterior was used as the final prediction value (p(disease|image) ) and the width of the distribution was used as a measure of how uncertain the model is about its prediction.\n\nWhy can‚Äôt we just use the probability as a measure of uncertainty?\nThere is a tricky and subtle difference between prediction uncertainty and model uncertainty. Prediction uncertainty (or simply the output probability of a class) says: ‚Äúassuming I am a correct model (I have determined the correct separating hyperplane for these data) , this is the probability that the observation is a given class‚Äù, where as model uncertainty says: ‚ÄúI recognize that my separating hyperplane is not exact, and taking that into account, this is how confident I am in my predicted probability.‚Äù\nIf you make the assumption that the data is truly Bernoulli after being conditioned on the inputs, you can calculate the variance of the prediction by doing the classic \\((1-p)*p\\) variance estimate, but the paper shows that this is too simplistic and under performs the dropout uncertainty measurement in all scenarios. ‚äïAnd even a random baseline! See figure 4."
  },
  {
    "objectID": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html#how-did-they-test-their-models",
    "href": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html#how-did-they-test-their-models",
    "title": "Leveraging uncertainty information from deep neural networks for disease detection - a summary",
    "section": "How did they test their models?",
    "text": "How did they test their models?\nAt first glance it seems hard to actually compare the traditional no-uncertainty models and the Bayesian models as they are fundamentally different. The paper does a rather smart thing to deal with this and simulates how they envision the models being used in real life.\nFirst they run predictions from the Bayesian model and the standard model on the entire test set, they then discard (or let an expert review) the predictions that the model was the most uncertain about. This discarding was done in multiple ways:\n\nThey set a boundary for uncertainty (any prediction that had an associated uncertainty above was discarded), they then computed model performance measures (AUC etc) on the retained data.\nThey set a boundary on the amount of observations retained: i.e.¬†only the top % of the predictions are kept. This allowed them to compare their model to a standard model (using the aforementioned Bernoulli variance as a stand in for uncertainty) and also by just randomly discarding %."
  },
  {
    "objectID": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html#what-relationships-did-they-find",
    "href": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html#what-relationships-did-they-find",
    "title": "Leveraging uncertainty information from deep neural networks for disease detection - a summary",
    "section": "What relationships did they find?",
    "text": "What relationships did they find?\nUsing the first uncertainty cutoff boundary they saw that, as they made their threshold more picky, the model performance monotonically increased. This shows that the model was correctly labeling the observations that it was getting wrong as uncertain more often than not. They showed this as well by simply looking at the distributions of uncertainty for the correctly classified observations and the incorrectly classified ones, with the incorrectly classified ones having a higher average uncertainty.\n\nResults of sliding the level of uncertainty tollerated for a prediction by the model (a) and sliding the proportion of the predictions that are not predicted (compared to a random baseline) (b).\nTheir second test setup, where they compared the standard model with no dropout at test time to the model with dropout at test time they saw that the dropout uncertainty always outperformed the random discarding of cases, where as the Bernoulli-based uncertainty actually performed worse than the random subset approach for a large range of the thresholds.\n Showing the effect of sliding the proportion of data predictions are performed on on the AUC of different models. Shows that the bayes by dropout approach is always better than the random baseline, and even is better than a deep gaussian process approach. Surprisingly the nieve uncertainty approach (‚Äòstandard dropout‚Äô) is often worse than random."
  },
  {
    "objectID": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html#what-implications-does-this-have",
    "href": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html#what-implications-does-this-have",
    "title": "Leveraging uncertainty information from deep neural networks for disease detection - a summary",
    "section": "What implications does this have?",
    "text": "What implications does this have?\nThis paper shows that, using the recently published finding that dropout can be used to approximate a posterior, it is incredibly simple to turn a deep neural network (of any architecture) trained on complex medical data into one that can be used to not only predict diseases but also say when you should not rely on the model and seek other input for diagnosis.\nPurely as a real-world demo that the idea that bayes as dropout works this paper is great, but the combination of well throughout experiments demonstrating the value of uncertainty in neural network prediction and demonstrating the power of bayes-by-dropout compared to other options for uncertainty, it becomes excellent."
  },
  {
    "objectID": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html#my-hesitations",
    "href": "posts/2017-12-24-uncertainty_in_disease_detection_review/index.html#my-hesitations",
    "title": "Leveraging uncertainty information from deep neural networks for disease detection - a summary",
    "section": "My hesitations",
    "text": "My hesitations\nData Balance\nIt was not clear from the paper if they properly accounted for the unbalance of the data. The data is split 70-30 between no-disease and disease. They properly use AUC for their performance measure but they also make statements about the distribution of certainty in their model over the true status of the observation. Some of these seem to be a result of simply the fact that there are many more no-disease examples than disease ones. For instance taking a look at the following figure where they look at the two-dimensional density of model prediction to uncertainty to predictions they got correct vs those they got incorrect‚Ä¶\n\nWe see a large blob of density in the bottom left corner. This seems to be to be a simple fact that there were many more non-diseased cases than diseased cases. I think the figure would be more informative if they used a balanced test set. If anything I think it would make their point stronger, but I think it would help elucidate if the model was just relying on the distribution of the training data to be a good predictor.\nRepurposing a softmax model\nThey are collapsing an ordinal outcome (disease severity) into a binary disease not-disease indicator. For one of their models they took a high-performing kaggle model and simply summed the softmax outputs for the disease-corresponding classes to get their probability of disease.\n\nThe publicly available network architecture and weights provided by the participant who scored very well in the Kaggle DR competition, which we will call JFnet‚Ä¶We recast the original model‚Äôs five output units (trained for Kaggle DR‚Äôs level discrimination task) to our binary tasks by summing the output of respective units.\n\nThis makes me uneasy as the model was trained to minimize the categorical cross entropy, but then they treat it like it was trained to minimize binary cross entropy. It feels like there may be some unexplored side effects of the model dedicating more of its training to optimizing the parts corresponding to disease classification than to non-disease classification.\nAlso, they are dichotomizing, and what is the name of this blog?"
  },
  {
    "objectID": "posts/2018-07-22-shinyviewr-intro/index.html",
    "href": "posts/2018-07-22-shinyviewr-intro/index.html",
    "title": "Shinyviewr: camera input for shiny",
    "section": "",
    "text": "My package shinysense has been around for more than a year now. It started as a package to add swiping via touch screens to shiny for our app Papr, but then slowly got built to include functions for hearing (shinyearr), movement (shinymovr), and drawing (shinydrawr). However one major sense was missing: vision.\nI had it on the to-do list for the package for a while but never got around to it. Then last week Dean Attali pinged me on github about the status of the camera-functionality to shinysense.\nThis, along with my recent dive into deep learning, spurred a renewed effort to add vision to shiny‚Äôs senses. The result is the new function shinyviewr, which, like the rest of shinysense, comes in the form of a shiny module that can be easily added to your app to endow it with the ability to sense the outside world.\n\n\nComing soon to a shiny app near you: webcam input! #rstats (Spurred on by @daattali) pic.twitter.com/1Qav8ftDJs\n\n‚Äî Nick Strayer (@NicholasStrayer) July 20, 2018"
  },
  {
    "objectID": "posts/2018-07-22-shinyviewr-intro/index.html#motivation",
    "href": "posts/2018-07-22-shinyviewr-intro/index.html#motivation",
    "title": "Shinyviewr: camera input for shiny",
    "section": "",
    "text": "My package shinysense has been around for more than a year now. It started as a package to add swiping via touch screens to shiny for our app Papr, but then slowly got built to include functions for hearing (shinyearr), movement (shinymovr), and drawing (shinydrawr). However one major sense was missing: vision.\nI had it on the to-do list for the package for a while but never got around to it. Then last week Dean Attali pinged me on github about the status of the camera-functionality to shinysense.\nThis, along with my recent dive into deep learning, spurred a renewed effort to add vision to shiny‚Äôs senses. The result is the new function shinyviewr, which, like the rest of shinysense, comes in the form of a shiny module that can be easily added to your app to endow it with the ability to sense the outside world.\n\n\nComing soon to a shiny app near you: webcam input! #rstats (Spurred on by @daattali) pic.twitter.com/1Qav8ftDJs\n\n‚Äî Nick Strayer (@NicholasStrayer) July 20, 2018"
  },
  {
    "objectID": "posts/2018-07-22-shinyviewr-intro/index.html#how-to-use",
    "href": "posts/2018-07-22-shinyviewr-intro/index.html#how-to-use",
    "title": "Shinyviewr: camera input for shiny",
    "section": "How to use",
    "text": "How to use\nIn this code I will supply demo code in the form of single page shiny apps. This means the UI and Server code are all contained in a single .R file. This makes reproducing everything easier for you. Just copy and paste the block into an RStudio console and run!\nSetup\nBefore you can run these examples you will need to have the latest version of shinysense downloaded from github. In addition, we will be running an example with a deep learning model obtained via the library Keras. Run these commands before to make sure you‚Äôre all setup.\n\n# Installs latest version of shinysense.\ndevtools::install_github('nstrayer/shinysense')\n\n# Installs latest version of keras. \ndevtools::install_github('rstudio/keras')\n\n# Make sure you have the python versions of keras/tensorflow installed on your machine.\nkeras::install_keras()\n\nNow that all that is out of the way let‚Äôs start with the most basic example.\n\nlibrary(shiny)\nlibrary(shinysense)\nlibrary(tidyverse)\n\nui &lt;- fluidPage(\n  shinyviewrUI(\"myCamera\", height = '200px'),\n  imageOutput(\"snapshot\")\n)\n\nserver &lt;- function(input, output) {\n  #server side call of the drawr module\n  myCamera &lt;- callModule(\n    shinyviewr, \"myCamera\", \n    outputHeight = 300,\n    outputWidth = 400)\n\n  # logic for what happens after a user has drawn their values. \n  observeEvent(myCamera(), {\n    photo &lt;- myCamera() \n    print(photo) #print to console for inspection\n    output$snapshot &lt;- renderPlot({\n      plot(as.raster(photo)) # plot photo\n    })\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\nResult:\n\nWe have a working app!"
  },
  {
    "objectID": "posts/2018-07-22-shinyviewr-intro/index.html#output-format",
    "href": "posts/2018-07-22-shinyviewr-intro/index.html#output-format",
    "title": "Shinyviewr: camera input for shiny",
    "section": "Output format",
    "text": "Output format\nA fair question at this point would be ‚Äúin what form does the image show up in shiny?‚Äù If you run the above example and look at the console you will see the image object printed. It is a rather simple thing: just a 3D array with the dimensions (height, width, channels), with channels being red, green, and blue quantities on a scale from 0-1.\n\ndim(photo)\n&gt; [1] 300 400   3\n\nTo plot, you can do what we did in the example, just send it to the as.raster function and then plot the resultant raster object."
  },
  {
    "objectID": "posts/2018-07-22-shinyviewr-intro/index.html#usage-on-phones",
    "href": "posts/2018-07-22-shinyviewr-intro/index.html#usage-on-phones",
    "title": "Shinyviewr: camera input for shiny",
    "section": "Usage on phones",
    "text": "Usage on phones\nHaving a webcam view is interesting but ultimately rather limiting. One of the more powerful features of shiny is the ability of the apps to run anywhere, including on a phone.\nshinyviewr can automatically detect that it is running on a phone with a rear camera and will append to its UI a drop down menu to choose between which camera you use to take a photo.\n\n\nFront camera: \n\n\nRear camera: \n\n\nThe ability to utilize rear-facing cameras on mobile devices hopefully opens up opportunities for researchers to collect data and run models right in the field, without having to awkwardly plug in an SD card and run a script later. Collect your data and run your model right in the moment!"
  },
  {
    "objectID": "posts/2018-07-22-shinyviewr-intro/index.html#example-app",
    "href": "posts/2018-07-22-shinyviewr-intro/index.html#example-app",
    "title": "Shinyviewr: camera input for shiny",
    "section": "Example app",
    "text": "Example app\nTaking a photo and just plotting it is‚Ä¶ kind of lame. The real power of image input into shiny is, in my opinion, the easy interfacing with powerful machine learning libraries. To demonstrate this, let‚Äôs build an app that attempts to classify whatever is in the photo you just took.\nThis is where Keras comes in from the setup script. We will use a pre-trained model that has been trained to recognize 1,000 different image classes from a dataset called ‚Äòimagenet.‚Äô Keras includes easy helpers to access multiple different models that have been already trained on this massive dataset for you, vastly simplifying your workload. We will load the model vgg50. ‚äïIn order to speed up your app, before you run the example, make sure to run the line loading the model. The first time this line is run it will go and fetch the (rather hefty) file containing the weights. After it‚Äôs run the first time it doesn‚Äôt need to be downloaded again though.\nImage classification example\n\nlibrary(shiny)\nlibrary(shinysense)\nlibrary(tidyverse)\nlibrary(keras)\n\n# instantiate the model\nmodel &lt;- application_resnet50(weights = 'imagenet')\n\nui &lt;- fluidPage(\n  titlePanel(\"Image Classifier\"),\n  fluidRow(\n    column(\n      width = 7,\n      h3(\"Webcam\"),\n      shinyviewrUI(\"myCamera\", height = '250px')\n    ),\n    column(\n      width = 4, offset = 1,\n      h3('Last Photo'),\n      imageOutput(\"snapshot\", height = '250px')\n    )\n  ),\n  h3(\"Predictions\"),\n  plotOutput(\"predPlot\")\n)\n\n\nserver &lt;- function(input, output) {\n  #server side call of the drawr module\n  myCamera &lt;- callModule(\n    shinyviewr, \"myCamera\", \n    outputWidth = 500, \n    outputHeight = 500\n  )\n  \n  observeEvent(myCamera(), {\n    \n    photo &lt;- myCamera() \n    photo_processed &lt;- photo %&gt;% \n      image_array_resize(224, 224) %&gt;% \n      {.*255} %&gt;%  #get image to 0-255 instead of 0-1\n      array_reshape(c(1, dim(.))) %&gt;% \n      imagenet_preprocess_input()\n    \n    # make predictions then decode and print them\n    preds &lt;- model %&gt;% \n      predict(photo_processed) %&gt;% \n      imagenet_decode_predictions(top = 20) %&gt;% \n      .[[1]]\n    \n    output$predPlot &lt;- renderPlot({\n        ggplot(preds, \n          aes(x = reorder(class_description, score), y = score)\n        ) +\n        geom_pointrange(aes(ymin = 0, ymax = score)) +\n        coord_flip()\n    })\n    output$snapshot &lt;- renderPlot({\n      plot(as.raster(photo))\n    })\n  })\n}\nshinyApp(ui = ui, server = server)\n\nWhile that may seem like a lot of code to look at. Keep in mind that it is everything needed to spin up an app that uses the deep learning to classify images coming from the camera of your phone or computer. It‚Äôs pretty crazy how few lines it is in that context!\n‚äïThe demo app linked will look a tad bit different from what you get if you run the code above. This is because I modified it with some stylistic changes that just added more code to the already long chunk above. If you want to see the exact code generating the demo app check out the repo. Here‚Äôs a snapshot of the results when running on my phone.\n\nHere‚Äôs a link to a live demo app for you to check out the app without having to run the code above. Beware, the app is hosted on the free tier of shinyapps.io so there‚Äôs a decent chance it will not work by the time you read this. If this is the case, just running the code above will accomplish the same thing!"
  },
  {
    "objectID": "posts/2017-02-18-the-dire-consequences-of-tests-for-linearity/index.html",
    "href": "posts/2017-02-18-the-dire-consequences-of-tests-for-linearity/index.html",
    "title": "The dire consequences of tests for linearity",
    "section": "",
    "text": "This is a tale of the dire type 1 error consequences that occur when you test for linearity.\n  \n\nEdvard Munch‚Äôs The Scream (1893), coincidentally also the face Frank Harrell makes when he sees students testing for linearity.\n\nFirst, my favorite explanation of type 1 error üê∫:\n\n\n\n@jgschraiber @eagereyes Pro-tip that changed my life: in The Boy Who Cried Wolf, the villagers first make a Type 1, and then a Type 2 error.\n\n‚Äî Sam (@geometrywarrior) September 28, 2016\n\n\n\nWe generally fix (or claim to fix) this type 1 error at 0.05, but sometimes our procedures can make this go awry!\nI‚Äôve prepared a very basic simulation.\n\ngenerate 100 data points from two independent random normal distributions, an outcome \\(y\\) and a predictor \\(x\\) (Since these are generated randomly, we would not expect there to be an association between \\(x\\) and \\(y\\). If all goes as planned, our type 1 error would be 0.05) \nfit simple linear model with a restricted cubic spline on the predictor \\(x\\)\ntest whether the nonlinear terms are significant\n\nif they are, leave them in and test the association between \\(x\\) and \\(y\\)\nif they are not, remove them and refit the model with only a linear term for \\(x\\) & proceed to test the association between \\(x\\) and \\(y\\).\n\n\ncalculate the type 1 error, how many times we detected a spurious significant association between \\(x\\) and \\(y\\).\n\nHere‚Äôs my simulation code (run it yourself!):\n\n\nhank you Pua Yong Hao for pointing out a typo in the original version of this function ‚Äì it has been updated!\n\nlibrary('rms')\n\nsim &lt;- function(wrong = TRUE){\n#generate completely random data\ny &lt;- rnorm(100)\nx &lt;- rnorm(100)\n#fit a model with a restricted cubic spline\nmod &lt;- ols(y ~ rcs(x))\n\nif (wrong == TRUE & anova(mod)[2, 5] &gt; 0.05){\n  #if the test for non-linearity is not \"significant\", remove nonlinear terms\n  mod &lt;- ols(y ~ x)\n} \n #save the p-value\n anova(mod)[1, 5]\n}\n\n\n [Type 1 error when removing non-significant nonlinear terms]\n\n\n\ntest &lt;- replicate(10000, sim()) \ncat(\"The type 1 error is\", mean(test &lt;= 0.05))\n\nThe type 1 error is 0.0812\n\n\nUh oh! That type 1 error is certainly higher than the nominal 0.05 we claim!\n\n [Type 1 error when not removing non-significant nonlinear terms]\n\nWe would expect the type 1 error to be 0.05 ‚Äì I perform the same simulation omitting the step of removing non-significant nonlinear terms and calculate the type 1 error again.\n\ntest &lt;- replicate(10000, sim(wrong = FALSE))\ncat(\"The type 1 error is\", mean(test &lt;= 0.05))\n\nThe type 1 error is 0.0501\n\n\nMuch better üëØ\nThe conclusion: fit flexible models - skip the tests for linearity!\nThis has been elegently demonstrated by others, check out Grambsch and O‚ÄôBrien."
  },
  {
    "objectID": "posts/2017-09-25-commentary-and-follow-up-to-p-0-005-suggestion/index.html",
    "href": "posts/2017-09-25-commentary-and-follow-up-to-p-0-005-suggestion/index.html",
    "title": "Commentary and follow up to p<0.005 suggestion",
    "section": "",
    "text": "A recent paper, Redefine Statistical Significance by 72 (r emo::ji(\"scream\")) co-authors, has caused quite a stir in the statistical community. Our student-run journal club at Vanderbilt will be discussing this contribution at our meeting led by Nathan James this week, so I‚Äôve attempted to create a list of significant responses/commentary that have come out since this paper was posted on PsyArXiv.\nThis was compiled mostly via a quick Twitter search - please let me know what we are missing!\n\nJustify Your Alpha: A Response to ‚ÄúRedefine Statistical Significance‚Äù: Response posted to PsyArXiv by 88 (r emo::ji(\"scream\")``r emo::ji(\"scream\")) researchers suggesting that researchers ought to select and justify P-value thresholds prior to collecting any data.\n\n‚ÄòOne-size-fits-all‚Äô threshold for P values under fire: Nature News article summarizing the original paper and the ‚ÄúJustify your Alpha‚Äù response; includes results from a poll of Nature readers ‚ÄúShould the P-value threshold be lowered.‚Äù\n\n\nFor what it‚Äôs worth, others have found opposing results in a different population:\n\n\nShould we lower the p-value for statistical significance from 0.05 to 0.005 ?Vote & discuss\n\n‚Äî C. Michael Gibson MD (@CMichaelGibson) July 27, 2017\n\n\n\nAbandon Statistical Significance: Response suggesting abandoning the null hypothesis significance testing paradigm entirely.\n\nWhat a nerdy debate about p-values shows about science ‚Äî and how to fix it\n\nChanging the default p-value threshold for statistical significance ought not be done, and is the least of our problems.: Medium post by psychologist Timothy Bates where he calls the proposal ‚Äúa risky distraction‚Äù from the root causes of irreproducible results.\n\nThe Effort Report, episode 55: The most recent episode of The Effort Report (a podcast by Elizabeth Matsui and Roger Peng) discusses this topic.\n\nPlease comment/let me know if we have missed anything!"
  },
  {
    "objectID": "posts/2020-05-21-survival-model-detective-1/index.html",
    "href": "posts/2020-05-21-survival-model-detective-1/index.html",
    "title": "Survival Model Detective: Part 1",
    "section": "",
    "text": "A paper by Grein et al.¬†was recently published in the New England Journal of Medicine examining a cohort of patients with COVID-19 who were treated with compassionate-use remdesivir. There are two things that were interesting about this paper:\nThis post focuses on the very neat figure!"
  },
  {
    "objectID": "posts/2020-05-21-survival-model-detective-1/index.html#figure-2",
    "href": "posts/2020-05-21-survival-model-detective-1/index.html#figure-2",
    "title": "Survival Model Detective: Part 1",
    "section": "Figure 2",
    "text": "Figure 2\nFigure 2 in the original paper shows the changes in oxygen-support status from baseline for each of the 53 patients. This figure includes information about:\n\nThe duration of follow up for each individual patient\nEach patient‚Äôs oxygen trajectory\nEach patient‚Äôs ultimate outcome (death, discharged, censored)\n\nYou can construct a whole dataset from this (and I did!) - you can find it on my GitHub.\n\nBelow is code to recreate their Figure 2 using #rstats üòé.\n\nlibrary(tidyverse)\nd &lt;- read_csv(\"https://raw.githubusercontent.com/LucyMcGowan/nejm-grein-reanalysis/master/data/data-fig-2.csv\")\n\n\nlong_dat &lt;- d %&gt;%\n  pivot_longer(day_1:day_36)\n\ncats &lt;- tibble(\n  value = 1:6,\n  cat = factor(c(\"Ambient air\", \"Low-flow oxygen\", \"High-flow oxygen\", \"NIPPV\", \n                 \"Mechanical ventilation\", \"ECMO\"),\n               levels = c(\"ECMO\", \"Mechanical ventilation\", \"NIPPV\", \n                          \"High-flow oxygen\", \"Low-flow oxygen\", \"Ambient air\"))\n)\nlong_dat %&gt;%\n  left_join(cats, by = \"value\") %&gt;%\n  filter(!is.na(value)) %&gt;%\n  mutate(day_oxy = as.numeric(gsub(\"day_\", \"\", name)) - 1,\n         day_oxy = ifelse(day_oxy &gt; 28, 28, day_oxy),\n         day = ifelse(day &gt; 28, 28, day),\n         patient = factor(patient, levels = 53:1),\n         event = ifelse(event == \"censor\", NA, event)\n  ) %&gt;%\n  ggplot(aes(x = patient, y = day_oxy, fill = cat)) +\n  geom_segment(aes(x = patient, xend = patient,\n                   y = 0, yend = day - 0.5), lty = 3) +\n  geom_tile(width = 0.5) + \n  scale_fill_manual(\"Oxygen support\",\n                    values = c(\"#7D3A2C\", \"#AA3B2F\", \"#D36446\", \"#DEA568\", \n                               \"#F5D280\", \"#FCEEBC\")) +\n  geom_point(aes(x = patient, y = day - 0.5, shape = event)) +\n  scale_shape_manual(\"Event\", values = c(15, 5),\n                     labels = c(\"Death\", \"Discharge\", \"\")) +\n  guides(fill = guide_legend(override.aes = list(shape = NA), order = 1)) +\n  coord_flip() +\n  labs(y = \"day\", x = \"\") +\n  theme_classic()\n\n\n\n\nI definitely applaud the authors for making this so accessible! Check out Part 2 to see a bit about how their statistics could be improved."
  },
  {
    "objectID": "posts/2017-07-17-ropensci-slack-emojis/index.html",
    "href": "posts/2017-07-17-ropensci-slack-emojis/index.html",
    "title": "Happy World Emoji Day: an analysis of rOpenSci‚Äôs Slack emojis",
    "section": "",
    "text": "HAPPY world emoji day! üåç üêî üìÜ\nIn honor of this momentous occasion, I have decided to analyze the emojis used on rOpenSci‚Äôs Slack.\n\nlibrary(\"dplyr\")\n\n\n\nIf you‚Äôd like to follow along, go fetch yourself a Slack token.\ntoken &lt;- \"MY_SLACK_API_TOKEN\" ## stick your token here\nWe will first use Slack‚Äôs reactions.list method.\n\n\nNotice here I am pulling the items from the response and then from each item I am interested in the message reactions. This ignores the reactions on files and comments.\nreq_lst &lt;- httr::POST(\n  \"https://slack.com/api/reactions.list\",\n  body = list(\n   token = token,\n   count = 500\n   )) %&gt;%\n  httr::content() %&gt;%\n  .$items %&gt;%\n  purrr::map(~.[[\"message\"]][[\"reactions\"]])\nLet‚Äôs pull out the name and count of each emoji used and stick it in a tibble üéâ.\ntbl &lt;- tibble::tibble(\n  name = purrr::map_chr(purrr::flatten(req_lst), \"name\"),\n  count = purrr::map_int(purrr::flatten(req_lst), \"count\")\n)\nOne of the most delightful features in Slack is the ability to create custom emojis! In order to be able to display both ordinary and custom emojis, I can pull in a list of all emojis we have customized in the rOpenSci Slack team using the emoji.list method.\n\n\nNotice here I am using the tibble::enframe() function. This is an awesome way to convert a vector or list to a two-column data frame.\nemojis_tbl &lt;- httr::POST(\n  \"https://slack.com/api/emoji.list\",\n  body = list(token = token)\n  ) %&gt;%\n  httr::content() %&gt;%\n  .$emoji %&gt;%\n  tibble::enframe() %&gt;%\n  mutate(value = unlist(value))\nThe emojis_tbl data frame contains the name of each custom emoji and a link to their associated image. Here I create a small function that will either read that image using the magick package or, if it is an ordinary emoji, use the emo package to look it up.\n\nread_emoji &lt;- function(x, y) {\n  if (!is.na(x)) {\n    magick::image_read(x)\n  } else {\n    emo::ji(y)\n  }\n}\n\nLet‚Äôs do a wee bit of data wrangling üöú to sort out which emojis are used the most.\n\ntop_emojis &lt;- tbl %&gt;%\n  group_by(name) %&gt;%\n  summarise(count = sum(count)) %&gt;%\n  arrange(desc(count)) %&gt;%\n  slice(1:10) %&gt;%\n  left_join(emojis_tbl, by = \"name\") %&gt;%\n  mutate(emoji = purrr::map2(value, name, read_emoji))\n\nI have written another small function to make sure the custom emojis print properly when I render my output.\n\n\nNote, if you are doing this for your blog, rather than saving a temporary file as I have demonstrated here, you should save this as a relative file path in your blog üå≥.\n\nrender_emoji &lt;- function(x, y) {\n  if (inherits(x, \"magick-image\")) {\n    tmp &lt;- tempfile(fileext = \".gif\")\n    x &lt;- magick::image_scale(x, \"25x25\")\n    magick::image_write(x, path = tmp)\n    emoji &lt;- rep(glue::glue(\"![]({tmp})\"), as.integer(y/7))\n    print(glue::glue(\"{glue::glue_collapse(emoji)}: {y}\\n\\n\"))\n  } else {\n    emoji &lt;- rep(x, as.integer(y/7))\n    print(glue::glue(\"{glue::glue_collapse(emoji)}: {y}\\n\\n\"))\n  }\n}\n\nNow let‚Äôs walk it out üíÉ.\n\npurrr::walk2(top_emojis$emoji, top_emojis$count, render_emoji)\n\n[1] ‚Äúüëãüëãüëãüëãüëãüëãüëãüëãüëãüëãüëãüëãüëãüëãüëãüëãüëãüëã: 130‚Äù\n[2] ‚Äúüëçüëçüëçüëçüëçüëçüëçüëçüëçüëçüëçüëçüëçüëçüëçüëçüëçüëç: 129‚Äù\n[3] ‚Äú‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è: 83‚Äù\n[4] ‚Äú: 75‚Äù [5] ‚Äúüéâüéâüéâüéâüéâüéâüéâüéâüéâ: 65‚Äù\n[6] ‚Äúüêìüêìüêìüêìüêì: 36‚Äù\n[7] ‚ÄúüòÇüòÇüòÇüòÇ: 34‚Äù\n[8] ‚Äú‚ûï‚ûï‚ûï‚ûï: 29‚Äù\n[9] ‚Äúüçªüçªüçªüçª: 28‚Äù\n[10] ‚Äú: 27‚Äù\nHow delightful! It is no surprise that the üëã is the most popular emoji used, since rOpenSci is an extraordinarily welcoming crew! In fact the community organizer, Stefanie, just wrote a blog post about our Slack #welcome channel. I am (obviously) very proud to see that üêì makes a strong appearance (#rchickenladies), and of course delighted to see our Aussie friends getting represented by the Aussie party parrot."
  },
  {
    "objectID": "posts/2020-04-09-bayes-theorem-and-probabilty-of-covid-19-infection/index.html",
    "href": "posts/2020-04-09-bayes-theorem-and-probabilty-of-covid-19-infection/index.html",
    "title": "Bayes Theorem and the Probability of Having COVID-19",
    "section": "",
    "text": "I‚Äôve seen a few papers describing the characteristics of people who tested positive for SARS-CoV-2 and this is sometimes being interpreted as describing people with certain characteristic‚Äôs the probability of infection. Let‚Äôs talk about why that‚Äôs likely not true.\nr emo::ji(\"point_right\") Usually when thinking about estimating the prevalence of a disease, we use the sensitivity and specificity of the test to help us\nr emo::ji(\"point_right\") The calculations assume that everyone is equally likely to get tested, and with SARS-CoV-2 that is likely not the case\nLet‚Äôs do some r emo::ji(\"thought_balloon\") thought experiments. For these, my goal is to estimate the probability of being infected with r emo::ji(\"microbe\") SARS-CoV-2 given you have r emo::ji(\"jigsaw\") Disease X\nFor example,r emo::ji(\"jigsaw\") Disease X could be:\nr emo::ji(\"heart_suit\") heart disease\nr emo::ji(\"rage\") hypertension\nr emo::ji(\"heavy_plus_sign\") it could also be any subgroup (for example age, etc)\nIn these r emo::ji(\"thought_balloon\") thought experiments, we don‚Äôt actually have perfect information about who is infected with r emo::ji(\"microbe\") SARS-CoV-2, we just know among those who are r emo::ji(\"test_tube\") tested who has been infected with r emo::ji(\"microbe\") SARS-CoV-2. This is really the crux of the matter.\nFor these r emo::ji(\"thought_balloon\") thought experiments, assume that the current tests are perfect (that is there are 0 false positives and 0 false negatives)\nr emo::ji(\"point_up\") Note that this is likely not the case, with the current testing framework false positives (+) are unlikely but false negatives (-) may be occurring.\nWe want the probability of being infected with SARS-CoV-2 given you have Disease X: P(r emo::ji(\"microbe\")|r emo::ji(\"jigsaw\"))\nTo get this, we need P(r emo::ji(\"jigsaw\")|r emo::ji(\"microbe\")) because based on Bayes‚Äô Theorem we know:\nP(r emo::ji(\"microbe\")|r emo::ji(\"jigsaw\")) = P(r emo::ji(\"jigsaw\")|r emo::ji(\"microbe\"))P(r emo::ji(\"microbe\")) / P(r emo::ji(\"jigsaw\"))\nBUT, instead of P(r emo::ji(\"jigsaw\")|r emo::ji(\"microbe\")), we actually have P(r emo::ji(\"jigsaw\")|r emo::ji(\"microbe\"), r emo::ji(\"test_tube\")) - the probability of having disease X given you have SARS-CoV-2 AND you were tested. So the crux of these thought experiments will be trying to get an accurate estimate of P(r emo::ji(\"jigsaw\")|r emo::ji(\"microbe\")) so that we can get back to P(r emo::ji(\"microbe\")|r emo::ji(\"jigsaw\"))."
  },
  {
    "objectID": "posts/2020-04-09-bayes-theorem-and-probabilty-of-covid-19-infection/index.html#thought-experiment-1-best-case-scenario",
    "href": "posts/2020-04-09-bayes-theorem-and-probabilty-of-covid-19-infection/index.html#thought-experiment-1-best-case-scenario",
    "title": "Bayes Theorem and the Probability of Having COVID-19",
    "section": "Thought experiment 1Ô∏è‚É£: Best case scenario",
    "text": "Thought experiment 1Ô∏è‚É£: Best case scenario\nr tufte::margin_note(\"Note: all of these numbers are made up!\") r emo::ji(\"jigsaw\") 20% of the population has disease X\nr emo::ji(\"microbe\") 50% are infected with SARS-CoV-2 r emo::ji(\"x\") There is no relationship between disease X and SARS-CoV-2\nr emo::ji(\"test_tube\") People with disease X are just as likely to get tested than people without disease X\n\nWhy is thought experiment 1Ô∏è‚É£ a best case scenario?\nIt looks like:\nr emo::ji(\"test_tube\") 50% have SARS-CoV-2 infection among those tested\nr emo::ji(\"test_tube\") Of those who tested positive, the prevalence of disease X is 20%\nP(r emo::ji(\"microbe\")|r emo::ji(\"jigsaw\")) = 50%\nr emo::ji(\"white_heavy_check_mark\") Reality (no relationship between disease X and SARS-CoV-2) matches what we see"
  },
  {
    "objectID": "posts/2020-04-09-bayes-theorem-and-probabilty-of-covid-19-infection/index.html#thought-experiment-2-oversampling-scenario",
    "href": "posts/2020-04-09-bayes-theorem-and-probabilty-of-covid-19-infection/index.html#thought-experiment-2-oversampling-scenario",
    "title": "Bayes Theorem and the Probability of Having COVID-19",
    "section": "Thought experiment 2Ô∏è‚É£: Oversampling scenario",
    "text": "Thought experiment 2Ô∏è‚É£: Oversampling scenario\nr emo::ji(\"jigsaw\") 20% of the population has disease X Microbe 50% have SARS-CoV-2 infection\nr emo::ji(\"cross_mark\") There is no relationship between disease X and SARS-CoV-2\nr emo::ji(\"test_tube\") People with disease X are 2x more likely to get tested than people without disease X\n\nWhy is thought experiment 2Ô∏è‚É£ bad?\nIt looks like:\nr emo::ji(\"test_tube\") 50% have SARS-CoV-2 infection among those tested\nr emo::ji(\"test_tube\") Of those who tested positive for SARS-CoV-2, the prevalence of disease X is 33% r emo::ji(\"face_screaming_in_fear\")\nr emo::ji(\"cross_mark\") If we plug in what we see (P(r emo::ji(\"jigsaw\")|r emo::ji(\"microbe\"), r emo::ji(\"test_tube\")) for P(r emo::ji(\"jigsaw\")|r emo::ji(\"microbe\"))), it looks like P(r emo::ji(\"microbe\")|r emo::ji(\"jigsaw\")) is 82.5%, when in reality it is 50%."
  },
  {
    "objectID": "posts/2020-04-09-bayes-theorem-and-probabilty-of-covid-19-infection/index.html#thought-experiment-3-undersampling-scenario",
    "href": "posts/2020-04-09-bayes-theorem-and-probabilty-of-covid-19-infection/index.html#thought-experiment-3-undersampling-scenario",
    "title": "Bayes Theorem and the Probability of Having COVID-19",
    "section": "Thought experiment 3Ô∏è‚É£: Undersampling scenario",
    "text": "Thought experiment 3Ô∏è‚É£: Undersampling scenario\nr emo::ji(\"jigsaw\") 20% of the population has disease X\nr emo::ji(\"microbe\") 50% have SARS-CoV-2 infection\nr emo::ji(\"cross_mark\") There is no relationship between disease X and SARS-CoV-2\nr emo::ji(\"test_tube\") People with disease X are 1/2 as likely to get tested than people without disease X\n\nWhy is thought experiment 3Ô∏è‚É£ bad?\nIt looks like: r emo::ji(\"test_tube\") 50% have SARS-CoV-2 infection among those tested\nr emo::ji(\"test_tube\") Of those who tested positive for SARS-CoV-2, the prevalence of disease X is 11%\nr emo::ji(\"cross_mark\") If we plug in what we see (P(r emo::ji(\"jigsaw\")|r emo::ji(\"microbe\"), r emo::ji(\"test_tube\")) for P(r emo::ji(\"jigsaw\")|r emo::ji(\"microbe\"))), it looks like P(r emo::ji(\"microbe\")|r emo::ji(\"jigsaw\")) is 27.5%, when in reality it is 50%."
  },
  {
    "objectID": "posts/2020-04-09-bayes-theorem-and-probabilty-of-covid-19-infection/index.html#thought-experiment-4-two-problems-scenario",
    "href": "posts/2020-04-09-bayes-theorem-and-probabilty-of-covid-19-infection/index.html#thought-experiment-4-two-problems-scenario",
    "title": "Bayes Theorem and the Probability of Having COVID-19",
    "section": "Thought experiment 4Ô∏è‚É£: two problems scenario",
    "text": "Thought experiment 4Ô∏è‚É£: two problems scenario\nr emo::ji(\"jigsaw\") 20% of the population has disease X\nr emo::ji(\"microbe\") 56% have SARS-CoV-2 infection\nr emo::ji(\"white_check_mark\") people with disease X are 1.6 times more likely to have SARS-CoV-2 infection, P(r emo::ji(\"microbe\")|r emo::ji(\"jigsaw\")) = 80%\nr emo::ji(\"test_tube\") People with disease X are 5 as likely to get tested than people without disease X\n\nWhy is thought experiment 4Ô∏è‚É£ bad?\nIt looks like:\nr emo::ji(\"microbe\")``r emo::ji(\"test_tube\") 66% have SARS-CoV-2 infection among those tested\nr emo::ji(\"jigsaw\")``r emo::ji(\"test_tube\") Of those who tested positive for SARS-CoV-2, the prevalence of disease X is 66%\nr emo::ji(\"cross_mark\") We‚Äôre getting both the prevalence of SARS-CoV-2 and it‚Äôs association with Disease X wrong"
  },
  {
    "objectID": "posts/2020-04-09-bayes-theorem-and-probabilty-of-covid-19-infection/index.html#how-can-we-fix-this",
    "href": "posts/2020-04-09-bayes-theorem-and-probabilty-of-covid-19-infection/index.html#how-can-we-fix-this",
    "title": "Bayes Theorem and the Probability of Having COVID-19",
    "section": "How can we fix this?",
    "text": "How can we fix this?\nOKAY, scenarios finished, so hopefully this highlights why we can‚Äôt take the prevalence of characteristics in the tested positive population as the prevalence of characteristics in the overall population with a SARS-CoV-2 infection. Now, here are tips for how we can correct the numbers.\nScenario 2Ô∏è‚É£: Oversampling by 2x\nr emo::ji(\"point_right\") take those with disease X that tested positive for SARS-CoV-2 and downweight them by a factor of 2.\nr emo::ji(\"white_heavy_check_mark\") the adjusted prevalence of Disease X among those that tested positive for SARS-CoV-2 (0.5 / 2.5) = 0.2 (20%)\nP(r emo::ji(\"microbe\")|r emo::ji(\"jigsaw\")) = 50%\n\nScenario 3Ô∏è‚É£: Undersampling by 1/2\nr emo::ji(\"point_right\") take those with disease X that tested positive for SARS-CoV-2 and upweight them by a factor of 2.\nr emo::ji(\"white_heavy_check_mark\") the adjusted prevalence of Disease X among those that tested positive for SARS-CoV-2 (2/ 10) = 0.2 (20%)\nP(r emo::ji(\"microbe\")|r emo::ji(\"jigsaw\")) = 50%\n\nScenario 4Ô∏è‚É£: Two problems\nFor the prevalence of SARS-CoV-2 infections, correct by weighing by the probability of being tested in each subgroup. Here:\nr emo::ji(\"jigsaw\") = disease X\nr emo::ji(\"x\")``r emo::ji(\"jigsaw\") = No disease X\nP(r emo::ji(\"microbe\")) = P(r emo::ji(\"microbe\") | r emo::ji(\"jigsaw\")) P(r emo::ji(\"jigsaw\")) + P(r emo::ji(\"microbe\") | r emo::ji(\"x\")``r emo::ji(\"jigsaw\")) P(r emo::ji(\"x\")``r emo::ji(\"jigsaw\"))\nr emo::ji(\"white_check_mark\")P(r emo::ji(\"microbe\")) = ‚Öò * 0.2 + ¬Ω * 0.8 = 56%\nSaid another way, for calculating the overall prevalence of SARS-CoV-2, this is like downweighting the oversampled Disease X people (divide by 5).\nr emo::ji(\"white_check_mark\") P(r emo::ji(\"microbe\")) = (‚Öò + 2) / (‚Öò + 2 + ‚Öï + 2) = 0.56\nFor calculating the prevalence of disease X among those with SARS-CoV-2 infections:\nr emo::ji(\"white_check_mark\") P(r emo::ji(\"jigsaw\") | r emo::ji(\"microbe\")) = P(r emo::ji(\"microbe\") | r emo::ji(\"jigsaw\")) P(r emo::ji(\"jigsaw\")) / P(r emo::ji(\"microbe\")) = ‚Öò * 0.2 / 0.56 = 0.285\nAgain, downweight the oversampled Disease X population (divide by 5).\nr emo::ji(\"white_check_mark\") P(r emo::ji(\"jigsaw\") | r emo::ji(\"microbe\")) = ‚Öò / (‚Öò + 2) = 0.285\nP(r emo::ji(\"microbe\") | r emo::ji(\"jigsaw\")) = 80%\n\nHopefully this is somewhat helpful when reading about characteristics of those who are currently testing positive for SARS-CoV-2. As always, please let me know if there is something I‚Äôve missed! r emo::ji(\"folded_hands\")"
  },
  {
    "objectID": "posts/2018-08-21-p-value-thoughts-a-twitter-follow-up/index.html",
    "href": "posts/2018-08-21-p-value-thoughts-a-twitter-follow-up/index.html",
    "title": "p-value thoughts: A twitter follow up",
    "section": "",
    "text": "A conversation about how ‚Äúconvincing‚Äù various studies were based on sample size and p-values led me to post the following poll on twitter.\n‚äïFun fact: This was my first ever Twitter poll! I‚Äôve avoided them because they seem hard to do well, but it led to fun conversations and thinking about the results was actually quite enjoyable! {{% tweet \"1029534591760195584\" %}}\nThe results definitely surprised me - while I somewhat expected the large study to win out, I definitely didn‚Äôt think it would by such a large margin. For what it‚Äôs worth, I would have voted for the small study if I could have voted in my own poll.\n‚äïYou can find Royall‚Äôs ‚ÄúThe Effect of Sample Size on the Meaning of Significance Tests‚Äù here. Unfortunately it is behind a paywall, but the first page is available has a lot of good discussion.My initial intention was to post this question, and follow up by posting an article by Royall titled ‚ÄúThe Effect of Sample Size on the Meaning of Significance Tests‚Äù where this conundrum is discussed. Dani√´l Lakens anticipated this and responded with a link to the same article as well!\n{{% tweet \"1029734016474390528\" %}}\nWhile this paper doesn‚Äôt necessarily tell you under which circumstance you ought to be more convinced, when I initially read it in Jeffrey Blume‚Äôs Advanced Topics course at Vandy, I walked away with two main impressions:\nI think I still stand by 2, but I want to delve a bit deeper into 1, since it seems most of the people in my biased twitter-sphere disagree with me. To do this, I am going to explore three scenarios. In all cases, there is a small study and a large study, and the p-values from both studies are the same."
  },
  {
    "objectID": "posts/2018-08-21-p-value-thoughts-a-twitter-follow-up/index.html#different-intervention-same-population",
    "href": "posts/2018-08-21-p-value-thoughts-a-twitter-follow-up/index.html#different-intervention-same-population",
    "title": "p-value thoughts: A twitter follow up",
    "section": "Different intervention, same population",
    "text": "Different intervention, same population\nThis first scenario is the one I was considering when I posted the poll (and when I initially read Royall‚Äôs paper). This definitely demonstrates one way twitter polls are so delightfully flawed ‚Äì they often don‚Äôt leave enough room to state all assumptions ‚Äì but in this case I actually think this lead to really neat discussion, since differing views came up in the replies!\nIn order to demonstrate what I mean here, I am going to set up a small simulation. We have two studies, Study A and Study B. Study A is ‚Äúsmall‚Äù, it has 16 participants. Study B is ‚Äúlarge‚Äù, it has 10,000 participants. Both are examining new blood pressure drugs, drug A and drug B, and comparing them to ‚Äústandard of care‚Äù. The study populations are the same, let‚Äôs say the distribution of systolic blood pressure in this population is Normal with a mean of 145 and a standard deviation of 30. I can generate this population in R with the following code:\n\nlibrary(tidyverse)\nset.seed(924)\npopulation &lt;- tibble(\n  baseline_bp = rnorm(10^6, mean = 145, sd = 30)\n)\n\nggplot(population, aes(baseline_bp)) + \n  geom_histogram(bins = 30)\n\n\n\n\nUnder these circumstances, in order for the two p-values to be the same, the effect size for the larger study, Study B, must be smaller. For example, let‚Äôs say the true effect of Drug A is -2.5, and the effect of Drug B is -0.1.\n\npopulation &lt;- population %&gt;%\n  mutate(bp_drug_a = baseline_bp - 2.5 + rnorm(10^6),\n         bp_drug_b = baseline_bp - 0.1 + rnorm(10^6),\n         bp_soc = baseline_bp + rnorm(10^6))\n\nLet‚Äôs create the sample for Study A and see what happens.\n\nset.seed(1)\n# sample 16 from the large population\nstudy_a &lt;- population[sample(1:10^6, 16), ]\n\n# update so the first half receive drug A and the second receive standard of care\nstudy_a &lt;- study_a %&gt;%\n  mutate(drug_a = case_when(\n    row_number() %in% 1:8 ~ 1,\n    TRUE ~ 0\n  ),\n  outcome = case_when(\n    drug_a == 1 ~ bp_drug_a,\n    TRUE ~ bp_soc\n  )) %&gt;%\n  select(baseline_bp, outcome, drug_a)\n\n# test association between drug A and blood pressure\nlm(outcome ~ baseline_bp + drug_a, data = study_a) %&gt;%\n  broom::tidy() %&gt;%\n  filter(term == \"drug_a\")\n\n# A tibble: 1 √ó 5\n  term   estimate std.error statistic  p.value\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 drug_a    -2.84     0.558     -5.08 0.000210\n\n\nAnd now the sample for Study B.\n\nset.seed(62)\n# sample 10000 from the large population\nstudy_b &lt;- population[sample(1:10^6, 10000), ]\n\n# update so the first half receive drug B and the second receive standard of care\nstudy_b &lt;- study_b %&gt;%\n  mutate(drug_b = case_when(\n    row_number() %in% 1:5000 ~ 1,\n    TRUE ~ 0\n  ),\n  outcome = case_when(\n    drug_b == 1 ~ bp_drug_b,\n    TRUE ~ bp_soc\n  )) %&gt;%\n  select(baseline_bp, outcome, drug_b)\n\n# test association between drug B and blood pressure\nlm(outcome ~ baseline_bp + drug_b, data = study_b) %&gt;%\n  broom::tidy() %&gt;%\n  filter(term == \"drug_b\")\n\n# A tibble: 1 √ó 5\n  term   estimate std.error statistic     p.value\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 drug_b   -0.104    0.0201     -5.19 0.000000220\n\n\nBoth of these have approximately the same p-value (0.0005), but the effect sizes are very different! Perhaps we can see this relationship a bit more clearly using confidence intervals.\n\nmod_a &lt;- lm(outcome ~ baseline_bp + drug_a, data = study_a)\nconfint_a &lt;- broom::tidy(mod_a) %&gt;%\n  bind_cols(broom::confint_tidy(mod_a)) %&gt;%\n  filter(term == \"drug_a\")\n\nWarning: confint_tidy is now deprecated and will be removed\nfrom a future release of broom. Please use the applicable\nconfint method.\n\nmod_b &lt;- lm(outcome ~ baseline_bp + drug_b, data = study_b)\nconfint_b &lt;- broom::tidy(mod_b) %&gt;%\n  bind_cols(broom::confint_tidy(mod_b)) %&gt;%\n  filter(term == \"drug_b\")\n\nWarning: confint_tidy is now deprecated and will be removed\nfrom a future release of broom. Please use the applicable\nconfint method.\n\nconfints &lt;- bind_rows(confint_a, confint_b)\n\nggplot(confints, aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) + \n  geom_pointrange() + \n  geom_hline(yintercept = 0, lty = 2) + \n  coord_flip()\n\n\n\n\nLooking at these results, if you were to ask me whether I would prefer to take Drug A or Drug B to lower my blood pressure, I‚Äôd say Drug A. Drug B is essentially detecting a clinically meaningless result. This came up a few times in the poll‚Äôs replies, for example here:\n{{% tweet \"1029588551162245120\" %}}\nIf you require a little more convincing, let‚Äôs introduce a third study that falls in the middle, Study C. This study has 100 participants and an effect size of -1. ‚äïHow am I picking these effect sizes? We know the p-value is a function of the effect size and standard error, so I‚Äôve chosen each effect size such that the ratio of the effect size to standard error would be equal. For example, in the first case, this is proportional to \\(-2.5\\sqrt{16}\\), and in the second case, this is proportional to \\(-0.1\\sqrt{10000}\\), both of which equal -10.\n\npopulation &lt;- population %&gt;%\n  mutate(bp_drug_c = baseline_bp - 1 + rnorm(10^6))\n\n\nset.seed(68)\n\n# sample 100 from the large population\nstudy_c &lt;- population[sample(1:10^6, 100), ]\n\n# update so the first half receive drug C and the second receive standard of care\nstudy_c &lt;- study_c %&gt;%\n  mutate(drug_c = case_when(\n    row_number() %in% 1:50 ~ 1,\n    TRUE ~ 0\n  ),\n  outcome = case_when(\n    drug_c == 1 ~ bp_drug_c,\n    TRUE ~ bp_soc\n  )) %&gt;%\n  select(baseline_bp, outcome, drug_c)\n\nmod_c &lt;- lm(outcome ~ baseline_bp + drug_c, data = study_c)\nconfint_c &lt;- broom::tidy(mod_c) %&gt;%\n  bind_cols(broom::confint_tidy(mod_c)) %&gt;%\n  filter(term == \"drug_c\")\n\nWarning: confint_tidy is now deprecated and will be removed\nfrom a future release of broom. Please use the applicable\nconfint method.\n\nconfints &lt;- bind_rows(confints, confint_c)\n\nggplot(confints, \n       aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) + \n  geom_pointrange() + \n  geom_hline(yintercept = 0, lty = 2) + \n  coord_flip()\n\n\n\n\nAs expected, Study C, which has approximately the same p-value as Study A and Study B, has an effect size that falls between the two, since the sample size falls between as well. This phenomenon was also explored in Peter Freeman‚Äôs The role of P-values in analysing trial results, as pointed out by Anupam Singh in the replies.\n{{% twitter \"1029620344410255360\" %}}\nOkay, so now I‚Äôve shown you why I would have chosen the small study in my poll. Note here the small study that I have simulated is truly better (in other words in the large population, Drug A really does have a much larger effect size when compared to B or C). If we were to sample 10,000 participants from the Drug A group from the large population, we would see the same effect size (-2.5) with a much tighter confidence interval; this isn‚Äôt just a spurious result."
  },
  {
    "objectID": "posts/2018-08-21-p-value-thoughts-a-twitter-follow-up/index.html#same-intervention-same-population",
    "href": "posts/2018-08-21-p-value-thoughts-a-twitter-follow-up/index.html#same-intervention-same-population",
    "title": "p-value thoughts: A twitter follow up",
    "section": "Same intervention, same population",
    "text": "Same intervention, same population\nLet‚Äôs now explore what would happen if we found the same p-value for the same intervention in the same population. I hadn‚Äôt really considered this case when I was posting the poll, but I think it is an important one to discuss! It was brought up a few times in the replies, for example:\n{{% twitter \"1029648273894727680\" %}}\nLet‚Äôs use my same population as above, but just examine Drug B. We have two studies, one with 16 participants analyzing the efficacy of Drug B, and one with 10,000 participants. The ‚Äútrue‚Äù underlying effect here is still -0.1, the p-value will be 0.0005, as we obtained in the section above. There are a few different ways I could imagine this happening.\n\nThe small study has a design flaw that biases the result\nThe small study gets a crazy large effect size by chance\n\nFor the purposes of this example, I am going to assume the study designs are the same (except for the sample size), and target the second cause. I can imagine there is some sample of 16 from our large population of a million that would result in this spurious result. Let‚Äôs see if we can find one.\n\nseed &lt;- 0\np_val &lt;- 1\nwhile (p_val != 0.0005) {\n  seed &lt;- seed + 1\n  set.seed(seed)\n  # sample 16 from the large population\n  study_b &lt;- population[sample(1:10^6, 16), ]\n  \n  # update so the first half receive drug B and the second receive standard of care\n  study_b &lt;- study_b %&gt;%\n    mutate(drug_b = case_when(\n      row_number() %in% 1:8 ~ 1,\n      TRUE ~ 0\n    ),\n    outcome = case_when(\n      drug_b == 1 ~ bp_drug_b,\n      TRUE ~ bp_soc\n    )) %&gt;%\n    select(baseline_bp, outcome, drug_b)\n  \n  # test association between drug B and blood pressure\n  p_val &lt;- lm(outcome ~ baseline_bp + drug_b, data = study_b) %&gt;%\n    broom::tidy() %&gt;%\n    filter(term == \"drug_b\") %&gt;%\n    pull(p.value) %&gt;%\n    round(4)\n}\n\nThis took a bit of time, but after 5124 tries, it did find a seed where it happens!\n\nset.seed(5125)\n# sample 16 from the large population\nstudy_b &lt;- population[sample(1:10^6, 16), ]\n\n# update so the first half receive drug B and the second receive standard of care\nstudy_b &lt;- study_b %&gt;%\n  mutate(drug_b = case_when(\n    row_number() %in% 1:8 ~ 1,\n    TRUE ~ 0\n  ),\n  outcome = case_when(\n    drug_b == 1 ~ bp_drug_b,\n    TRUE ~ bp_soc\n  )) %&gt;%\n  select(baseline_bp, outcome, drug_b)\n\n# test association between drug B and blood pressure\nlm(outcome ~ baseline_bp + drug_b, data = study_b) %&gt;%\n  broom::tidy() %&gt;%\n  filter(term == \"drug_b\")\n\n# A tibble: 1 √ó 5\n  term   estimate std.error statistic p.value\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 drug_b   -0.268     0.613    -0.437   0.669\n\n\nHere, the effect size is super exaggerated. We know the true effect size is -0.1, and yet here we are seeing an effect of -1.4. We are definitely seeing this by chance. Let‚Äôs look at the distribution of p-values in this small sample.\n\nget_p_val &lt;- function() {\n  study_b &lt;- population[sample(1:10^6, 16), ]\n  \n  # update so the first half receive drug B and the second receive standard of care\n  study_b &lt;- study_b %&gt;%\n    mutate(drug_b = case_when(\n      row_number() %in% 1:8 ~ 1,\n      TRUE ~ 0\n    ),\n    outcome = case_when(\n      drug_b == 1 ~ bp_drug_b,\n      TRUE ~ bp_soc\n    )) %&gt;%\n    select(baseline_bp, outcome, drug_b)\n  \n  # test association between drug B and blood pressure\n  lm(outcome ~ baseline_bp + drug_b, data = study_b) %&gt;%\n    broom::tidy() %&gt;%\n    filter(term == \"drug_b\") %&gt;%\n    pull(p.value)\n}\np_vals &lt;- tibble(\n  p_val = map_dbl(1:1000, ~get_p_val())\n)\nggplot(p_vals, aes(p_val)) + \n  geom_histogram(bins = 100) + \n  geom_vline(xintercept = 0.0005, color = \"red\")\n\n\n\n\nThe red line denotes 0.0005. In this run of 1,000 simulations, we only had 0 where the p-value was less than 0.0005.\nAll this is to say that if I were to observe two studies measuring the same intervention with the same study design, one was small and one was large, I would definitely trust the large study. Perhaps this is what three quarters of my twitter-sphere were thinking! This seems akin to Gelman‚Äôs ‚ÄúWhat doesn‚Äôt kill my significance makes it stronger‚Äù fallacy, brought up in the replies:\n{{% twitter \"1029777778806280193\" %}}\nIf a small and large study are evaluating the exact same thing, but show different results, I would certainly trust the large study more."
  },
  {
    "objectID": "posts/2018-08-21-p-value-thoughts-a-twitter-follow-up/index.html#same-intervention-different-populations",
    "href": "posts/2018-08-21-p-value-thoughts-a-twitter-follow-up/index.html#same-intervention-different-populations",
    "title": "p-value thoughts: A twitter follow up",
    "section": "Same intervention, different populations",
    "text": "Same intervention, different populations\n‚äïPHEW this post seems really long, sorry, dear readers! Here is a cute gif of a cat covered in chicks for your troubles. This brings us to our final scenario, measuring the same intervention on different populations. This is another scenario that I didn‚Äôt consider prior to posting this poll! A few people brought up the possibility by asking about the effect size, for example Miles McBain said:\n{{% twitter \"1029555525648703489\" %}}\nTo which I initially replied that I didn‚Äôt think that was possible. Nick then chimed in to set me straight, and after much back and forth I finally understood how that was possible. Essentially if the larger study ALSO had a larger standard deviation, proportional to the ratio of the square root of the sample sizes, this would happen. Here I‚Äôm not sure exactly which I would find more convincing, as they are really looking at different things."
  },
  {
    "objectID": "posts/2018-08-21-p-value-thoughts-a-twitter-follow-up/index.html#wrap-up",
    "href": "posts/2018-08-21-p-value-thoughts-a-twitter-follow-up/index.html#wrap-up",
    "title": "p-value thoughts: A twitter follow up",
    "section": "Wrap up",
    "text": "Wrap up\nSO there we have it! This poll generated a lot of fun discussion, please feel free to let me know if you think I‚Äôve left off an important scenario to consider. There are so many more fun things to discuss when thinking about what makes studies ‚Äúconvincing‚Äù. None of these simulations take into account systematic bias, for example, from which a large sample size will never save you! Perhaps that can be left for a post for another day on the trade-off between bias and power. After all of this, my real conclusions remain similar to how I felt previously,\nü§î p-values are hard\nüïØ confidence intervals shed more light on the situation\nüéâ twitter is a fun way to spur good discussion!"
  },
  {
    "objectID": "posts/2019-08-24-extending-the-analogy-the-boy-you-cried-wolf-p-hackin/index.html",
    "href": "posts/2019-08-24-extending-the-analogy-the-boy-you-cried-wolf-p-hackin/index.html",
    "title": "Extending the analogy: The boy who cried wolf was p-hacking!",
    "section": "",
    "text": "During my postdoc with Jeff Leek, we worked on a few p-value, study design, and p-hacking \"explainers\". Two of these were incorporated into TED-Ed cartoons (The totally ironically named (NOT BY ME) This one weird trick will help you spot clickbait and the less ironic Can you spot the problem with these headlines?), but the analogy written about here was never used, so here it is!\nr tufte::margin_note(\"In math-speak this can be written as $p = P(X \\\\ge x | H_0)$, the probability of seeing something as extreme or more extreme than what was observed, $x$, given the null hypothesis, $H_0$, is true. Here, we're assuming more extreme events are BIG, hence $\\\\ge$, this is known as a \\\"right tail\\\" event. Similarly, we could look to detect extremely small events, \\\"left tail\\\" events with $p = P(X \\\\le x | H_0)$ . Most often, we want to detect extremes in either direction, so a \\\"two tailed\\\" event. This is written as $p = 2 \\\\min \\\\{P(X \\\\le x | H_0), P(X \\\\ge x | H_0)\\\\}$ BUT I DIGRESS!\")\n\nWhat is a p-value\nLet‚Äôs start off by discussing what a p-value is. Often scientists want to test whether a hypothesis is true. Many times this hypothesis testing is set up such there there is a ‚Äúnull‚Äù hypothesis, in other words there is no relationship between a cause and effect, and an ‚Äúalternative‚Äù hypothesis. A p-value is one way scientists determine the significance of their results. It is a number between 0 and 1 that indicates the probability of observing a result as extreme or more extreme than what you are seeing given the null hypothesis is true. Smaller p-values (often p-values less than 0.05) indicate evidence against the null hypothesis.\n\n\nWhat is p-hacking\nThere once was a village in a small hillside. Here, the villagers all took turns watching the sheep. A young boy named Peter was finally old enough to take a turn, so one night he went out and began his watch. Peter was a restless boy, and easily bored, so within a short time he decided he needed to create some excitement. ‚ÄúWooooolf,‚Äù he cried, ‚ÄúWOOOLF!‚Äù. The villagers all came running to his aid to help save the sheep, but when they arrived, there was no wolf to be found. Peter created what is known as a false positive. The villagers thought there was a wolf, when in fact there was not. This can happen in scientific studies when a scientist rejects a null hypothesis that is actually true. r tufte::margin_note(glue::glue(\"$H_0:$ {emo::ji('no_good_woman')} {emo::ji('wolf')}, $H_A:$ {emo::ji('wolf')}\")) Here the ‚Äúnull hypothesis‚Äù is that there is not a wolf, and the alternative is that a wolf is present. r tufte::margin_note(glue::glue(\"In math-speak, the villagers first committed a Type I error, by thinking there WAS a {emo::ji('wolf')}, when in fact there was NOT.\"))As you may know from the famous Aesop‚Äôs Fable, Peter does this again and again, causing the villagers to eventually grow tired of the constant false positives. r tufte::margin_note(glue::glue(\"This subsequent error is known as a Type II error, thinking there WAS NOT a {emo::ji('wolf')}, when in fact there was WAS {emo::ji('scream')}\")) When one day there really is a wolf, Peter cries ‚ÄúWoolf, WOOOLF,‚Äù and the villagers do nothing, as they expect that it is another false positive. This time, the villagers commit a false negative error. In a previously unknown version of this tale, the villagers set up a deal with Peter to lay out some ground rules. r tufte::margin_note(\"More math-speak, this 5% is known as $\\alpha$\") Peter could only watch the sheep once a month and he could only cry wolf (a false positive) 5% of the time. This is similar to how scientists try to control the chance of getting a false positive by setting up their statistical tests to have a fixed chance of rejecting the null hypothesis when it is true. These statistical tests often assume you are just testing one hypothesis. The first night this was implemented, if Peter cried ‚ÄúWOOLF!‚Äù it would be possible, but unlikely that it is a false positive. Because Peter was so bored, however, he soon found ways to get around these rules. Peter didn‚Äôt tell the villagers which night he was watching the sheep, so he actually watched them every night. Although he still abided by the 5% rule, since he was going every night the chances that there will be any false positives increased. Problems like this arise if scientists are testing many hypotheses. For example, if they test many hypotheses, all of which are truly null, each with a 5% chance of coming out with a false positive result, they will end up with an overall chance of getting a false positive across all tests much higher than 5%. In fact, they can test so many hypotheses that eventually it is inevitable that they will make a false positive error! This is essentially p-hacking ‚Äì testing many hypotheses until you see one that is significant.\n\n\nWhat are scientists doing about it\nThere are a couple of ways scientists are combating this potential issue. Instead of testing many things and only reporting the significant finding, scientists can use statistical techniques to account for all of the hypotheses that they have tested. In order to keep scientists on track, there has been a movement to pre-register study hypotheses. This would be like Peter agreeing to tell the villagers which night he would be watching the sheep in advance to ensure that he wasn‚Äôt actually watching them every night. In science, this is a way to pre-specify the hypothesis that will be tested, preventing scientists from looking at the data many different ways until they find something to report. An additional way to combat p-hacking is to see if multiple studies have shown the same result, in other words is the result reproducible. In Peter‚Äôs case, this would be akin to another villager watching the sheep with him and them both claiming to have seen a wolf. If we see the same result replicated elsewhere, we may be more likely to believe that we aren‚Äôt just seeing it by chance. The open science movement has been instrumental in helping with this cause, encouraging data and analysis steps to be made publicly available, eliminating the opportunity to surreptitiously p-hack. A final way to prevent p-hacking is to find ways to uplift scientists and incentivize them to be able to conduct research without the pressure to come up with significant results. There are many think pieces and new organizations working to achieve this mission. With Peter, perhaps we find a way for him to curb his boredom ‚Äì maybe we buy him a kindle so he can read instead of tormenting his fellow villagers."
  },
  {
    "objectID": "posts/2016-12-24-wait-what-are-p-values/index.html",
    "href": "posts/2016-12-24-wait-what-are-p-values/index.html",
    "title": "Wait, what are P-values?",
    "section": "",
    "text": "Frequently, and especially recently, misunderstandings of common statistical terms/ concepts have caused confusion and even anger. I would like to (attempt) to clear up a big player in the world of commonly used (and commonly misunderstood) statistical concepts: the p-value.\nStealing Lucy D‚ÄôAgostino McGowan‚Äôs XKCD embedding strategy.\n##TL DR\nA p-value is not a probability of the true parameter being something, but the percentage of times that the data you saw, or more extreme data, would occur given some ‚Äúnull‚Äù model. These are subtly, but importantly, different concepts."
  },
  {
    "objectID": "posts/2016-12-24-wait-what-are-p-values/index.html#setup",
    "href": "posts/2016-12-24-wait-what-are-p-values/index.html#setup",
    "title": "Wait, what are P-values?",
    "section": "Setup:",
    "text": "Setup:\nWe will illustrate this concept with a story.\nSay you are a cheating detection analyst at a casino. One day one of the casino‚Äôs employees comes up to you and tells you that there potentially are unfair coins being used in the casino (they seem to land on tails more frequently). It‚Äôs your job to figure out if they are fair or not. The employee hands you a piece of paper with something written on it and then runs away to attend to more important things than statistics. The paper says the following:\n\nHeads = \\(h\\), Tails = \\(t\\) | \\(t,t,h,t,t,h\\)\n\nAfter staring at this paper for a few minutes, you decide what you have is data on which face of a coin landed upright on a given flip, for a total of 6 flips. A fair coin in your opinion is one that has the same chance of falling on heads as it does tails, or 50-50. This is your null hypothesis: \\(P(\\text{tails}) = 0.5\\). The employee said they thought the coins were biased towards tails, you want to test if they are, this is your alternative hypothesis: \\(P(\\text{tails}) &gt; 0.5\\). Your job as a statistician is to take this incredibly complex data and distill it to a single decision, the coin is fair (null), or the coin is biased towards tails (alternative).\n\n  Fox Sports."
  },
  {
    "objectID": "posts/2016-12-24-wait-what-are-p-values/index.html#procedure",
    "href": "posts/2016-12-24-wait-what-are-p-values/index.html#procedure",
    "title": "Wait, what are P-values?",
    "section": "Procedure",
    "text": "Procedure\nYou have a problem: you don‚Äôt even know how to find an unfair coin (or how unfair of a coin to find). You do, however, have a normal quarter in your back pocket (that you‚Äôre sure is fair). You decide that instead of getting up and finding a tail-biased coin, you can use your quarter to test if the data you have is not from a fair coin. (You also enjoy injecting negatives into your statements to obfuscate your point as much as possible.)\nYou roll over to your coin flipping table, get out your laptop and flip your quarter 6 times.\n\n#Write down coin flip results\nflip_data &lt;-  data_frame(flip = c(\"tails\", \"tails\", \"heads\", \"heads\", \"tails\", \"tails\"))\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\n‚Ñπ Please use `tibble()` instead.\n\n#Plot the coin flips. \n#Code for plot_flips() is at the end of this document (it's ugly)\nflip_data %&gt;% plot_flips()\n\n\n\n\n\n\n\n\nOkay, so we got 2 heads on our 6 flips. Obviously the data given to us is from an unfair coin. You‚Äôre a good frequentist however, so you decide that, to be safe, you should repeat the experiment again to see what you get.\nOh no, you dropped your coin, better use R instead.\n\n#Function to flip our coin 6 times\nflip_coin &lt;- function(numberOfCoins = 6, probHeads = 0.5){\n  heads &lt;- runif(numberOfCoins) &gt; probHeads\n  return(data_frame(flip = ifelse(heads, \"heads\", \"tails\")))\n}\n\n#flip our virtual coin 6 times\nsecond_flip &lt;- flip_coin()\n\n#plot it\nsecond_flip %&gt;% plot_flips()\n\n\n\n\n\n\n\n\nOh look at that‚Ä¶ 2 heads‚Ä¶ that‚Äôs nice, but maybe we should do this a few more times. Maybe 100?\nBack to R‚Ä¶\n\n#Dataframe to hold our coin flips\nflip_results &lt;- data_frame(flip = character(), trial = numeric())\n\n#Number of trials of flipping our coin 6 times we want to do. \nnumber_of_trials &lt;- 100\n\n#Actually run the trials \nfor(trial_number in 1:number_of_trials){\n  \n  #Flip Coin 6 times and record results along with trial number\n  flips &lt;- flip_coin() %&gt;%\n    mutate(trial = trial_number)\n  \n  #Append this to our big results dataframe\n  flip_results &lt;- flip_results %&gt;% bind_rows(flips)\n}\n\n#Let's plot all of these results into one big mega-graph\nflip_results %&gt;% \n  plot_flips() + \n  facet_wrap(~trial) + #make a new mini plot for each trial\n  labs(title = \"Six Coin Flips | 100 Trials\")\n\n\n\n\n\n\n\n\nWell look at that, investigating this plot it doesn‚Äôt actually seem that out of the ordinary to get 4 tails in 6 flips, even though intuitively that sounds like tails happening twice as often as heads.\nJust to make sure lets simplify the above plot to summarize the number of tails we saw for each of our 100 trials.\n\n#Count how many tails we got in each trial\ntails_by_trial &lt;- flip_results %&gt;%   #Take our results\n  filter(flip == \"tails\") %&gt;%        #Look at only tails results\n  group_by(trial) %&gt;%                #Collapse per trial\n  summarise(number_of_tails = n())   #Count the number of heads per trial\n\n#Look at the first few of our results....\ntails_by_trial %&gt;% \n  head() %&gt;% \n  kable(align = c(\"r\", \"c\"))\n\n\n\n\ntrial\nnumber_of_tails\n\n\n\n\n1\n5\n\n\n2\n3\n\n\n3\n4\n\n\n4\n3\n\n\n5\n4\n\n\n6\n3\n\n\n\n\n\nLooking at the first few results we can see that we have a range of tails counts, looking at tables is boring though. Let‚Äôs plot our data to really see what‚Äôs going on.\n\ntails_by_trial %&gt;%\n  mutate(`bias to tails` = ifelse(number_of_tails &gt;= 4, \"equal or more \", \"less than observed data\")) %&gt;% \n  ggplot(aes(x = number_of_tails, fill = `bias to tails`)) + \n  geom_bar() + \n  labs(title = \"Number of Tails Seen in 6 Flips\", \n       subtitle = \"100 Trials\", x = \"# of tails\", y = \"times seen\") + \n  scale_x_continuous(breaks = 0:6) +               #Beyond here is unneccesary ggplot style stuff. \n  theme_minimal() +                                #I like pretty graphs\n  theme(panel.grid.major.x = element_blank(),\n        panel.grid.major.y = element_line( size=.1, color=\"black\" ),\n        panel.grid.minor = element_blank(),\n        legend.position = \"top\") + \n  scale_fill_discrete(guide = guide_legend(reverse=T))\n\nWarning: The `size` argument of `element_line()` is deprecated as of\nggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\nThe data we have: 4 tails out of 6 flips, looks pretty darn normal if our coin was fair. Case closed, right? Well you are a statistician so it‚Äôs your job to distill this down to a number, so let‚Äôs see exactly how ‚Äúnormal‚Äù our result is. We will do this by simply counting. Looking at the 100 trials that we did, how many times did the number of tails look at least as unfair as our data? Aka, how many times did we flip 4 or more tails in our 100 trials?\n\nunusualness &lt;- sum(tails_by_trial$number_of_tails &gt;= 4) \nunusualness\n\n[1] 31\n\n\nSo, we have just shown that, given the coin is truly fair, in 100 trials we saw 31 of them to be as ‚Äúbiased‚Äù towards tails as the data we were given. \\[\\frac{31}{100} = 0.31= \\boxed{\\text{our p-value}}\\]\nNote that this is not ‚Äúthe probability that our coin is not fair‚Äù, it is simply, ‚Äúgiven the coin was fair, how odd are our results?‚Äù\nNote: This is not actually how most p-value are calculated. This is because when lots of this p-value stuff was developed we didn‚Äôt have computers around to do our coin-flipping-bidding so smart people come up with mathematical formulas that describe the behavior, thus allowing p-values to be calculated with pen and paper. These formulas are also more accurate than what we did in that they mimic flipping the coin an infinite amount of times.\n\n  Me trying to understand p-values for the first time: starecat.com."
  },
  {
    "objectID": "posts/2016-12-24-wait-what-are-p-values/index.html#the-caveat",
    "href": "posts/2016-12-24-wait-what-are-p-values/index.html#the-caveat",
    "title": "Wait, what are P-values?",
    "section": "The Caveat",
    "text": "The Caveat\nBut wait, you can‚Äôt leave just yet. We made one very important assumption in constructing this p-value. We assumed the ‚Äúmodel‚Äù that our data came from. In this case we assumed that the ‚Äúheads‚Äù and ‚Äútails‚Äù written on the page were from a single coin, flipped 6 times with two possible results (‚Äúheads‚Äù or ‚Äútails‚Äù). What if it wasn‚Äôt the case? What if in fact our data came from a mysterious 3 sided coin (all coins technically are). Then our p-value is totally wrong.\nSomething to always be aware of when looking at statistical results is that, to quote statistician George Box,\n\nAll models are wrong, but some are useful.\n\nAlmost never in real life are the p-values you see in the newspaper or a scientific journal article using the perfectly correct model. Lots of work has been done to make sure that we‚Äôre not making huge mistakes (or else statisticians like me would be out of a job), but very rarely (even in the example just given) are we using the correct model to generate our p-value."
  },
  {
    "objectID": "posts/2016-12-24-wait-what-are-p-values/index.html#addendum",
    "href": "posts/2016-12-24-wait-what-are-p-values/index.html#addendum",
    "title": "Wait, what are P-values?",
    "section": "Addendum",
    "text": "Addendum\nI most likely made some mistakes somewhere in this article. If you catch them and feel them important enough to be fixed send me a message on twitter or if you are feeling particularly altruistic, submit a pull request on the repo for this article\nHere is the plotting code I used for the head tails plots. It‚Äôs kind of ugly and I‚Äôm sure there‚Äôs a more elegant way to code it.\n\n#Function for plotting coin flips. \n#Takes a dataframe with at least the column \"flip\" containing heads or tails in character value\nplot_flips &lt;- function(flip_data){\n  flip_data %&gt;% \n    mutate(value = 1, flip_num = 1:dim(flip_data)[1]) %&gt;% \n    ggplot(aes(x = flip, y = value, group = flip_num, fill = flip)) +\n    geom_bar(position = \"stack\", stat = \"identity\", color = \"white\") +\n    labs(x = \"\", y = \"times seen\", title = \"Six Coin Flips\") +\n    theme_minimal() + \n    theme(panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank(),\n          strip.background = element_blank(),\n          strip.text = element_blank(),\n          axis.title = element_blank(),\n          axis.text = element_blank(),\n          axis.ticks = element_blank(),\n          legend.position = \"bottom\",\n          legend.title = element_blank()) \n}"
  },
  {
    "objectID": "posts/2017-02-24-intro-to-GMD/index.html#the-problem",
    "href": "posts/2017-02-24-intro-to-GMD/index.html#the-problem",
    "title": "Intro to GMD",
    "section": "The Problem",
    "text": "The Problem\nYou are working in a collaborative situation working on some form of analysis. You want to produce a nice looking document of your work at the end and have easy contribution from all sources."
  },
  {
    "objectID": "posts/2017-02-24-intro-to-GMD/index.html#the-solutions",
    "href": "posts/2017-02-24-intro-to-GMD/index.html#the-solutions",
    "title": "Intro to GMD",
    "section": "The Solutions",
    "text": "The Solutions\nSit in one room and have one person write up all your work as you do it\nProblems:\n\n  source\n\nUse github with branches and pull requests.\nProblems:\n\nRequires groupmates to all get second PhD in git.\n\n\n  source (This was cited as a good version too.)\n\n\nWrite everything in a collaborative editor like google docs.\nProblems:\n\nYou have to copy and paste every single time you want to change your code, group member changed something above that you forgot to copy and paste? Oh well.\n\n\n   source\n\n\n\nUse GMD!\nProblems:\n\nIt‚Äôs brand new software and it will break.\n\n\n   source This may be my favorite image on the internet."
  },
  {
    "objectID": "posts/2017-02-24-intro-to-GMD/index.html#install",
    "href": "posts/2017-02-24-intro-to-GMD/index.html#install",
    "title": "Intro to GMD",
    "section": "Install",
    "text": "Install\n\n#in case you don't already have devtools\ninstall.packages(\"devtools\")\n\n#install GMD from our github repo. \ndevtools::install_github(\"LFOD/GMD\")\n\nSweet, now you have GMD installed (and loaded). Let‚Äôs actually do something with it."
  },
  {
    "objectID": "posts/2017-02-24-intro-to-GMD/index.html#setup-google-doc",
    "href": "posts/2017-02-24-intro-to-GMD/index.html#setup-google-doc",
    "title": "Intro to GMD",
    "section": "Setup Google Doc",
    "text": "Setup Google Doc\nFirst we will go make a nice google doc and share it with our collaborators.\n\nNote: set this up exactly  as you would a normal  RMarkdown file with the  yaml and code chunks  - Lucy\n\nI‚Äôve gone through the liberty of writing this intro in one so you can check it out.\nSimply grab the share link from the sharing settings and send it out to people. You can use this one\nhttps://docs.google.com/document/d/13jSZZ7o7xtQqFn8a8Po5a6nsKLBJEZl01s4PwY8KruM/edit\nNow that you and your collaborators have the share link simply hop back into R run these commands."
  },
  {
    "objectID": "posts/2017-02-24-intro-to-GMD/index.html#authenticate-with-google",
    "href": "posts/2017-02-24-intro-to-GMD/index.html#authenticate-with-google",
    "title": "Intro to GMD",
    "section": "Authenticate with Google",
    "text": "Authenticate with Google\n\nlibrary(GMD) #if you didnt just install\nlibrary(dplyr) #We made it pipe friendly \n\n#grab an authentication token\ntoken &lt;- get_auth()\n\nThe function get_auth() communicates with google‚Äôs server and authenticates you as a google user allowing you to access the google docs that you own or are shared with you. We store this authentication ‚Äútoken‚Äù as a variable for later use."
  },
  {
    "objectID": "posts/2017-02-24-intro-to-GMD/index.html#download-your-doc",
    "href": "posts/2017-02-24-intro-to-GMD/index.html#download-your-doc",
    "title": "Intro to GMD",
    "section": "Download your Doc",
    "text": "Download your Doc\nNow simply send the url you have to the command GMD‚Ä¶\n\nurl &lt;- \"https://docs.google.com/document/d/13jSZZ7o7xtQqFn8a8Po5a6nsKLBJEZl01s4PwY8KruM/edit\"\n\nmyDoc &lt;- GMD(url, token, output_name = \"gmd_rocks\")\n\nAs long as no errors popped up, you should be in business.\nYou will notice that we set a variable equal to our function. What did we do exactly?\nThe function GMD actually returns another function. Now that you have the function saved as myDoc that has been associated with the given google doc link all you need to do to download the latest from your doc is simply call it.\n\nmyDoc()\n\nThat‚Äôs practically it. Now you can just run these few lines (or one if you pipe) and then anytime you want to update your copy of the google doc with what‚Äôs been added just run your function."
  },
  {
    "objectID": "posts/2017-02-24-intro-to-GMD/index.html#but-wait-theres-more.",
    "href": "posts/2017-02-24-intro-to-GMD/index.html#but-wait-theres-more.",
    "title": "Intro to GMD",
    "section": "But Wait, There‚Äôs More.",
    "text": "But Wait, There‚Äôs More.\nIt‚Äôs all fine and dandy that you can grab your document easily now, but really that saved you what, like 3 clicks? What if you wanted to essentially use google docs as the text editor for R? Well, you can do that too.\n\nspeediness &lt;- 1 #how many seconds between redownloads\nmyDoc %&gt;% live_update(refresh_rate = speediness)\n\nThis now takes your document function and runs it every second (or however often you desire). Now you can simply write your code in google docs and have it show up here."
  },
  {
    "objectID": "posts/2021-08-17-vaccine-effectiveness-and-breakthrough-cases/index.html",
    "href": "posts/2021-08-17-vaccine-effectiveness-and-breakthrough-cases/index.html",
    "title": "Vaccine effectiveness and breakthrough cases",
    "section": "",
    "text": "I‚Äôm seeing lots of confusion around the frequency of breakthrough cases and the effectiveness of vaccines (in fact, a recent interview I did resulted in a confusing headline on this topic!) so let‚Äôs dive in!\nVaccine effectiveness is a relative measure, it tells us how protected you will be relative to an unvaccinated person. Even with delta, this looks ok for infections (and very good for severe illness)\n\nScenario 1:\nü§í if an unvaccinated person has a 10% chance of getting sick\nüíâ and we think the vaccine effectiveness is 60%\nüí™ a vaccinated person‚Äôs chance of getting sick is only 4%\nWhy? Here‚Äôs the math:\n\nVaccine effectiveness is:\n(risk unvaxed - risk vaxed) / (risk unvaxed)\n(.10 - .04) / .10 = 0.6\n-or-\nRisk for vaccinated is:\nrisk unvaxed - (vaccine effectiveness x risk unvaxed)\n.10 - (.60 x .10) = .04\n\nBut what if there is more virus around / fewer mitigation efforts in place?\n\n\nScenario 2\nWith more virus around /fewer mitigation efforts in place, everyone‚Äôs risk may increase by 2.5x\nü§í an unvaccinated person has a 25% chance of getting sick\nüíâ we still think the vaccine effectiveness is 60%\nüí™ a vaccinated person‚Äôs chance of getting sick is 10%\nIn these scenarios, the vaccine effectiveness didn‚Äôt change, but the risk for the vaccinated increased because the overall risk increased. In fact in Scenario 2 the risk to the vaccinated person was equal to the risk of the unvaccinated person in Scenario 1!\nWhy does this matter? Breakthrough cases are going to be more frequent for this exact reason. It doesn‚Äôt mean the vaccine isn‚Äôt effective, it means everyone‚Äôs baseline risk is increasing because Covid-19 is just more prevalent everywhere.\nThere is hope, though! There are many things we can do to bring that baseline risk back down!\nüíâ get more people vaccinated\nüò∑ mask up\nüß™ make liberal use of testing\n‚ö†Ô∏è use caution when possible until things improve"
  },
  {
    "objectID": "posts/2017-11-08-lstm-as-baseball/index.html",
    "href": "posts/2017-11-08-lstm-as-baseball/index.html",
    "title": "LSTM neural nets as told by baseball",
    "section": "",
    "text": "Currently I am studying for my qualifying exams on which the topic is ‚Äúusing deep neural networks for classification of time series data.‚Äù One extremely popular neural net architecture for doing this is the LSTM, or Long Short Term Memory model. The LSTM is a relatively recent advance ‚äïIntroduced in 1997 by Hochreiter and Schmidhuber in the the class of networks known as Recurrent Neural Networks (RNNs).\nMy main problem when I was learning about LSTMs was the horrifically mathematical way the are first described. ‚äï The typical way an LSTM is first introduced. This is from The Beginers Guide to RNNs‚Ä¶ Hope you‚Äôre an advanced beginner. Almost always the first introduction you get to them is a giant hairball diagram with a bunch of proprietary symbols and shapes accompanied by some equally confusing math equations. Personally, all this does is make me question my decision to go to grad school, not help me understand what‚Äôs going on. After struggling through a lot of these introductions however, I have come up with a way that I use to intuitively understand what LSTM networks do using a baseball analogy. Hopefully it helps you too."
  },
  {
    "objectID": "posts/2017-11-08-lstm-as-baseball/index.html#problem-setup",
    "href": "posts/2017-11-08-lstm-as-baseball/index.html#problem-setup",
    "title": "LSTM neural nets as told by baseball",
    "section": "Problem Setup",
    "text": "Problem Setup\nYou and a friend are sitting in your house watching a baseball game. Your friend is a fair-weather baseball fan and really doesn‚Äôt care about the game‚Äôs details, just who‚Äôs winning. They decide that instead of wasting their time watching the game they can make animojis on their phone and just have you dictate the game to them.\nIn order to do this efficiently you need to devise a strategy that will allow you to give your friend what he needs to know, while keeping track of the game in your own head.\n ‚äïHow our baseball neural network is setup."
  },
  {
    "objectID": "posts/2017-11-08-lstm-as-baseball/index.html#quick-rnn-background",
    "href": "posts/2017-11-08-lstm-as-baseball/index.html#quick-rnn-background",
    "title": "LSTM neural nets as told by baseball",
    "section": "Quick RNN background",
    "text": "Quick RNN background\nRNNs are neural networks that keep track of what they have seen and use that to aid their classification or regression output for a given element in a sequence. For instance, if you were modeling a sentence, an RNN would see the letters COO and then, because it remembers its context from the previous three letters, will predict L as the next letter. This is contrasted with a naive approach which would not remember the previously seen data and would always predict E because it‚Äôs the most common letter in the English language.\n‚äïFun fact: recently Google switched over their translation system to use LSTMs. Language translation is a particularly good usecase for these types of models as sometimes words in early parts of a sentence matter a lot in later parts, but they don‚Äôt always occur the same distance before. Thus having a flexible model that can keep track of what it‚Äôs seen helps a great deal in translation.\nThere are many problems with the simple implementations of RNNs that are way too complicated to go into here and not the point of this post. If you want to know more about them I would recommend any combination of original LSTM paper, this excelent blog post by Chris Olah, and the chapter on RNNs from Goodfellow et al.‚Äôs deep learning textbook. All that you need to know is that, the LSTM solved many of the issues associated with training RNNs by using some clever strategies, mainly their ability to remember and forget using ‚Äúgates.‚Äù\n How our baseball RNN will work.\nTo recap, there are two main facts you need to remember in regards to RNNs (including their subset the LSTM):\n\nAn RNN takes an input and passes that input through a ‚Äòhidden‚Äô layer before returning its output of interest.\nThe hidden layers can communicate with each other through the sequence. Aka the hidden layer knows the values of itself at the previou time point."
  },
  {
    "objectID": "posts/2017-11-08-lstm-as-baseball/index.html#your-strategy-aka-an-lstm",
    "href": "posts/2017-11-08-lstm-as-baseball/index.html#your-strategy-aka-an-lstm",
    "title": "LSTM neural nets as told by baseball",
    "section": "Your strategy (aka an LSTM)",
    "text": "Your strategy (aka an LSTM)\n\n\n\nYour LSTM strategy for watching the game.\nYou decide to break your strategy for keeping your friend informed into three parts: what you care about, what you don‚Äôt care about anymore, and what your friend cares about.\n\nPart 1: What you care about (the input gate)\nSay a batter is up and he has two strikes. The next pitch comes in and he fouls it to right field. Since you can‚Äôt strike out on a foul ball this pitch essentially doesn‚Äôt matter to the current status of the game. ‚äïSure you could argue that it does in fact matter that the pitcher is more tired and the batter is more weary, but let‚Äôs be reasonable In the slot in your brain for keeping track of the batters risk of striking out you simply don‚Äôt change anything in response to this new information. What you have done is decided to supress the input of this new information.\n\n\nPart 2: What information no longer matters (the forget gate)\nLet‚Äôs continue with the current batter with two strikes. Say on the next pitch he hits a great fly to left field and the fielder misses it. He gets all the way to second for a double. ‚äï Of no importance, I just thought this was a nice looking gate. Source The previous knowledge that you were carrying with you, the fact that the count was two strikes, no longer is important to the game. Once a batter has gotten on base, even though he had two strikes before he got there that information no longer makes any difference to the game‚Äôs outcome. Because of this you decide you can forget this information because you will no longer need it.\n\n\nPart 3: What you tell your friend (the output gate)\nLet‚Äôs stay with the play we considered for part 2. In this case the batter has made it to a base, and thus the state of the game has changed. For instance if a home run is hit now, there would be two runs instead of just one. Due to this, you have chosen to remember the fact that there is a runner on second base. ‚äï SourceHowever, since your friend doesn‚Äôt care about anything but the score of the game, he would gain nothing from you telling him this information, so you simply omit it in your report of the play. So, while you still know it‚Äôs important to the potential future score of the game, at this time it has not changed anything so you supress your output to your friend."
  },
  {
    "objectID": "posts/2017-11-08-lstm-as-baseball/index.html#recap",
    "href": "posts/2017-11-08-lstm-as-baseball/index.html#recap",
    "title": "LSTM neural nets as told by baseball",
    "section": "Recap",
    "text": "Recap\nUsing these simple steps to simultaneously keep track of the information that may be important for the future plays, while at the same time keeping your friend updated for the current play, you have essentially become the hidden layer in an LSTM RNN.\nThere are plenty of other nitty gritty details about what exactly goes on in an LSTM but what is the most important about how they work is their ‚Äògates.‚Äô These gates help them efficiently keep track of dependencies in the outcome that are influenced by input events happening at flexible amounts of time before. Traditional RNNs simply take their previous knowledge and transfer it wholesale to themselves at the next time-point, which, as illustrated in part two, can be very wasteful, and hard to optimize.\n‚äï By far the easiest to parse technical diagram of the LSTM I‚Äôve found. Source\nRNNs in general are fantastically powerful algorithms because of their flexibility to learn the patterns in time that are important. This contrasts with old school methods like sliding windows, where the model is only capable of looking backwards a set amount of time, which is bad if an event of interest is further back than that window, or if most of data contained in the window is not neccesary for the current situation.\nI admit though that I wrote this article from the entirely unrealistic perspective of a person who has been reading literature on this for a good bit now, so it is very likely I glossed over some points that are critical to your grasp of the problem. If this happened, please don‚Äôt hesitate to send me angry messages on twitter or leave a comment below."
  },
  {
    "objectID": "posts/2020-05-21-survival-model-detective-2/index.html",
    "href": "posts/2020-05-21-survival-model-detective-2/index.html",
    "title": "Survival Model Detective: Part 2",
    "section": "",
    "text": "A paper by Grein et al.¬†was recently published in the New England Journal of Medicine examining a cohort of patients with COVID-19 who were treated with compassionate-use remdesivir. There are two things that were interesting about this paper:\nCheck out #1 in the Part 1 post. This post focuses on #2, in particular it focuses on competing risks. This criticism was made by Stefanos Bonovas and Daniele Piovani in a letter to the Editor a few days ago."
  },
  {
    "objectID": "posts/2020-05-21-survival-model-detective-2/index.html#what-is-their-question",
    "href": "posts/2020-05-21-survival-model-detective-2/index.html#what-is-their-question",
    "title": "Survival Model Detective: Part 2",
    "section": "What is their question?",
    "text": "What is their question?\nI believe the authors are interested in telling us about clinical improvement in this cohort of patients taking remdesivir, in particular they want to estimate the cumulative incidence of clinical improvement by 28 days. For the purposes of their analysis ‚Äúclinical improvement‚Äù is defined as being discharged alive or having a decrease of 2 points or more in a 6-level ordinal scale of oxygen support:\n\nECMO\n\nMechanical ventilation\n\nNIPPV\n\nHigh-flow oxygen\n\nLow-flow oxygen\nAmbient air\n\nThey use a Kaplan Meier plot to show this. Let‚Äôs recreate it first."
  },
  {
    "objectID": "posts/2020-05-21-survival-model-detective-2/index.html#recreate-their-plots",
    "href": "posts/2020-05-21-survival-model-detective-2/index.html#recreate-their-plots",
    "title": "Survival Model Detective: Part 2",
    "section": "Recreate their plots",
    "text": "Recreate their plots\nI spent some time trying to recreate their analysis using the data from Figure 2, and I wasn‚Äôt quite about to do it. So I‚Äôve painstakingly pulled every number from Figure 3A üòÖ\n\nlibrary(tidyverse)\nlibrary(survival)\nlibrary(survminer)\nlibrary(cowplot)\nlibrary(cmprsk)\nd &lt;- read_csv(\"https://raw.githubusercontent.com/LucyMcGowan/nejm-grein-reanalysis/master/data/data-fig-2.csv\")\n\n\nfig_3 &lt;- tibble(\n  time = c(4, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 9,\n           10, 10, 10, 11, 11, 11, 11, 12, 12, 13,\n           13, 13, 13, 14, 14, 15, 15, 16, 16, 16, \n           16, 17, 17, 17, 17, 18, 18, 20, 22, 22, \n           23, 23, 23, 25, 26, 27, 28, 28, 29, 33),\n  event = c(1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n            1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, \n            1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, \n            0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n            0)\n)\n\n\ns &lt;- survfit(\n  Surv(time = time, event = event) ~ 1, \n  data = fig_3\n)\n\nggsurvplot(fit = s, \n           risk.table = TRUE, \n           break.time.by = 4,\n           risk.table.y.text = FALSE,\n           ylab = \"Days\",\n           tables.theme = theme_cleantable(),\n           fun = \"event\")\n\n\n\n\nI haven‚Äôt quite figured out how to map these back to Figure 2, but it seems it replicates Figure 3A pretty well. Let‚Äôs estimate the cumulative incidence at 28 days:\n\ns_df &lt;- tibble(\n  time = s$time,\n  cumulative_incidence = 1 - s$surv\n)\ns_df %&gt;%\n  filter(time == 28)\n\n# A tibble: 1 √ó 2\n   time cumulative_incidence\n  &lt;dbl&gt;                &lt;dbl&gt;\n1    28                0.841\n\n\nLooks very similar to the 84% reported in the initial paper. Cool, what‚Äôs the problem? The idea main idea is to examine time to clinical improvement and assess at 28 days what the cumulative incidence of improvement is. The way a typical survival analysis (like the ones the authors did here) works is typically you follow a group of patients for a certain amount of time. If they have an event (for example in the analysis above if they improve) you record them as such and the time the event occurred, otherwise you censor them and record the final time they were observed. In the figure here we have 33 patients that improved and 20 that were censored. Sounds fine, what‚Äôs the problem? A key assumption is that censoring should be ‚Äúnon-informative‚Äù, in other words, the patient is followed for a certain amount of time, never has the event, and then is no longer followed. Our best guess is that we know they didn‚Äôt have the event up until the last day we saw them, so we censor them on that day. Maybe they‚Äôll have the event the next day, or maybe in a year - in order for the assumptions to be appropriately met, it must be the case that patients who have been censored are just as likely to have the event as those who are still being followed in the study. Let‚Äôs pull up Figure 2 again.\n\n\n\n\n\n7 of the patients in this cohort died. If someone dies we know that they are not going to improve later. This is not non-informative censoring!! Luckily there is a very straightforward way to deal with this in statistics - competing risks!"
  },
  {
    "objectID": "posts/2020-05-21-survival-model-detective-2/index.html#competing-risks",
    "href": "posts/2020-05-21-survival-model-detective-2/index.html#competing-risks",
    "title": "Survival Model Detective: Part 2",
    "section": "Competing Risks",
    "text": "Competing Risks\nIn a competing risk analysis, we can separate out the the death outcome from the remaining censored outcomes. We can then appropriately estimate the cumulative incidence of improving. I‚Äôve recoded the 7 deaths (guessed from Figure 2) below.\n\nfig_3_fixed &lt;- tibble(\n  time = c(4, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 9,\n           10, 10, 10, 11, 11, 11, 11, 12, 12, 13,\n           13, 13, 13, 14, 14, 15, 15, 16, 16, 16, \n           16, 17, 17, 17, 17, 18, 18, 20, 22, 22, \n           23, 23, 23, 25, 26, 27, 28, 28, 29, 33),\n  event = c(1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 1, 1, 2,\n            1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, \n            1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 2, 1, \n            2, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n            0)\n)\n\n\n\nInterested in learning more about competing risk analyses in R? Emily Zabor has an amazing tutorial on this\nLet‚Äôs see how the plot looks now.\n\nx &lt;- cuminc(fig_3_fixed$time,\n            fig_3_fixed$event, \n            cencode = 0)\n\nggcompetingrisks(x, \n                 conf.int = TRUE,\n                 gnames = c(\"Improvement Improvement\", \"Death Death\"),\n                 ggtheme = theme_classic())\n\n\n\n\nOk, let‚Äôs calculate the cumulative incidence now, taking death into account.\n\nx %&gt;% \n  map_df(`[`, c(\"time\", \"est\", \"var\"), .id = \"id\") %&gt;% \n  filter(id %in% c(\"1 1\"), time == 28) %&gt;%\n  slice(2)\n\n# A tibble: 1 √ó 4\n  id     time   est     var\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 1 1      28 0.734 0.00612\n\n\nThe estimated cumulative incidence of clinical improvement by day 28 is 73%. Here‚Äôs two plots that show the difference.\n\n\n\n\n\n\n\n\nWhy does this matter? It‚Äôs crucial that the questions we answer statistically match the ones we are posing in practice. Understanding the underlying assumptions of the models is so important!"
  },
  {
    "objectID": "posts/2020-05-21-survival-model-detective-2/index.html#figure-3b",
    "href": "posts/2020-05-21-survival-model-detective-2/index.html#figure-3b",
    "title": "Survival Model Detective: Part 2",
    "section": "Figure 3B",
    "text": "Figure 3B\nIt turns out really this is a story about Figure 3B. Why? Because 6/7 of the patients who died were on invasive oxygen support at baseline, so modeling this correctly has the largest impact on the Invasive line on this plot. Here they were examining the same outcome, stratifying by whether patient had invasive or noninvasive baseline oxygen support.\n\nfig_3_fixed &lt;- tibble(\n  time = c(4, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 9,\n           10, 10, 10, 11, 11, 11, 11, 12, 12, 13,\n           13, 13, 13, 14, 14, 15, 15, 16, 16, 16, \n           16, 17, 17, 17, 17, 18, 18, 20, 22, 22, \n           23, 23, 23, 25, 26, 27, 28, 28, 29, 33),\n  event = c(1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 1, 1, 2,\n            1, 1, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 0, \n            1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 2, 1, \n            2, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n            0),\n  invasive = c(0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, \n               0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, \n               1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, \n               1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n               1)\n)\n\n\nfit &lt;- survfit(\n  Surv(time, ifelse(event == 1, 1, 0)) ~ invasive, \n  data = fig_3_fixed\n)\nggsurvplot(\n  fit = fit, \n  risk.table = TRUE,\n  break.time.by = 4,\n  risk.table.y.text = FALSE,\n  xlim = c(0, 36),\n  tables.theme = theme_cleantable(),\n  conf.int = TRUE,\n  fun = \"event\",\n  legend.labs = c(\"Noninvasive\", \"Invasive\"))\n\n\n\n\nNow what happens if we do a competing risks model? Let‚Äôs see what that looks like now\n\nx &lt;- cuminc(fig_3_fixed$time, \n            fig_3_fixed$event, \n            group = fig_3_fixed$invasive,\n            cencode = 0)\n\n\nggcompetingrisks(x, \n                 conf.int = TRUE,\n                 gnames = c(\"Noninvasive Improvement\", \"Invasive Improvement\",\n                            \"Noninvasive Death\", \"Invasive Death\"),\n                 ggtheme = theme_classic(),\n                 ylim = c(0, 1))\n\n\n\n\nWe wouldn‚Äôt expect the Noninvasive group to change much, because only one patient in this stratum died. Let‚Äôs focus instead on just the Invasive group to compare how that line changes with a proper analysis.\n\n\n\n\n\n\n\n\nBasically, all this is to say that a competing risk analysis would have been more appropriate here. Hopefully this code-through has been helpful!"
  },
  {
    "objectID": "posts/2023-04-29-two-wrong-models-missing-data-style/index.html",
    "href": "posts/2023-04-29-two-wrong-models-missing-data-style/index.html",
    "title": "Are two wrong models better than one? Missing data style",
    "section": "",
    "text": "After my previous post about missing data, Kathy asked on Twitter whether two wrong models (the imputation model + the outcome model) would be better than one (the outcome model alone).\nThis is a great question! I am going to investigate via a small simulation (so the answer could be ‚Äúit depends‚Äù, but at least we will know how it seems to work in this very simple case) üòÜ.\nOk so here I have some predictor, x that is missing 50% of the time, dependent on c_x and c_y. The right imputation model would have c_x, the right outcome model needs c_y. Unfortunately, we only have access to one, which we will try to use in our imputation model (and outcome model). Let‚Äôs see whether two (wrong) models are better than one!\nA ‚Äúcorrect‚Äù model will be one that estimates that the coefficient for x is 1.\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mice)\n\nn &lt;- 1000\n\nset.seed(928)\n\ndata &lt;- tibble(\n  c_x = rnorm(n, sd = 0.71),\n  x = c_x + rnorm(n, sd = 0.71),\n  c_y = rnorm(n),\n  y = x + c_y + rnorm(n),\n  noise = rnorm(n),\n  x_miss = rbinom(n, 1, 1 / (1 + exp(-(c_x + c_y)))),\n  x_obs = ifelse(\n    x_miss,\n    NA,\n    x\n  )\n)"
  },
  {
    "objectID": "posts/2023-04-29-two-wrong-models-missing-data-style/index.html#we-only-have-c_x",
    "href": "posts/2023-04-29-two-wrong-models-missing-data-style/index.html#we-only-have-c_x",
    "title": "Are two wrong models better than one? Missing data style",
    "section": "We only have c_x",
    "text": "We only have c_x\nOk first let‚Äôs look at the whole dataset.\n\nmod_full_c_x &lt;- lm(y ~ x + c_x, data = data) |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_full_c_x\n\n# A tibble: 1 √ó 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     1.01    0.877      1.14\n\n\nThis checks out! c_x basically does nothing for us here, but because c_y is not actually a confounder (it just informs the missingness & y, which we aren‚Äôt observing here), we are just fine estimating our ‚Äúwrong‚Äù model in the fully observed data. Now let‚Äôs do the ‚Äúcomplete cases‚Äù analysis.\n\ndata_cc &lt;- na.omit(data)\nmod_cc_c_x &lt;- lm(y ~ x + c_x, data = data_cc) |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_cc_c_x\n\n# A tibble: 1 √ó 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.999    0.812      1.19\n\n\nThis does fine! Now let‚Äôs do some imputation. I am going to use the mice package.\n\nimp_data_c_x &lt;- mice(\n  data, \n  m = 5, \n  method = \"norm.predict\",\n  formulas = list(x_obs ~ c_x),\n  print = FALSE)\n\nOk let‚Äôs compare how this model does ‚Äúalone‚Äù.\n\nmod_imp_c_x &lt;- with(imp_data_c_x, lm(y ~ x_obs)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_imp_c_x\n\n  estimate  conf.low conf.high\n1 1.026666 0.9042858  1.149046\n\n\nGreat! This was the right model, so we would expect this to perform well.\nNow what happens if we adjust for c_x in addition in the outcome model:\n\nmod_double_c_x &lt;- with(imp_data_c_x, lm(y ~ x_obs + c_x)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_double_c_x\n\n   estimate  conf.low conf.high\n1 0.9991868 0.7968509  1.201523\n\n\nThe right imputation model with the wrong outcome model is fine!"
  },
  {
    "objectID": "posts/2023-04-29-two-wrong-models-missing-data-style/index.html#we-only-have-c_y",
    "href": "posts/2023-04-29-two-wrong-models-missing-data-style/index.html#we-only-have-c_y",
    "title": "Are two wrong models better than one? Missing data style",
    "section": "We only have c_y",
    "text": "We only have c_y\nOk first let‚Äôs look at the whole dataset.\n\nmod_full_c_y &lt;- lm(y ~ x + c_y, data = data) |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_full_c_y\n\n# A tibble: 1 √ó 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     1.01    0.946      1.08\n\n\nLooks good! Now let‚Äôs do the ‚Äúcomplete cases‚Äù analysis.\n\nmod_cc_c_y &lt;- lm(y ~ x + c_y, data = data_cc) |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_cc_c_y\n\n# A tibble: 1 √ó 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     1.01    0.909      1.11\n\n\nGreat! It works. This shows that as long as we have the right outcome model we can do complete case analysis even if the data is missing not at random (cool!). Now let‚Äôs do some imputation.\n\nimp_data_c_y &lt;- mice(\n  data, \n  m = 5, \n  method = \"norm.predict\",\n  formulas = list(x_obs ~ c_y),\n  print = FALSE)\n\nOk let‚Äôs compare how this model does ‚Äúalone‚Äù.\n\nmod_imp_c_y &lt;- with(imp_data_c_y,lm(y ~ x_obs)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_imp_c_y\n\n   estimate conf.low conf.high\n1 0.6888255 0.527796  0.849855\n\n\nOh no, very bad! The wrong imputation model is worse than complete case! By a lot! This estimate is off by 0.31. Does conditioning on c_y help us at all?\n\nmod_double_c_y &lt;-  with(imp_data_c_y, lm(y ~ x_obs + c_y)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_double_c_y\n\n  estimate  conf.low conf.high\n1 1.009655 0.8887281  1.130582\n\n\nPhew, the wrong imputation model with the wrong outcome model is back to being fine."
  },
  {
    "objectID": "posts/2023-04-29-two-wrong-models-missing-data-style/index.html#what-if-both-are-wrong",
    "href": "posts/2023-04-29-two-wrong-models-missing-data-style/index.html#what-if-both-are-wrong",
    "title": "Are two wrong models better than one? Missing data style",
    "section": "What if both are wrong?",
    "text": "What if both are wrong?\nOk, what if we just had our useless variable, noise.\n\nmod_full_noise &lt;- lm(y ~ x + noise, data = data) |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_full_noise\n\n# A tibble: 1 √ó 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.992    0.898      1.09\n\n\nThis is fine! c_x and c_y aren‚Äôt confoudners so we can estimate the coefficent for x without them ‚Äì noise doesn‚Äôt do anything, but it also doesn‚Äôt hurt. What about complete case?\n\nmod_cc_noise &lt;- lm(y ~ x + noise, data = data_cc) |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_cc_noise\n\n# A tibble: 1 √ó 3\n  estimate conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.887    0.748      1.03\n\n\nOops! We‚Äôve got bias (as expected!) ‚Äì we end up with a biased estimate by ~0.11.\nWhat if we build the (wrong) imputation model?\n\nimp_data_noise &lt;- mice(\n  data, \n  m = 5, \n  method = \"norm.predict\",\n  formulas = list(x_obs ~ noise),\n  print = FALSE) \n\nOk let‚Äôs compare how this model does ‚Äúalone‚Äù.\n\nmod_imp_noise &lt;-  with(imp_data_noise, lm(y ~ x_obs)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_imp_noise\n\n   estimate  conf.low conf.high\n1 0.8807755 0.7217368  1.039814\n\n\nThis is also wrong (womp womp!) What if we try two wrong models?\n\nmod_double_noise &lt;-  with(imp_data_noise,lm(y ~ x_obs + noise)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_double_noise\n\n   estimate  conf.low conf.high\n1 0.8865363 0.7275635  1.045509\n\n\nNope üò¢. Two wrong models here are not better than one! It‚Äôs worse! Womp womp.\nLet‚Äôs put these all together:\n\nbind_rows(\n  mod_full_c_x,\n  mod_full_c_y,\n  mod_full_noise,\n  mod_cc_c_x,\n  mod_cc_c_y,\n  mod_cc_noise,\n  mod_imp_c_x,\n  mod_imp_c_y,\n  mod_imp_noise,\n  mod_double_c_x,\n  mod_double_c_y,\n  mod_double_noise\n) |&gt;\n  mutate(\n    mod = factor(c(\"Full data with c_x\", \n                   \"Full data with c_y\", \n                   \"Full data with noise\",\n                   \"Complete case with c_x\",\n                   \"Complete case wtih c_y\",\n                   \"Complete case with noise\",\n                   \"Imputation with c_x\",\n                   \"Imputation with c_y\",\n                   \"Imputation with noise\",\n                   \"Two models with c_x\",\n                   \"Two models with c_y\",\n                   \"Two models with noise\" ),\n                 levels = c(\"Full data with c_x\", \n                            \"Complete case with c_x\",\n                            \"Imputation with c_x\",\n                            \"Two models with c_x\",\n                            \"Full data with c_y\", \n                            \"Complete case wtih c_y\",\n                            \"Imputation with c_y\",\n                            \"Two models with c_y\",\n                            \"Full data with noise\",\n                            \"Complete case with noise\",\n                            \"Imputation with noise\",\n                            \"Two models with noise\" )),\n    mod = fct_rev(mod),\n  ) -&gt; to_plot\n\nggplot(to_plot, aes(x = estimate, xmin = conf.low, xmax = conf.high, y = mod)) +\n  geom_pointrange() + \n  geom_vline(xintercept = 1, lty = 2)\n\n\n\n\nSo there you have it, two wrong models are rarely better than one."
  },
  {
    "objectID": "posts/2023-04-29-two-wrong-models-missing-data-style/index.html#addendum",
    "href": "posts/2023-04-29-two-wrong-models-missing-data-style/index.html#addendum",
    "title": "Are two wrong models better than one? Missing data style",
    "section": "Addendum!",
    "text": "Addendum!\nIn writing this post, I found that I was getting biased results when I was correctly specifying my imputation model when using the {mice} defaults (which is why the code above specifies norm.predict for the method, forcing it to use linear regression, as the data were generated). I didn‚Äôt understand why this is happening until some helpful friends on Twitter explained it (thank you Rebecca, Julian, and Mario. I‚Äôll show you what is happening and then I‚Äôll show a quick explanation. Let‚Äôs try to redo the imputation models using the defaults:\n\nimp_default_c_x &lt;- mice(\n  data, \n  m = 5, \n  formulas = list(x_obs ~ c_x),\n  print = FALSE)\n\n\nmod_imp_c_x &lt;- with(imp_default_c_x, lm(y ~ x_obs)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_imp_c_x\n\n   estimate  conf.low conf.high\n1 0.7353276 0.6194273 0.8512279\n\n\nBad!\n\nmod_double_c_x &lt;-  with(imp_default_c_x, lm(y ~ x_obs + c_x)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_double_c_x\n\n   estimate conf.low conf.high\n1 0.4602637  0.30023 0.6202974\n\n\nEven worse!!\n\nimp_default_c_y &lt;- mice(\n  data, \n  m = 5, \n  formulas = list(x_obs ~ c_y),\n  print = FALSE)\n\n\nmod_imp_c_y &lt;- with(imp_default_c_y, lm(y ~ x_obs)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_imp_c_y\n\n   estimate  conf.low conf.high\n1 0.3405789 0.1126282 0.5685295\n\n\nYIKES!\n\nmod_double_c_y &lt;-  with(imp_default_c_y, lm(y ~ x_obs + c_y)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_double_c_y\n\n   estimate  conf.low conf.high\n1 0.5128878 0.3216547 0.7041208\n\n\nBetter since we are conditioning on c_y (but still bad!)\n\nimp_default_noise &lt;- mice(\n  data, \n  m = 5, \n  formulas = list(x_obs ~ noise),\n  print = FALSE) \n\n\nmod_imp_noise &lt;-  with(imp_default_noise, lm(y ~ x_obs)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_imp_noise\n\n   estimate  conf.low conf.high\n1 0.4076967 0.2352155 0.5801779\n\n\nEEK!\n\nmod_double_noise &lt;-  with(imp_default_noise,lm(y ~ x_obs + noise)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_double_noise\n\n   estimate  conf.low conf.high\n1 0.4124791 0.2497624 0.5751959\n\n\nJust as bad..\nLet‚Äôs put those in the original plot:\n\nbind_rows(\n  mod_full_c_x,\n  mod_full_c_y,\n  mod_full_noise,\n  mod_cc_c_x,\n  mod_cc_c_y,\n  mod_cc_noise,\n  mod_imp_c_x,\n  mod_imp_c_y,\n  mod_imp_noise,\n  mod_double_c_x,\n  mod_double_c_y,\n  mod_double_noise\n) |&gt;\n  mutate(\n    mod = factor(c(\"Full data with c_x\", \n                   \"Full data with c_y\", \n                   \"Full data with noise\",\n                   \"Complete case with c_x\",\n                   \"Complete case wtih c_y\",\n                   \"Complete case with noise\",\n                   \"Default Imputation with c_x\",\n                   \"Default Imputation with c_y\",\n                   \"Default Imputation with noise\",\n                   \"Two models with c_x\",\n                   \"Two models with c_y\",\n                   \"Two models with noise\" ),\n                 levels = c(\"Full data with c_x\", \n                            \"Complete case with c_x\",\n                            \"Default Imputation with c_x\",\n                            \"Two models with c_x\",\n                            \"Full data with c_y\", \n                            \"Complete case wtih c_y\",\n                            \"Default Imputation with c_y\",\n                            \"Two models with c_y\",\n                            \"Full data with noise\",\n                            \"Complete case with noise\",\n                            \"Default Imputation with noise\",\n                            \"Two models with noise\" )),\n    mod = fct_rev(mod),\n  ) -&gt; to_plot\n\nggplot(to_plot, aes(x = estimate, xmin = conf.low, xmax = conf.high, y = mod)) +\n  geom_pointrange() + \n  geom_vline(xintercept = 1, lty = 2)\n\n\n\n\nAHH! This makes me so scared of imputation!!\nRebecca Andridge‚Äôs tweet finally helped me see why this is happening. The way the missing data is generated, larger values of c_x have a higher probability of missingness, and for particularly high values of c_x that probability is almost 1.\n\nggplot(data, aes(x = x, y = y, color = factor(x_miss))) +\n  geom_point() + \n  geom_vline(xintercept = 2.31, lty = 2) +\n  labs(color = \"missing\")\n\n\n\n\nTake a look at the plot above. We have no non-missing x values that are greater than 2.3. The way predictive mean matching (the default {mice} method) works is it finds the observation(s) that have the closest predicted value to the observation that is missing a data point and gives you that non-missing data point‚Äôs value. So here, we are essentially truncating our distribution at 2.3, since that is the highest value observed. Any value that would have been higher is going to be necessarily too small instead of the right value (this is different from the linear model method used in the first part of this post, which allows you to extrapolate). This is supposed to be a less biased approach, since it doesn‚Äôt allow you to extrapolate beyond the bounds of your observed data, but it can actually induce bias when you have pockets of missingness with no observed xs (which I would argue might happen frequently!). Here is an example of one of the imputed datasets, notice nothing is above that 2.3 line!\n\nggplot(complete(imp_default_c_x), aes(x = x_obs, y = y, color = factor(x_miss))) + \n  geom_point() + \n  scale_x_continuous(limits = c(-2.8, 3.1)) +\n  geom_vline(xintercept = 2.31, lty = 2) +\n  labs(color = \"imputed\")"
  },
  {
    "objectID": "posts/2023-04-29-two-wrong-models-missing-data-style/index.html#what-if-we-use-y-in-the-imputation-models",
    "href": "posts/2023-04-29-two-wrong-models-missing-data-style/index.html#what-if-we-use-y-in-the-imputation-models",
    "title": "Are two wrong models better than one? Missing data style",
    "section": "What if we use y in the imputation models",
    "text": "What if we use y in the imputation models\nIncluding y in the imputation model is definitely recommended, as was hammered home for me by the wonderful Frank Harrell, but I‚Äôm not sure this recommendation has permeated through the field yet (although this paper re-iterating this result just came out yesterday so maybe it is!).\nLet‚Äôs see how that improves our imputation models:\n\nimp_y_c_x &lt;- mice(\n  data, \n  m = 5, \n  formulas = list(x_obs ~ c_x + y),\n  print = FALSE)\n\n\nmod_imp_c_x &lt;- with(imp_y_c_x, lm(y ~ x_obs)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_imp_c_x\n\n  estimate  conf.low conf.high\n1 1.034138 0.9109341  1.157343\n\n\nBeautiful!\n\nmod_double_c_x &lt;-  with(imp_y_c_x, lm(y ~ x_obs + c_x)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_double_c_x\n\n  estimate  conf.low conf.high\n1  1.08074 0.8772936  1.284186\n\n\nA bit worse, but not bad!\n\nimp_y_c_y &lt;- mice(\n  data, \n  m = 5, \n  formulas = list(x_obs ~ c_y + y),\n  print = FALSE)\n\n\nmod_imp_c_y &lt;- with(imp_y_c_y, lm(y ~ x_obs)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_imp_c_y\n\n   estimate  conf.low conf.high\n1 0.9298631 0.8137157  1.046011\n\n\nNot bad!! A bit biased but way better.\n\nmod_double_c_y &lt;-  with(imp_y_c_y, lm(y ~ x_obs + c_y)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_double_c_y\n\n  estimate  conf.low conf.high\n1  1.01812 0.9415964  1.094644\n\n\nLove it, looks great after conditioning on c_y\n\nimp_y_noise &lt;- mice(\n  data, \n  m = 5, \n  formulas = list(x_obs ~ noise + y),\n  print = FALSE) \n\n\nmod_imp_noise &lt;-  with(imp_y_noise, lm(y ~ x_obs)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_imp_noise\n\n   estimate  conf.low conf.high\n1 0.9671593 0.8580147  1.076304\n\n\nOo lala, even does well when we don‚Äôt have the right model (this makes sense because we are using y!)\n\nmod_double_noise &lt;-  with(imp_y_noise, lm(y ~ x_obs + noise)) |&gt;\n  pool() |&gt;\n  tidy(conf.int = TRUE) |&gt;\n  filter(term == \"x_obs\") |&gt;\n  select(estimate, conf.low, conf.high)\n\nmod_double_noise\n\n   estimate  conf.low conf.high\n1 0.9697639 0.8612499  1.078278\n\n\nLet‚Äôs put those in the original plot:\n\nbind_rows(\n  mod_full_c_x,\n  mod_full_c_y,\n  mod_full_noise,\n  mod_cc_c_x,\n  mod_cc_c_y,\n  mod_cc_noise,\n  mod_imp_c_x,\n  mod_imp_c_y,\n  mod_imp_noise,\n  mod_double_c_x,\n  mod_double_c_y,\n  mod_double_noise\n) |&gt;\n  mutate(\n    mod = factor(c(\"Full data with c_x\", \n                   \"Full data with c_y\", \n                   \"Full data with noise\",\n                   \"Complete case with c_x\",\n                   \"Complete case wtih c_y\",\n                   \"Complete case with noise\",\n                   \"Imputation with c_x and y\",\n                   \"Imputation with c_y and y\",\n                   \"Imputation with noise and y\",\n                   \"Two models with c_x\",\n                   \"Two models with c_y\",\n                   \"Two models with noise\" ),\n                 levels = c(\"Full data with c_x\", \n                            \"Complete case with c_x\",\n                            \"Imputation with c_x and y\",\n                            \"Two models with c_x\",\n                            \"Full data with c_y\", \n                            \"Complete case wtih c_y\",\n                            \"Imputation with c_y and y\",\n                            \"Two models with c_y\",\n                            \"Full data with noise\",\n                            \"Complete case with noise\",\n                            \"Imputation with noise and y\",\n                            \"Two models with noise\" )),\n    mod = fct_rev(mod),\n  ) -&gt; to_plot\n\nggplot(to_plot, aes(x = estimate, xmin = conf.low, xmax = conf.high, y = mod)) +\n  geom_pointrange() + \n  geom_vline(xintercept = 1, lty = 2)\n\n\n\n\nPretty good! Maybe I‚Äôm feeling a little better about imputation. Maybe. But it had better include the outcome (which I‚Äôll admit feels very weird for my little causal brain)."
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html",
    "href": "posts/2017-09-28-r-release-names/index.html",
    "title": "R release names",
    "section": "",
    "text": "I always love discussions about R release names and their origin. I have been working on this list for a while ‚Äì with the release of ‚ÄúShort Summer‚Äù today, I thought it‚Äôd be a good time to post!\nAll of the release names are references to Peanuts strips/films. I think this is just so delightful! A few months ago, I attempted to get permission to embed the strips on our blog, but unfortunately it was denied (unless with a limited audience and password protection r emo::ji(\"woman_shrugging\")), so I‚Äôve just linked to the comics ‚Äì Enjoy!"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#r-devel-unreleased-development-version-unsuffered-consequences",
    "href": "posts/2017-09-28-r-release-names/index.html#r-devel-unreleased-development-version-unsuffered-consequences",
    "title": "R release names",
    "section": "r-devel (unreleased development version) Unsuffered Consequences",
    "text": "r-devel (unreleased development version) Unsuffered Consequences\nReference: Peanuts August 17, 1967"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#great-pumpkin",
    "href": "posts/2017-09-28-r-release-names/index.html#great-pumpkin",
    "title": "R release names",
    "section": "2.14.0 (2011-10-31) Great Pumpkin",
    "text": "2.14.0 (2011-10-31) Great Pumpkin\nReference: Peanuts October 29, 1973"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#december-snowflakes",
    "href": "posts/2017-09-28-r-release-names/index.html#december-snowflakes",
    "title": "R release names",
    "section": "2.14.1 (2011-12-22) December Snowflakes",
    "text": "2.14.1 (2011-12-22) December Snowflakes\n\n\n\n\nReference: A Charlie Brown Christmas\nThis is very close to the Peanuts January 5, 1960, however they mention January snowflakes rather than December. The ‚ÄúDecember Snowflakes‚Äù quote is from A Charlie Brown Christmas."
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#gift-getting-season",
    "href": "posts/2017-09-28-r-release-names/index.html#gift-getting-season",
    "title": "R release names",
    "section": "2.14.2 (2012-02-29) Gift-Getting Season",
    "text": "2.14.2 (2012-02-29) Gift-Getting Season\nReference: This is a line Lucy says in the short film It‚Äôs the Easter Beagle, Charlie Brown! ‚Äì referring to Easter as the ‚Äúgift-getting season‚Äù."
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#easter-beagle",
    "href": "posts/2017-09-28-r-release-names/index.html#easter-beagle",
    "title": "R release names",
    "section": "2.15.0 (2012-03-30) Easter Beagle",
    "text": "2.15.0 (2012-03-30) Easter Beagle\nReference: Peanuts April 11, 1971\nThis also likely references the It‚Äôs the Easter Beagle, Charlie Brown!."
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#roasted-marshmallows",
    "href": "posts/2017-09-28-r-release-names/index.html#roasted-marshmallows",
    "title": "R release names",
    "section": "2.15.1 (2012-06-22) Roasted Marshmallows",
    "text": "2.15.1 (2012-06-22) Roasted Marshmallows\nReference: Peanuts June 6, 1987"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#trick-or-treat",
    "href": "posts/2017-09-28-r-release-names/index.html#trick-or-treat",
    "title": "R release names",
    "section": "2.15.2 (2012-10-26) Trick or Treat",
    "text": "2.15.2 (2012-10-26) Trick or Treat\nReference: Peanuts October 31, 1969"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#security-blanket",
    "href": "posts/2017-09-28-r-release-names/index.html#security-blanket",
    "title": "R release names",
    "section": "2.15.3 (2013-03-01) Security Blanket",
    "text": "2.15.3 (2013-03-01) Security Blanket\nReference: Peanuts October 23, 1965"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#masked-marvel",
    "href": "posts/2017-09-28-r-release-names/index.html#masked-marvel",
    "title": "R release names",
    "section": "3.0.0 (2013-04-03) Masked Marvel",
    "text": "3.0.0 (2013-04-03) Masked Marvel\nReference: Peanuts June 23, 1981\nEdit: Got some insider info from the source himself that this is from Charlie Brown and Snoopy Show!\n\n\n\n\n Source MoviesAfterMidnight"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#good-sport",
    "href": "posts/2017-09-28-r-release-names/index.html#good-sport",
    "title": "R release names",
    "section": "3.0.1 (2013-05-16) Good Sport",
    "text": "3.0.1 (2013-05-16) Good Sport\nReference: You‚Äôre a Good Sport, Charlie Brown\nThis is likely from the film You‚Äôre a Good Sport Charlie Brown, however there Peanuts November 22, 1953 does refer to being a ‚ÄúGood Sport‚Äù as well!"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#frisbee-sailing",
    "href": "posts/2017-09-28-r-release-names/index.html#frisbee-sailing",
    "title": "R release names",
    "section": "3.0.2 (2013-09-25) Frisbee Sailing",
    "text": "3.0.2 (2013-09-25) Frisbee Sailing\nReference: Peanuts September 3, 1971"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#warm-puppy",
    "href": "posts/2017-09-28-r-release-names/index.html#warm-puppy",
    "title": "R release names",
    "section": "3.0.3 (2014-03-06) Warm Puppy",
    "text": "3.0.3 (2014-03-06) Warm Puppy\nr tufte::margin_note(\"There is also a book titled [*Happiness is a Warm Puppy*](https://www.goodreads.com/book/show/82191.Happiness_is_a_Warm_Puppy).\") Reference: Peanuts January 11, 1965"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#spring-dance",
    "href": "posts/2017-09-28-r-release-names/index.html#spring-dance",
    "title": "R release names",
    "section": "3.1.0 (2014-04-10) Spring Dance",
    "text": "3.1.0 (2014-04-10) Spring Dance\nReference: Peanuts March 22, 1971\n\n\n   Source ebay"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#sock-it-to-me",
    "href": "posts/2017-09-28-r-release-names/index.html#sock-it-to-me",
    "title": "R release names",
    "section": "3.1.1 (2014-07-10) Sock it to Me",
    "text": "3.1.1 (2014-07-10) Sock it to Me\nReference: This seems to be referring to a mini jigsaw puzzle, available on ebay!\n\n\n\n\n Source MoviesAfterMidnight"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#pumpkin-helmet",
    "href": "posts/2017-09-28-r-release-names/index.html#pumpkin-helmet",
    "title": "R release names",
    "section": "3.1.2 (2014-10-31) Pumpkin Helmet",
    "text": "3.1.2 (2014-10-31) Pumpkin Helmet\nReference: You‚Äôre a Good Sport, Charlie Brown\nIt‚Äôs a bit later in the clip, around 16:45.\nr tufte::margin_note('&lt;a href = \"../../media/images/2017-09-28/happiness-sidewalk.jpg\"&gt; &lt;img src = \"../../media/images/2017-09-28/happiness-sidewalk.jpg\" width = 200&gt;&lt;/img&gt;&lt;/a&gt;&lt;br&gt; *Source* [Something to be Found](http://www.somethingtobefound.com/2013/04/somethings-books-desk-papers-and-pottery.html)')"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#smooth-sidewalk",
    "href": "posts/2017-09-28-r-release-names/index.html#smooth-sidewalk",
    "title": "R release names",
    "section": "3.1.3 (2015-03-09) Smooth Sidewalk",
    "text": "3.1.3 (2015-03-09) Smooth Sidewalk\nReference: This is a page from the Happiness is a Warm Puppy book."
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#full-of-ingredients",
    "href": "posts/2017-09-28-r-release-names/index.html#full-of-ingredients",
    "title": "R release names",
    "section": "3.2.0 (2015-04-16) Full of Ingredients",
    "text": "3.2.0 (2015-04-16) Full of Ingredients\nReference: Peanuts April 7, 1966"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#world-famous-astronaut",
    "href": "posts/2017-09-28-r-release-names/index.html#world-famous-astronaut",
    "title": "R release names",
    "section": "3.2.1 (2015-06-18) World-Famous Astronaut",
    "text": "3.2.1 (2015-06-18) World-Famous Astronaut\nReference: Peanuts March 10, 1969\nr tufte::margin_note('&lt;a href = \"../../media/images/2017-09-28/fire-saftey.jpg\"&gt;&lt;img src = \"../../media/images/2017-09-28/fire-saftey.jpg\" width = 200&gt;&lt;/img&gt;&lt;/a&gt;&lt;br&gt; *Source* [Business Wire](http://www.businesswire.com/news/home/20041006005405/en/Brush-Fire-Safety-Basics-MetLife-Auto-Home)')"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#fire-safety",
    "href": "posts/2017-09-28-r-release-names/index.html#fire-safety",
    "title": "R release names",
    "section": "3.2.2 (2015-08-14) Fire Safety",
    "text": "3.2.2 (2015-08-14) Fire Safety\nReference: It seems that MetLife created a Peanuts themed Fire Saftey Brochure coloring and activity book."
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#wooden-christmas-tree",
    "href": "posts/2017-09-28-r-release-names/index.html#wooden-christmas-tree",
    "title": "R release names",
    "section": "3.2.3 (2015-12-10) Wooden Christmas-Tree",
    "text": "3.2.3 (2015-12-10) Wooden Christmas-Tree\nReference: This is a line from A Charlie Brown Christmas ‚Äì Linus says ‚ÄúGee, I didn‚Äôt know they still made wooden Christmas trees‚Äù."
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#very-secure-dishes",
    "href": "posts/2017-09-28-r-release-names/index.html#very-secure-dishes",
    "title": "R release names",
    "section": "3.2.4 (2016-03-11) Very Secure Dishes",
    "text": "3.2.4 (2016-03-11) Very Secure Dishes\nReference: Peanuts February 20, 1964"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#very-very-secure-dishes-a-rebadged-3.2.4-revised",
    "href": "posts/2017-09-28-r-release-names/index.html#very-very-secure-dishes-a-rebadged-3.2.4-revised",
    "title": "R release names",
    "section": "3.2.5 (2016-04-11) Very, Very Secure Dishes (a rebadged 3.2.4-revised)",
    "text": "3.2.5 (2016-04-11) Very, Very Secure Dishes (a rebadged 3.2.4-revised)\nI assume this is still a reference to Peanuts February 20, 1964"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#supposedly-educational",
    "href": "posts/2017-09-28-r-release-names/index.html#supposedly-educational",
    "title": "R release names",
    "section": "3.3.0 (2016-05-03) Supposedly Educational",
    "text": "3.3.0 (2016-05-03) Supposedly Educational\nReference: Peanuts May 7, 1971"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#bug-in-your-hair",
    "href": "posts/2017-09-28-r-release-names/index.html#bug-in-your-hair",
    "title": "R release names",
    "section": "3.3.1 (2016-06-21) Bug in Your Hair",
    "text": "3.3.1 (2016-06-21) Bug in Your Hair\nReference: Peanuts June 15, 1967"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#sincere-pumpkin-patch",
    "href": "posts/2017-09-28-r-release-names/index.html#sincere-pumpkin-patch",
    "title": "R release names",
    "section": "3.3.2 (2016-10-31) Sincere Pumpkin Patch",
    "text": "3.3.2 (2016-10-31) Sincere Pumpkin Patch\nReference: Peanuts Oct 30, 1968"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#another-canoe",
    "href": "posts/2017-09-28-r-release-names/index.html#another-canoe",
    "title": "R release names",
    "section": "3.3.3 (2017-03-06) Another Canoe",
    "text": "3.3.3 (2017-03-06) Another Canoe\nReference: Peanuts June 29, 1966"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#you-stupid-darkness",
    "href": "posts/2017-09-28-r-release-names/index.html#you-stupid-darkness",
    "title": "R release names",
    "section": "3.4.0 (2017-04-21) You Stupid Darkness",
    "text": "3.4.0 (2017-04-21) You Stupid Darkness\nReference: Peanuts September 9, 1965"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#single-candle",
    "href": "posts/2017-09-28-r-release-names/index.html#single-candle",
    "title": "R release names",
    "section": "3.4.1 (2017-06-30) Single Candle",
    "text": "3.4.1 (2017-06-30) Single Candle\nReference: Peanuts September 9, 1965"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#short-summer",
    "href": "posts/2017-09-28-r-release-names/index.html#short-summer",
    "title": "R release names",
    "section": "3.4.2 (2017-09-28) Short Summer",
    "text": "3.4.2 (2017-09-28) Short Summer\nReference: It was a Short Summer, Charlie Brown"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#kite-eating-tree",
    "href": "posts/2017-09-28-r-release-names/index.html#kite-eating-tree",
    "title": "R release names",
    "section": "3.4.3 (2017-11-30) Kite-eating Tree",
    "text": "3.4.3 (2017-11-30) Kite-eating Tree\nReference: Peanuts February 19, 1967\nPeter Dalgaard made a gif to commemorate the momentous occasion!\n\nr tufte::margin_note('&lt;a href = \"../../media/images/2017-09-28/someone-to-lean-on.jpg\"&gt;&lt;img src = \"../../media/images/2017-09-28/someone-to-lean-on.jpg\" width = 200&gt;&lt;/img&gt;&lt;/a&gt;&lt;br&gt; *Source* [catawiki auctions](https://auction.catawiki.com/kavels/2493737-peanuts-charlie-brown-snoopy-happiness-is-having-someone-to-lean-on-1971)')"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#someone-to-lean-on",
    "href": "posts/2017-09-28-r-release-names/index.html#someone-to-lean-on",
    "title": "R release names",
    "section": "3.4.4 (2018-03-15) Someone to Lean On",
    "text": "3.4.4 (2018-03-15) Someone to Lean On\nReference: Peanuts Figurine (1971). There are a couple of different versions of this, some with Charlie Brown and Snoopy, one with Linus and Snoopy, and one with Woodstock and Snoopy. Many of them were Hallmark cards, but there was also a badge and this figurine. The oldest (dated) one I could find was this one from 1971, so we went with that!"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#joy-in-playing",
    "href": "posts/2017-09-28-r-release-names/index.html#joy-in-playing",
    "title": "R release names",
    "section": "3.5.0 (2018-04-23) Joy in Playing",
    "text": "3.5.0 (2018-04-23) Joy in Playing\nReference: Peanuts January 27, 1973"
  },
  {
    "objectID": "posts/2017-09-28-r-release-names/index.html#feather-spray",
    "href": "posts/2017-09-28-r-release-names/index.html#feather-spray",
    "title": "R release names",
    "section": "3.5.1 (2018-07-02) Feather Spray",
    "text": "3.5.1 (2018-07-02) Feather Spray\nReference: Peanuts March 9, 1972\n\nLooks like we have some validation (thanks Peter!) ‚Äì I‚Äôve updated the Masked Marvel accordingly r emo::ji(\"tada\").\n\n\nMostly on target. A few are from films/badges rather than strips. Notably Masked Marvel from the unforgettable arm-wrestling w/Lucy.\n\n‚Äî Peter Dalgaard (@pdalgd) September 28, 2017"
  },
  {
    "objectID": "posts/2020-05-15-first-year-as-faculty/index.html",
    "href": "posts/2020-05-15-first-year-as-faculty/index.html",
    "title": "First year as faculty",
    "section": "",
    "text": "I just completed my first year as a faculty member - here is what I‚Äôve learned! I‚Äôll start by giving some context for where I am, what my university is like, etc. Then I‚Äôll describe four recommendations, summarized by:\nr emo::ji(\"performing_arts\") Discover your institution‚Äôs culture\nr emo::ji(\"woman_office_worker\") Become a peer\nr emo::ji(\"notes\") Find harmony\nr emo::ji(\"dancing_women\") Build community\nHere we gooo!"
  },
  {
    "objectID": "posts/2020-05-15-first-year-as-faculty/index.html#context",
    "href": "posts/2020-05-15-first-year-as-faculty/index.html#context",
    "title": "First year as faculty",
    "section": "Context",
    "text": "Context\nWhere am I? How does this compare to where you are going?\n\nI‚Äôm an assistant professor at Wake Forest University r tufte::margin_note(\"R1? R2? What are these acronyms? The [Carnegie Classification of Institutions of Higher Education](http://carnegieclassifications.iu.edu) is a way to classify institutions, briefly, R1: Doctoral Universities ‚Äì Very high research activity, R2: Doctoral Universities ‚Äì High research activity.\")\nWake is an R2, I think of it as a ‚Äúhybrid‚Äù between a teaching-focused institution (like an SLAC) & a research-focused one (like an R1) (this is not to say that folks at SLACs don‚Äôt do research or people at R1s don‚Äôt focus on teaching!)\n\nThe official breakdown of my responsibilities are:\n\nr emo::ji(\"woman_teacher\") 45% teaching\nr emo::ji(\"woman_scientist\") 45% research\nr emo::ji(\"woman_mechanic\") 10% service\n\nOn the teaching side, we have a 2-2 teaching load, small (&lt; 35 students) classes, and are very undergraduate focused\n\nOn the research side, grant funding is encouraged, I was given lab space and start-up funds to build a lab, my lab will be predominantly filled with undergraduate students"
  },
  {
    "objectID": "posts/2020-05-15-first-year-as-faculty/index.html#culture",
    "href": "posts/2020-05-15-first-year-as-faculty/index.html#culture",
    "title": "First year as faculty",
    "section": "Culture",
    "text": "Culture\nGetting to know your new academic home\nIn my first year I‚Äôve tried to navigate the norms. There are several layers to explore here, for example there are university norms, departmental norms, and within your department, there may be norms within your specialty. Here are some things I thought about:\n\nUniversity norms\n\nMeetings: Figure out which meetings assistant professors usually attend. There are a lot of meetings that are open to all faculty. Figuring out where to spend your limited time is crucial!\n\nService roles: Figure out what types of service roles are typically expected of incoming faculty\n\nVisibility: Figure out where it is important to be visible. Outside of meetings and service roles, there are often several other events (such as professional development opportunities, socials, etc). Ask around to figure out which will give the most bang for your buck in terms of helping your long term career goals\n\n\n\nDepartmental norms\n\nMeetings: Figure out how meetings are run. For example, does your department like to follow Robert‚Äôs Rules of Order for all meetings? Are there unspoken expectations about where people should sit? A tradition in my department is for everyone to sit in equally visible seats, some of these traditions are not necessarily reiterated at the beginning of each year, so it can be helpful to ask!\nBeing heard: Figure out the best way to have your voice heard. Are you particularly passionate about curriculum choices for the Introduction to Statistics course? Figure out how those decisions are made at the department-level. Maybe it means you should request to join a committee, or maybe it means you should set up some coffee chats with the faculty member that tends to make these decisions\n\nVisibility: Figure out where/when it is important to be visible. Different departments have different cultures with respect to whether professors tend to work with their doors open, do people tend to work from home when they aren‚Äôt teaching, etc. Ask around to see if such expectations exist in your new academic home!\n\nBeing a team player: Being on faculty is so exciting, you are joining a new team of people who all have (academic) interests like you! Figuring out how to best contribute to the team is an exciting and important prospect in the first year!"
  },
  {
    "objectID": "posts/2020-05-15-first-year-as-faculty/index.html#becoming-a-peer",
    "href": "posts/2020-05-15-first-year-as-faculty/index.html#becoming-a-peer",
    "title": "First year as faculty",
    "section": "Becoming a peer",
    "text": "Becoming a peer\nThe transition from graduate student / postdoc to peer\nAs with any job after being a student for quite some time, it can be difficult to figure out how to transition from being a student to being a peer! For me, this manifested in three ways:\n\nCollaboration: Figure out what your role is as a collaborator. Transitioning from being a collaborator on projects as a student to being a collaborator as a peer can be challenging!\n\nYour name: Take some time to consider what you would like to be called, both in the classroom and in collaborations. For example, would you like to go by Dr.¬†‚Äî, or Professor? or by your first name? There is not just one right answer here, but it‚Äôs worth taking into consideration in your first year. It is also worth considering that your decision (like lots of decisions we make!) may have ripple effects outside of just you. For example, if you go by your first name, it may make it difficult for your colleagues to make a different decision.\n\nGraduate students: Consider if/how you will work with graduate students. This was difficult for me to navigate! Even with a postdoc between graduate school and my faculty position, I felt like I was a graduate five seconds ago, how can I be leading graduate student research now? For me, I oscillate between a bit of humility along with a bit of confidence."
  },
  {
    "objectID": "posts/2020-05-15-first-year-as-faculty/index.html#finding-harmony",
    "href": "posts/2020-05-15-first-year-as-faculty/index.html#finding-harmony",
    "title": "First year as faculty",
    "section": "Finding harmony",
    "text": "Finding harmony\nBetween teaching, research, and service\nI like the word harmony here instead of balance. I think of this similarly to how I think about work-life ‚Äúbalance‚Äù. We‚Äôre really striving for harmony and long-run averages rather than day-to-day balance!\n\n\nI like work-life harmony! I don‚Äôt think balance ‚öñÔ∏è is really always the goal (like, with a small baby it‚Äôs going to tip towards life, with a grant deadline it‚Äôll top towards work, but keeping things in harmony is the goal) pic.twitter.com/oJO8ZyHo8T\n\n‚Äî Lucy D‚ÄôAgostino McGowan (@LucyStats) March 25, 2020\n\n\n\nTeaching\nTeaching, especially in your first year if you‚Äôre building new courses, can take up lots of time, and for the most part I think this is great! A lot of that investment will pay dividends in the future! It can be good to consistently take a look at how you‚Äôre spending your time throughout the year to make sure you‚Äôre striking a nice harmony r emo::ji(\"smile\"). I found that it was possible to spend infinite time tweaking my teaching, whether it was finding the perfect dataset to use as an example or developing thought-provoking yet complex-enough exam questions. Some of this time was definitely well spent and some of it maybe less so. Finding a strategy to be able to assess what is time well spent and what isn‚Äôt can be really useful! Given the lack of infinite time, some strategies I found helpful include:\n\nTeaching in blocks. I request to teach on Tuesday and Thursday back-to-back. This helps me focus on my teaching on these days; this strategy works well for me, but I know many others that like other schedules! Find one that works best with your flow!\n\nContentiously schedule non-teaching time on your calendar. I found it really useful to block off time on my calendar specifically to be spent on other tasks\n\n\n\nResearch\nIf you plan on doing research, here are a few things I would recommend in year one!\n\nGet to know the administration - they will be your friends and advocates in getting your research agenda off the ground!\n\nFamiliarize yourself with procedures, i.e.¬†how are reimbursements processed, how early do you need grant submissions internally routed before they can be officially submitted, who do you need to talk to about paying graduate students\n\nIf you plan on getting grant funding, apply ASAP if for nothing else to figure out the process. This is not a totally popular opinion, but I found it really helpful. I was on several grants as a graduate student and postdoc, however I never organized one from start to finish myself. The process was really eye opening - I think getting this experience early can be really useful, especially if you will rely on the funding.\n\n\n\nService\nService is tricky because I LOVE service roles and yet, again, I don‚Äôt have infinite time. I have been given the advice several times that it‚Äôs unusual for strong service to make up for not-so-strong teaching / research, so this balance is very important. That being said, if you are interested in serving in particular leadership roles, I‚Äôve found it very helpful to make that desire known, especially to senior colleagues. Many folks in senior positions are given leadership opportunities that they are more than happy to pass down."
  },
  {
    "objectID": "posts/2020-05-15-first-year-as-faculty/index.html#community",
    "href": "posts/2020-05-15-first-year-as-faculty/index.html#community",
    "title": "First year as faculty",
    "section": "Community",
    "text": "Community\nFinding a community of mentors / peers to uplift you\n\nMentors\nIt can be so helpful in this first year to find mentors to help you shape and achieve your short and long term career goals. I have found it the most useful to have at least one mentor: inside the department, outside the department but inside the university, outside the university. These mentors have helped me navigate all of the previous sections mentioned here!\n\n\nPeers\nIn addition to mentors, having peers to bounce ideas off of, complain to, and uplift, is super helpful! Peers both inside your department and outside are crucial! For example, I spend a lot of time on the New PI Slack - a community for assistant professors around the world."
  },
  {
    "objectID": "posts/2020-05-15-first-year-as-faculty/index.html#wrap-up",
    "href": "posts/2020-05-15-first-year-as-faculty/index.html#wrap-up",
    "title": "First year as faculty",
    "section": "Wrap up",
    "text": "Wrap up\nSO there it is. Some things I‚Äôve learned in my first year. In sum, four things to focus on:\nr emo::ji(\"performing_arts\") Discover your institution‚Äôs culture\nr emo::ji(\"woman_office_worker\") Become a peer\nr emo::ji(\"notes\") Find harmony\nr emo::ji(\"dancing_women\") Build community\nI look forward to learning lots more in the years to come!"
  },
  {
    "objectID": "posts/2023-08-04-visual-diagnostic-tools-for-causal-inference/index.html",
    "href": "posts/2023-08-04-visual-diagnostic-tools-for-causal-inference/index.html",
    "title": "Visual Diagnostic Tools for Causal Inference",
    "section": "",
    "text": "Here we are going to look at several diagnostic plots are helpful when attempting to answer a causal question. They can be used to visualize the target population, balance, and treatment effect heterogeneity."
  },
  {
    "objectID": "posts/2023-08-04-visual-diagnostic-tools-for-causal-inference/index.html#setup",
    "href": "posts/2023-08-04-visual-diagnostic-tools-for-causal-inference/index.html#setup",
    "title": "Visual Diagnostic Tools for Causal Inference",
    "section": "Setup",
    "text": "Setup\nI‚Äôve simulated data to demonstrate the utility of the various plots. In each simulation, we have four pre-treatment variables: var1, var2, var3, and var4, a treatment, t, and an outcome y. I have also fit a propensity score model for each and calculated ATE, ATT, and overlap weights (unsure what these are? Check out this post on propensity score weighting). I will also create three propensity score matched cohorts, each with a different caliper.\nI use the following packages:\n\nlibrary(tidyverse)\nlibrary(propensity)\nlibrary(halfmoon)\nlibrary(MatchIt)\n\n\n\nFor brevity I‚Äôve hidden the simulation code, but if you would like to see it to reproduce the analyses yourself, just click the Code toggle.\n\n\nCode\nset.seed(8)\nn &lt;- 1000\n\n## Main Data ----\n\nvar1 &lt;- rnorm(n, sd = 0.25)\nvar2 &lt;- rnorm(n, sd = 0.25)\nvar3 &lt;- rnorm(n, sd = 0.25)\nvar4 &lt;- rnorm(n, sd = 0.25)\ne_x &lt;- 1 / (1 + exp(-(var1 + var2 + var3 + var4)))\nt &lt;- rbinom(n, 1, e_x)\ny1 &lt;- 0.5 * (var1 + var2 + var3 + var4) + rnorm(n)\ny0 &lt;- - 0.5 * (var1 + var2 + var3 + var4) + rnorm(n)\ny_obs &lt;- t * y1 + (1 - t) * y0\ndata &lt;- data.frame(\n  y = y_obs,\n  t = factor(t), \n  var1, \n  var2,\n  var3,\n  var4\n)\n\ndata &lt;- data |&gt;\n  mutate(p = glm(t ~ var1 + var2 + var3 + var4, data = data, family = binomial) |&gt;\n           predict(type = \"response\"),\n         w_ate = wt_ate(p, t, .treated = 1),\n         w_ato = wt_ato(p, t, .treated = 1),\n         w_att = wt_att(p, t, .treated = 1)\n  ) \n\nmatch_1 &lt;- matchit(t ~ var1 + var2 + var3 + var4, data = data, caliper = 0.1)\nmatch_2 &lt;- matchit(t ~ var1 + var2 + var3 + var4, data = data, caliper = 0.01)\nmatch_3 &lt;- matchit(t ~ var1 + var2 + var3 + var4, data = data, caliper = 0.001)\nmatches &lt;- bind_matches(data, match_1, match_2, match_3)\n\n## Non-linear data ----\n\ne_x &lt;- 1 / (1 + exp(- (var1 + var2 + var3 + 2 * I(var4 &lt; -0.5) + 6 * I(var4 &gt; 0.5))))\nt &lt;- rbinom(n, 1, e_x)\n\ny1 &lt;- 1 + var2 + var2 + var3 + var4 \ny0 &lt;- var1 + var2 + var3 + var4\ny_obs &lt;- t * y1 + (1 - t) * y0\ndata_nonlinear &lt;- data.frame(\n  y = y_obs,\n  t = factor(t), \n  var1, \n  var2,\n  var3,\n  var4\n)\n\ndata_nonlinear &lt;- data_nonlinear |&gt;\n  mutate(p = glm(t ~ var1 + var2 + var3 + var4, data = data_nonlinear, family = binomial) |&gt;\n           predict(type = \"response\"),\n         w_ate = wt_ate(p, t, .treated = 1),\n         w_ato = wt_ato(p, t, .treated = 1),\n         w_att = wt_att(p, t, .treated = 1)\n  ) \n## Positivity violation data ----\n\nset.seed(2186)\nvar4 &lt;- rgamma(n, 1, 1.5)\ne_x &lt;- 1 / (1 + exp(- (var1 + var2 + var3 + var4 + 0.2 * var4^2 + 0.001 * var4^3)))\nt &lt;- rbinom(n, 1, e_x)\n\ndata_positivity &lt;- data.frame(\n  t = factor(t), \n  var1, \n  var2,\n  var3,\n  var4\n)\n\ndata_positivity &lt;- data_positivity |&gt;\n  mutate(p = glm(t ~ var1 + var2 + var3 + poly(var4, 3), data = data_positivity, family = binomial) |&gt;\n           predict(type = \"response\"),\n         w_ate = wt_ate(p, t, .treated = 1),\n         w_ato = wt_ato(p, t, .treated = 1),\n         w_att = wt_att(p, t, .treated = 1)\n  )"
  },
  {
    "objectID": "posts/2023-08-04-visual-diagnostic-tools-for-causal-inference/index.html#target-population",
    "href": "posts/2023-08-04-visual-diagnostic-tools-for-causal-inference/index.html#target-population",
    "title": "Visual Diagnostic Tools for Causal Inference",
    "section": "Target Population",
    "text": "Target Population\nTargeting different causal estimands will yield different target populations (for a longer discussion of target populations see this post and check out this awesome pre-print by Noah Griefer and Liz Stuart. For example, if you are interested in answering a question with the treated group in mind, an estimand that estimates the average treatment effect among the treated (ATT), will be appropriate. Targeting this estimand will lead to selecting unexposed individuals who match the characterstics of the treated population (whether via matching to these individuals or upweighting them in the sample). Mirrored histograms can be a nice way to visualize the distribution of your target population after incorporating the propensity score when either matching or weighting are used. These basic plots are simply histograms of the propensity score, stratified by exposure. ‚ÄúMirroring‚Äù these histograms above and below the x-axis, can make it easier to compare regions of overlap.\n\nWhat am I looking for?\n\nOverlap: What is the degree of overlap between the exposure groups?\nPositivity violations: Does everyone have a non-zero probability of each level of exposure?\nExtreme weights: Are there any extreme weights that could induce finite sample bias / extreme variance?\n\nLet‚Äôs take a look at an example. We can use the halfmoon package to create these.\n\n\nThe cobalt package is another excellent tool that can create many of these plots in R.\nBelow is the basic implementation of this mirrored histogram prior to incorporating the propensity score. On the top half of the visualization, we see the distribution of the propensity score in the treated group (blue); the bottom half displays the distribution among the controls (orange). Looking at this plot, I see good overlap (i.e.¬†the two distributions overlap), and I do not see evidence of positivity violations.\n\nggplot(data, aes(p, fill = t, group = t)) +\n  geom_mirror_histogram(bins = 30) + \n  annotate(\"label\", 0.5, -10, label = \"control\", color = \"orange\") +\n  annotate(\"label\", 0.5, 10, label = \"treated\", color = \"cornflower blue\") +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\",\n       fill = \"treatment\",\n       y = \"count\") + \n  theme(legend.position = \"none\")\n\n\n\n\nNow let‚Äôs incorporate the propensity score. First, let‚Äôs see what this plot looks like if our target population is the whole population, meaning we are interested in estimate the ATE. I have added the following line of code to the ggplot layers below: geom_mirror_histogram(bins = 30, aes(weight = w_ate), alpha = 0.5). Now, I can see the pseudo-population that is created after implementing the propensity score weight. Notice the shape of the distributions match between the two groups (this is what the ATE weight is trying to do!). Looking at the figure below, I also can conclude that there aren‚Äôt any extreme weights.\n\nggplot(data, aes(p, fill = t, group = t)) +\n  geom_mirror_histogram(bins = 30, fill = \"grey\") + \n  geom_mirror_histogram(bins = 30, aes(weight = w_ate), alpha = 0.5) + \n  annotate(\"label\", 0.5, -10, label = \"control\", color = \"orange\") +\n  annotate(\"label\", 0.5, 10, label = \"treated\", color = \"cornflower blue\") +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\",\n       fill = \"treatment\",\n       y = \"count\") + \n  theme(legend.position = \"none\")\n\n\n\n\nThese plots can be useful as a pedagogical tool to give a sense for how the different target estimands lead to different target populations. For example, let‚Äôs see what the pseudo-population looks like after using the ATT weight. Notice in the figure below, the ‚Äúweighted‚Äù pseudo-population in the treated arm exactly overlaps with the actual distribution of the treated observations ‚Äì this is exactly what an ATT weight does, everyone in the treated population receives a weight of 1. Now look at the bottom half of the figure ‚Äì the distribution of the propensity scores in the control group now matches that of the treated ‚Äì in the regions were there are fewer treated observations, the control observations are down-weighted (where the propensity score is lower) and in the regions where there are more treated observations the control observations are up-weighted (where the propensity score is higher).\n\nggplot(data, aes(p, fill = t, group = t)) +\n  geom_mirror_histogram(bins = 30, fill = \"grey\") + \n  geom_mirror_histogram(bins = 30, aes(weight = w_att), alpha = 0.5) + \n  annotate(\"label\", 0.5, -10, label = \"control\", color = \"orange\") +\n  annotate(\"label\", 0.5, 10, label = \"treated\", color = \"cornflower blue\") +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\",\n       fill = \"treatment\",\n       y = \"count\") + \n  theme(legend.position = \"none\")\n\n\n\n\nFinally, let‚Äôs see how an overlap (ATO) weight compares. Notice in the figure below all observations appear to be down-weighted ‚Äì the overlap weights are bounded by 0 and 1 (which means they have nice variance properties! There is no risk of having an extreme weight!). Also notice the shape of the distribution ‚Äì it matches between the two groups.\n\nggplot(data, aes(p, fill = t, group = t)) +\n  geom_mirror_histogram(bins = 30, fill = \"grey\") + \n  geom_mirror_histogram(bins = 30, aes(weight = w_ato), alpha = 0.5) + \n  annotate(\"label\", 0.5, -10, label = \"control\", color = \"orange\") +\n  annotate(\"label\", 0.5, 10, label = \"treated\", color = \"cornflower blue\") +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\",\n       fill = \"treatment\",\n       y = \"count\") + \n  theme(legend.position = \"none\")\n\n\n\n\nI like to compare the overlap weights to the distribution after matching with a caliper, as they are both use to estimate a similar estimand. Here, I have created three matched cohorts, each with an increasingly smaller caliper. We can think of matching as an extreme form of weighting, where the observation will receive a weight of 1 if they are in the cohort and 0 otherwise. Here, I have created a dataset called matches that has three columns with these indiciators match_1, match_2, and match_3 (you can see the code above by clicking the Code toggle in the Setup section).\n\n\nPropensity score matching with a caliper means that you only consider matches within a pre-specified distance of each other. Smaller calipers will result in fewer matches.\n\n\nCode\nggplot(matches, aes(p, fill = t, group = t)) +\n  geom_mirror_histogram(bins = 30, fill = \"grey\") + \n  geom_mirror_histogram(bins = 30, aes(weight = match_1), alpha = 0.5) + \n  annotate(\"label\", 0.5, -10, label = \"control\", color = \"orange\") +\n  annotate(\"label\", 0.5, 10, label = \"treated\", color = \"cornflower blue\") +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\",\n       y = \"count\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nCode\nggplot(matches, aes(p, fill = t, group = t)) +\n  geom_mirror_histogram(bins = 30, fill = \"grey\") + \n  geom_mirror_histogram(bins = 30, aes(weight = match_2), alpha = 0.5) + \n  annotate(\"label\", 0.5, -10, label = \"control\", color = \"orange\") +\n  annotate(\"label\", 0.5, 10, label = \"treated\", color = \"cornflower blue\") +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\",\n       y = \"count\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nCode\nggplot(matches, aes(p, fill = t, group = t)) +\n  geom_mirror_histogram(bins = 30, fill = \"grey\") + \n  geom_mirror_histogram(bins = 30, aes(weight = match_3), alpha = 0.5) + \n  annotate(\"label\", 0.5, -10, label = \"control\", color = \"orange\") +\n  annotate(\"label\", 0.5, 10, label = \"treated\", color = \"cornflower blue\") +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\",\n       y = \"count\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\nFor demonstration purposes, let‚Äôs see what one of these plots looks like in a dataset that doesn‚Äôt have perfect overlap. Whoa! Look at that weight in the figure below. This is an example where we see a possible positivity violation (some observations of propensity scores very close to 1) and extreme weights (check out that control with a weight &gt; 500!).\n\nggplot(data_positivity, aes(p, fill = t, group = t)) +\n  geom_mirror_histogram(bins = 30) + \n  geom_mirror_histogram(bins = 30, aes(weight = w_ate), alpha = 0.5) +\n  annotate(\"label\", 0.6, -20, label = \"control\", color = \"orange\") +\n  annotate(\"label\", 0.6, 20, label = \"treated\", color = \"cornflower blue\") +\n  scale_y_continuous(labels = abs) +\n  labs(x = \"propensity score\",\n       y = \"count\") + \n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/2023-08-04-visual-diagnostic-tools-for-causal-inference/index.html#balance-visualization",
    "href": "posts/2023-08-04-visual-diagnostic-tools-for-causal-inference/index.html#balance-visualization",
    "title": "Visual Diagnostic Tools for Causal Inference",
    "section": "Balance visualization",
    "text": "Balance visualization\nOk, once we‚Äôve checked out the target population, we can see how balanced the exposure groups are after incorporating the propensity score.\n\nWhat am I looking for?\n\nbalance (in the mean) between treatment groups\nbalance across the distribution (for continuous confounders)\n\n\n\nBalance in the mean\nA common way to look at balance is the standardized mean difference. This will tell us whether the (standardized) means are balanced between the treatment groups (we often target an absolute standardized mean difference less than 0.1 as a rule of thumb). We can use the tidy_smd function to calculate the standardized mean differences between the exposure groups in our example dataset. Let‚Äôs see how they compare across the matched datasets.\n\nmatches_smd &lt;- tidy_smd(\n  matches,\n  var1:var4,\n  .group = t,\n  .wts = c(match_1, match_2, match_3)\n)\n\nA nice way to visualize these is a Love Plot (named for Thomas Love, who was one of the first folks to use them). In the halfmoon package there is a geom_love that will help create this as a layer in a ggplot, or you can use the shorthand love_plot. Below we can see that all three matched sets achieve balance with respect to demonstrating standardized mean differences across all pre-treatment variables less than the rule of thumb (0.1). Each of the different ‚Äúmatches‚Äù denote a different caliper (from largest: match_1 to smallest: match_3). We see here that using a smaller caliper seems to help balance var2 at the expense of var4 and var1 compared to the larger calipers.\n\nggplot(matches_smd, aes(abs(smd), variable, group = method, color = method)) + \n  geom_love() + \n  labs(x = \"Absolute standardized mean difference\")\n\n\n\n\nLet‚Äôs look at another dataset. I have simulated data I am calling data_nonlinear. Let‚Äôs check out the Love Plot for this data. This time I will use our propensity score weights.\n\nweighted_smd &lt;- tidy_smd(\n  data_nonlinear,\n  var1:var4,\n  .group = t,\n  .wts = c(w_ate, w_att, w_ato)\n)\n\nggplot(weighted_smd, aes(x = abs(smd), y = variable, group = method, color = method)) + \n  geom_love() +   \n  labs(x = \"Absolute standardized mean difference\")\n\n\n\n\nGreat! Looks like any of our weighting choices will achieve balance on the mean. Check out the green line (the overlap weights) ‚Äì the standardized mean differences are exactly 0! This is a feature of this weight, if the propensity score is fit using logistic regression, any variables included in the model will be perfectly balanced on the mean ‚Äì COOL! BUT as you may have guessed by the name of this dataset, the mean does not tell the whole story. These variables are continuous, so being balanced on the mean does not guarantee that the whole distribution is balanced. To examine the distribution of a variable across treatment groups, we can use an empirical cumulative distribution plot."
  },
  {
    "objectID": "posts/2023-08-04-visual-diagnostic-tools-for-causal-inference/index.html#balance-across-the-distribution",
    "href": "posts/2023-08-04-visual-diagnostic-tools-for-causal-inference/index.html#balance-across-the-distribution",
    "title": "Visual Diagnostic Tools for Causal Inference",
    "section": "Balance across the distribution",
    "text": "Balance across the distribution\nThe geom_ecdf function in the halfmoon package allows for you to visualize weighted empirical CDFs. Let‚Äôs first look at the unweighted eCDF for var4. We are going to plot the range of var4 values on the x-axis and the proportion of var4 values that are less than the given x-value on the y-axis (the empirical CDF), stratified by treatment. Looking at the figure below, we see gaps between the lines (meaning the two lines to not overlap, implying that the distributions differ).\n\nggplot(data_nonlinear, aes(x = var4, group = t, color = t)) + \n  geom_ecdf() + \n  labs(y = \"Proportion &lt;= x\") +\n  theme(legend.position = \"none\")\n\n\n\n\nNow we can compare this to the weighted eCDF to see if the propensity score weighting improves the balance ‚Äì I‚Äôll use the overlap weights for demonstration purposes. Hmm. We can see in the plot below that the lines cross (they are balanced on the mean, we knew that from the Love Plot), but there are still pretty large gaps across other portions of the distribution. This suggests that there are some non-linear effects that the propensity score is failing to capture.\n\nggplot(data_nonlinear, aes(x = var4, group = t, color = t)) + \n  geom_ecdf(aes(weights = w_ato)) + \n  labs(y = \"Proportion &lt;= x\") +\n  theme(legend.position = \"none\")\n\n\n\n\nLet‚Äôs try to refit our propensity score model with a spline on the var4 variable, and then recreate our plot.\n\ndata_nonlinear &lt;- data_nonlinear |&gt;\n  mutate(p = glm(t == 1 ~ var1 + var2 + var3 + splines::ns(var4, 4), \n                 data = data_nonlinear) |&gt;\n           predict(type = \"response\"),\n         w_ato = wt_ato(p, t, .treated = 1)) \n\nggplot(data_nonlinear, aes(x = var4, group = t, color = t)) + \n  geom_ecdf(aes(weights = w_ato)) + \n  labs(y = \"Proportion &lt;= x\") +\n  theme(legend.position = \"none\")\n\n\n\n\nMuch better!"
  },
  {
    "objectID": "posts/2023-08-04-visual-diagnostic-tools-for-causal-inference/index.html#heterogeneous-treatment-effect",
    "href": "posts/2023-08-04-visual-diagnostic-tools-for-causal-inference/index.html#heterogeneous-treatment-effect",
    "title": "Visual Diagnostic Tools for Causal Inference",
    "section": "Heterogeneous treatment effect",
    "text": "Heterogeneous treatment effect\nHere is the last visual! This is a quick plot that can help explore possible treatment heterogeneity.\n\nWhat am I looking for?\n\nDifferences in the treatment effect across the covariate space\n\nIn this example dataset, the average treatment effect is 0. Let‚Äôs show that. There are lots of ways to estimate this, for example, we can use the ATE weights.\n\nlm(y ~ t, data = data, weight = w_ate)\n\n\nCall:\nlm(formula = y ~ t, data = data, weights = w_ate)\n\nCoefficients:\n(Intercept)           t1  \n   -0.02095     -0.05364  \n\n\nAwesome! Now let‚Äôs create a plot to see if this effect is constant across the covariate space. One way to summarize the ‚Äúcovariate space‚Äù is the propensity score! This simple plot has the propensity score on the x-axis and the outcome on the y-axis. We then stratify by the treatment and look at a smoothed line in both groups.\n\nggplot(data, aes(x = p, y = y, color = t)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"loess\", formula = \"y ~ x\")\n\n\n\n\nThe lines cross! This indicates that there is treatment effect heterogeneity (in this particular case, when the propensity score is greater than 0.5, there is a positive treatment effect, and when less than 0.5 there is a negative treatment effect).\nJust to see what it looks like when there is not a heterogeneous treatment effect, let‚Äôs check out the data_nonlinear dataset (where I simulated a constant effect). Notice below the lines don‚Äôt cross, the width between them is constant across the covariate space.\n\nggplot(data_nonlinear, aes(x = p, y = y, color = t)) +\n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"loess\", formula = \"y ~ x\", span = 50)\n\n\n\n\n\n\n\n\nYou can find a longer post about these plots here."
  },
  {
    "objectID": "posts/2023-07-26-visual-diagnostic-tool-for-causal-inference/index.html",
    "href": "posts/2023-07-26-visual-diagnostic-tool-for-causal-inference/index.html",
    "title": "Visual Diagnostic Tool for Causal Inference",
    "section": "",
    "text": "To celebrate the 40th anniversary of the paper The Central Role of the Propensity Score in Observational Studies for Causal Effects published in Biometrika in 1983, the journal Observational Studies had a special issue highlighting the methods in the paper and developed since. This led us to take a closer look at this seminal paper, and in doing so we noticed mention of a visual diagnostic tool that we haven‚Äôt see used often but might be useful for exploring potential treatment effect heterogeneity."
  },
  {
    "objectID": "posts/2023-07-26-visual-diagnostic-tool-for-causal-inference/index.html#whats-old-is-new",
    "href": "posts/2023-07-26-visual-diagnostic-tool-for-causal-inference/index.html#whats-old-is-new",
    "title": "Visual Diagnostic Tool for Causal Inference",
    "section": "What‚Äôs old is new!",
    "text": "What‚Äôs old is new!\nHere are the lines from the original paper:\n\nSince covariance adjustment is effectively adjustment for the linear discriminant, plots of the responses \\(r_{1i}\\) and \\(r_{0i}\\) or residuals \\(r_{ki} ‚Äî \\hat{r}_{ki}\\) where \\(\\hat{r}_{ki}\\) is the value of \\(r_{ki}\\) predicted from the regression model used in the covariance adjustment, versus the linear discriminant are useful in identifying nonlinear or nonparallel response surfaces, as well as extrapolations, which might distort the estimate of the average treatment effect. Furthermore, such a plot is a bivariate display of multivariate adjustment, and as such might be useful for general presentation.\n\n\nGenerally, plots of responses and residuals from covariance analysis against the propensity score \\(e(x)\\) are more appropriate than against the discriminant, unless of course the covariates are multivariate normal with common covariance matrix in which case the propensity score is a monotone function of the discriminant.\n\nSo simple! They are just suggesting for continuous outcomes plotting the outcome (or residuals) against the propensity score. If you use a linear discriminant to estimate the propensity score, this is basically a residuals vs.¬†fits plot (stratified by treatment group). Let‚Äôs look at an example"
  },
  {
    "objectID": "posts/2023-07-26-visual-diagnostic-tool-for-causal-inference/index.html#toy-example",
    "href": "posts/2023-07-26-visual-diagnostic-tool-for-causal-inference/index.html#toy-example",
    "title": "Visual Diagnostic Tool for Causal Inference",
    "section": "Toy Example",
    "text": "Toy Example\nLet‚Äôs generate some data to see how this works. We have a standard normal confounder, \\(X\\):\n\nlibrary(tidyverse)\nset.seed(8)\nn &lt;- 1000\nx &lt;- rnorm(n)\n\nThe ‚Äútrue‚Äù propensity score, that is the probability that \\(T=1\\) given \\(X\\) is as follows:\n\\(e(x) = \\exp(x) / (1 + \\exp(x))\\)\n\ne_x &lt;- exp(x) / (1 + exp(x))\nt &lt;- rbinom(n, 1, e_x)\n\nThe effect of the treatment, \\(T\\), is heterogeneous, that is, it depends on \\(X\\), as follows:\n\\[\n\\begin{aligned}\nY(1) &= 0.5 \\times X\\\\\nY(0) &= -0.5 \\times X\\\\\nY_{obs} &= T\\times Y(1) + (1-T)\\times Y(0)\n\\end{aligned}\n\\]\n\ny1 &lt;- 0.5 * x + rnorm(n)\ny0 &lt;- - 0.5 * x + rnorm(n)\ny_obs &lt;- t * y1 + (1 - t) * y0\n\nOk, now we have our observed data, let‚Äôs put it all together:\n\ndata &lt;- data.frame(\n  y = y_obs,\n  t = t,\n  x = x\n)\n\n\nCovariate adjustment\nLet‚Äôs begin by estimating our causal effect by adjusting for \\(X\\) directly in a model. Since \\(X\\) is standard normal, this is actually going to be just fine.\n\n\nOn when covariate adjustment will work, from Rosenbaum & Rubin 1983: Cases where covariance adjustment has been seen to perform quite poorly are precisely those cases in which the linear discriminant is not a monotone function of the propensity score, so that covariance adjustment is implicitly adjusting for a poor approximation to the propensity score. In the case of univariate \\(x\\), the linear discriminant is a linear function of \\(x\\), whereas the propensity score may not be a monotone function of \\(x\\) if the variances of \\(x\\) in the treated and control groups are unequal.\n\nmodel_noint &lt;- lm(y ~ t + x, data)\nmodel_noint\n\n\nCall:\nlm(formula = y ~ t + x, data = data)\n\nCoefficients:\n(Intercept)            t            x  \n   0.226609    -0.003062    -0.012806  \n\n\nLooks like the treatment effect controlling for \\(X\\) is ~0. Let‚Äôs look at some diagnostics, for example we can look at the residuals vs.¬†fits plot.\n\n\nIn an unfortunate oversight, in the paper I call this a biased estimate of the causal effect ‚Äì it isn‚Äôt biased (it is the actual average treatment effect!) ‚Äì It just isn‚Äôt terribly interesting as it doesn‚Äôt take the interaction into account.\n\nggplot(data, aes(x = fitted(model_noint), y = residuals(model_noint))) + \n  geom_point() + \n  geom_smooth(method = \"loess\", formula = \"y ~ x\", se = FALSE, span = 1) + \n  labs(x = \"Fitted\", y = \"Residuals\")\n\n\n\n\nOk, looks ok ‚Äì maybe suggests a slightly non-linear relationship. Now let‚Äôs do what Rosenbaum & Rubin suggest and examine this same plot stratified by treatment group.\n\nggplot(data, aes(x = fitted(model_noint), y = residuals(model_noint), color = factor(t))) + \n  geom_point(alpha = 0.3) + \n  geom_smooth(method = \"loess\", formula = \"y ~ x\", se = FALSE, span = 1) + \n  labs(x = \"Fitted\", y = \"Residuals\", color = \"treatment\")\n\n\n\n\nWhoa! Look at that heterogeneity. Looks like that ‚Äúaverage‚Äù treatment effect really only represents those in the middle of the distribution of the confounder, \\(X\\) (which of course is exactly how we created the variable!).\nWe can fit the ‚Äúcorrect‚Äù model, including the interaction and examine the plot again.\n\nmodel_int &lt;- lm(y ~ t + x + t*x, data)\nmodel_int\n\n\nCall:\nlm(formula = y ~ t + x + t * x, data = data)\n\nCoefficients:\n(Intercept)            t            x          t:x  \n    0.01437      0.02939     -0.48962      0.98383  \n\n\nNow let‚Äôs recreate the plot.\n\nggplot(data, aes(x = fitted(model_int), y = residuals(model_int), color = factor(t))) + \n  geom_point(alpha = 0.3) + \n  geom_smooth(method = \"loess\", formula = \"y ~ x\", se = FALSE, span = 1) + \n  labs(x = \"Fitted\", y = \"Residuals\", color = \"treatment\")\n\n\n\n\nBeautiful!\n\n\nPropensity score\nInstead of using covariate adjustment, we could estimate our causal effect using propensity scores.\n\n\nNote, here I am using logistic regression to fit the propensity score model, if I had used LDA instead and adjusted for that we would get the exact same answer as the previous section\nLet‚Äôs begin by fitting a propensity score model:\n\ndata &lt;- data |&gt;\n  mutate(p = glm(t ~ x, data = data, family = binomial) |&gt;\n           predict(type = \"response\")\n  ) \n\nWe can examine the distribution of the propensity score. Let‚Äôs use the halfmoon package to create a mirrored histogram üòé\n\n\nMalcolm Barrett, Travis Gerke, and I have been working on a new suite of R packages for helping complete common causal inference tasks, check out our most recent posts here: https://r-causal.github.io/r-causal-blog/\n\nlibrary(halfmoon)\n\nggplot(data, aes(p, fill = factor(t))) +\n  geom_mirror_histogram(bins = 30) + \n  labs(x = \"propensity score\",\n       fill = \"treatment\")\n\n\n\n\nGreat! Looks good.\nNow let‚Äôs estimate our average treatment effect. We can use the propensity package to calculate ATE weights:\n\ndata &lt;- data |&gt;\n  mutate(wt_ate = propensity::wt_ate(p, t, .treated = 1))\nmodel &lt;- lm(y ~ t, weights = wt_ate, data)\nmodel\n\n\nCall:\nlm(formula = y ~ t, data = data, weights = wt_ate)\n\nCoefficients:\n(Intercept)            t  \n    0.05113     -0.02105  \n\n\nGreat (although again we know this isn‚Äôt representative of most people in this population since we‚Äôve induced some serious heterogeneity!). Now let‚Äôs create the plot suggested in the 1983 paper, examining the outcome vs the propensity score.\n\nggplot(data, aes(x = p, y = y, color = factor(t))) + \n  geom_point(alpha = 0.3) + \n  geom_smooth(method = \"loess\", formula = \"y ~ x\", se = FALSE, span = 1) + \n  labs(x = \"propensity score\", color = \"treatment\")\n\n\n\n\nWhoa! Look at that effect heterogeneity! Looks like the only place where the average treatment effect (~0) is representative is when the propensity score is 0.5, when it is less, the effect is negative, and greater is it positive.\nSo there you have it! A simple plot to give a little more information than a single number summary (the average treatment effect). When estimating causal effects using covariate adjustment, a sensible and straightforward diagnostic plot to use is the residuals versus fits plot stratified by treatment assignment. If using a propensity score (as in this example) you can look at the outcome versus the propensity score. As mentioned in the paper, ideally these plots would be generated during the exploratory phase of the modeling process and once in the confirmatory phase the correct relationship between the treatment, confounders, and outcome would be well understood allowing the correct model to be pre-specified. Perhaps this can be added to the set of routine diagnostic tools used when assessing propensity score + outcome models in the future."
  },
  {
    "objectID": "posts/2023-04-24-causal-quartets/index.html",
    "href": "posts/2023-04-24-causal-quartets/index.html",
    "title": "Causal Quartets",
    "section": "",
    "text": "On this weeks episode of Casual Inference we talk about a ‚ÄúCausal Quartet‚Äù a set of four datasets generated under different mechanisms, all with the same statistical summaries (including visualizations!) but different true causal effects.\nThe figures and tables are from our recent preprint: https://arxiv.org/pdf/2304.02683.pdf \n\nGiven a single dataset with 3 variables: exposure, outcome and covariate (z) how can statistics help you decide whether to adjust for z? It can‚Äôt! For example here, the correlation between z and the exposure in all 4 datasets is 0.7!\n  So if Stats can‚Äôt help what can we do? Well the best thing is just to know the data generating mechanism but that is hard! An easier solution is to make sure to have time varying measurements and only adjust for pre-exposure covariates! This solves the problem in 3/4 of the sets!\n\n\nThe one it doesn‚Äôt solve is M-bias, but as our podcast episode title suggests (M-Bias: Much Ado About Nothing?) this may be much ado about nothing (give a listen to find out why!) Also‚Ä¶credit to ChatGPT for our episode title üòÇ\nMalcolm Barrett, Travis Gerke, and I have a preprint with details: https://arxiv.org/pdf/2304.02683.pdf\nAlso the quartets package includes the datasets if you‚Äôd like to play with it yourselves!\nI also learned last month about another awesome ‚Äúcausal quartet‚Äù by Gelman, Jessica Hullman, and Lauren Kennedy that focuses on treatment heterogeneity so I could use help coming up with a new name for ours!"
  },
  {
    "objectID": "posts/2017-08-30-exponential-power-series/index.html",
    "href": "posts/2017-08-30-exponential-power-series/index.html",
    "title": "The Exponential Power Series",
    "section": "",
    "text": "I am a big fan of the Poisson distribution, there‚Äôs something about its simplicity and elegance (I really like \\(\\lambda\\)s) that makes it way easier to deal with than some monstrosity like the gamma or normal distribution.\nAnother thing I am a big fan of is the book I am currently reading: Surely You‚Äôre Joking, Mr Feynman. In one of the chapters, Dr.¬†Feynman discusses his interest in the power series expansion of \\(e^x\\), and how efficient it is. Naturally I agreed with him, because this same power series expansion is the formula from which the Poisson distribution is derived. Let‚Äôs show this real quick:\n\nPower Series and the Poisson\nFirst let‚Äôs start with the power series:\n\\[\\begin{aligned}\ne^{\\lambda} = & \\sum_{n = 0}^{\\infty} \\frac{\\lambda ^ n}{n!} && \\\\\ne^{\\lambda}/e^{\\lambda} = & \\sum_{n = 0}^{\\infty} \\frac{\\lambda ^ n}{n!} /e^{\\lambda}&&  \\text{divide by }e^{\\lambda}\\\\\n1 = & \\sum_{n = 0}^{\\infty} \\frac{e^{-\\lambda}\\lambda ^ n}{n!}&& \\text{simplify }\\\\\n\\end{aligned}\\]\nWe can see that by simply dividing the power series representation by \\(e^{\\lambda}\\) (aka multiplying by \\(e^{-\\lambda}\\)) we get a series that sums to one, when we have a series (or integral) that comes out to one, that (usually) means we‚Äôre dealing with a distribution function. ‚äïThis idea of making a probability distribution out of basically any series that sums or integrates to one is a trick that will come in massively handy anytime you‚Äôre doing any sort of probability theory work. Simply find the constant and divide by it to switch back and forth between a series/ probability. In this case it‚Äôs the Poisson. To get the probability mass function (since we are dealing with the nicer discrete case) we simply take a single step of this function, aka \\(\\frac{e^{-\\lambda}\\lambda ^ n}{n!}\\).\nThis means that any probability associated with a specific count for a poisson distributed variable is simply a scaled step in the power series expansion of \\(e^{\\lambda}\\)! This is the kind of stuff that gets me excited.\n\n\nInvestigating the power series\nLet‚Äôs go back to the power series and investigate its form:\n\\[\\begin{aligned}\ne^{\\lambda} = & \\sum_{n = 0}^{\\infty} \\frac{\\lambda ^ n}{n!} = 1/1 + \\lambda/1 + \\lambda^2 / 2 + ...\n\\end{aligned}\\]\nWe see the first term is always 1 ‚äïYou can use the fact that the first term in this power expansion is one to reason about why something raised to the zero power is one and not zero. and then after that we get a series of terms that change in size following a nice pattern. To get the \\(i^{th}\\) step we simply take the \\((j - 1)^{th}\\) step and multiply it by \\(\\lambda\\) and divide it by \\(j\\) (\\(s_{j+ 1} = s_j\\lambda/j.\\)) It‚Äôs quite beautiful in its simplicity. By carrying this process out an infinite number of times our sum will converge to the true value of \\(e^{\\lambda}\\), and rather rapidly at that. ‚äïRecognizing patterns like this one can result in massively sped up algorithms. For instance the naive way to calculate this series would be to perform n factorial calculations, where as if you use the iterative pattern described you only ever have to multiply and divide at each step.\nIn order to demonstrate this I have put together a little interactive that allows you to fiddle with the exponent for \\(e^{\\lambda}\\) and the number of times you carry out this sum operation and see how well the series approximates the true value.\nThe bar plot shows the size of each step, aka how big is \\(\\lambda^i/i!\\), above this is the sum of all of the bars, aka the series estimate of \\(e^{\\lambda}\\) at the number of steps you have specified.\n\n\n\n\n\n\n\n\n‚äïNote that as you increase the exponent the number of steps you need to get a good estimate increases with it, why might this be?\nDo you have a favorite series or mathematical construct? Let me know!\n\n\nAddendum\nI really am a large fan of the Poisson distribution, so much so I even have it permanently written on my body.\n\n\nFinally did it. Guess im stuck with this statistics thing now. #poissonPmf pic.twitter.com/DtTPzOiErd\n\n‚Äî Nick Strayer (@NicholasStrayer) August 19, 2017"
  },
  {
    "objectID": "posts/2019-01-17-understanding-propensity-score-weighting/index.html",
    "href": "posts/2019-01-17-understanding-propensity-score-weighting/index.html",
    "title": "Understanding propensity score weighting",
    "section": "",
    "text": "For this post, I‚Äôm going to use this generated dataset as an example. ‚äïHere I am using the terminology ‚Äòtreatment‚Äô for consistency, but these methods are not confined to the medical setting. You can also think of this as an ‚Äòexposure‚Äô. For example, you could be interested in how ‚Äòexposing‚Äô users to a certain UI effects an outcome, like whether they purchase a product. Here, I am simulating two variables, \\(x_1\\) and \\(x_2\\). These are my pre-treatment characteristics (we‚Äôll define that soon!). I am then using these to create a binary treatment variable. This treatment variable depends on x_1 and x_2. Finally, I have generated an outcome variable dependent on the treatment and these pre-treatment characteristics, making the treatment effect equal to 2.\n\nlibrary(tidyverse)\nset.seed(928)\nn &lt;- 10000\nX &lt;- mvtnorm::rmvnorm(n,\n                      mean = c(0.5, 1),\n                      sigma = matrix(c(2, 1, 1, 1), ncol = 2)\n)\n\ndat &lt;- tibble(\n  x_1 = X[, 1],\n  x_2 = X[, 2],\n  treatment = as.numeric(- 0.5 + 0.25 * x_1 + 0.75 * x_2 + rnorm(n, 0, 1) &gt; 0),\n  outcome = 2 * treatment + x_1 + x_2 + rnorm(n, 0, 1)\n)\ndat\n\n# A tibble: 10,000 √ó 4\n      x_1    x_2 treatment outcome\n    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 -0.554  1.57          0   0.832\n 2 -0.154  0.949         1   2.68 \n 3  0.138  2.42          1   5.12 \n 4  0.668  1.36          1   4.27 \n 5  3.11   2.76          1   7.87 \n 6 -0.151  0.683         0  -0.260\n 7  1.01  -0.414         0   0.551\n 8 -0.114  0.856         0   1.63 \n 9  2.66   1.23          1   6.41 \n10  2.96   2.53          1   8.91 \n# ‚Ñπ 9,990 more rows"
  },
  {
    "objectID": "posts/2019-01-17-understanding-propensity-score-weighting/index.html#lets-create-a-toy-dataset",
    "href": "posts/2019-01-17-understanding-propensity-score-weighting/index.html#lets-create-a-toy-dataset",
    "title": "Understanding propensity score weighting",
    "section": "",
    "text": "For this post, I‚Äôm going to use this generated dataset as an example. ‚äïHere I am using the terminology ‚Äòtreatment‚Äô for consistency, but these methods are not confined to the medical setting. You can also think of this as an ‚Äòexposure‚Äô. For example, you could be interested in how ‚Äòexposing‚Äô users to a certain UI effects an outcome, like whether they purchase a product. Here, I am simulating two variables, \\(x_1\\) and \\(x_2\\). These are my pre-treatment characteristics (we‚Äôll define that soon!). I am then using these to create a binary treatment variable. This treatment variable depends on x_1 and x_2. Finally, I have generated an outcome variable dependent on the treatment and these pre-treatment characteristics, making the treatment effect equal to 2.\n\nlibrary(tidyverse)\nset.seed(928)\nn &lt;- 10000\nX &lt;- mvtnorm::rmvnorm(n,\n                      mean = c(0.5, 1),\n                      sigma = matrix(c(2, 1, 1, 1), ncol = 2)\n)\n\ndat &lt;- tibble(\n  x_1 = X[, 1],\n  x_2 = X[, 2],\n  treatment = as.numeric(- 0.5 + 0.25 * x_1 + 0.75 * x_2 + rnorm(n, 0, 1) &gt; 0),\n  outcome = 2 * treatment + x_1 + x_2 + rnorm(n, 0, 1)\n)\ndat\n\n# A tibble: 10,000 √ó 4\n      x_1    x_2 treatment outcome\n    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 -0.554  1.57          0   0.832\n 2 -0.154  0.949         1   2.68 \n 3  0.138  2.42          1   5.12 \n 4  0.668  1.36          1   4.27 \n 5  3.11   2.76          1   7.87 \n 6 -0.151  0.683         0  -0.260\n 7  1.01  -0.414         0   0.551\n 8 -0.114  0.856         0   1.63 \n 9  2.66   1.23          1   6.41 \n10  2.96   2.53          1   8.91 \n# ‚Ñπ 9,990 more rows"
  },
  {
    "objectID": "posts/2019-01-17-understanding-propensity-score-weighting/index.html#what-are-propensity-scores-good-for",
    "href": "posts/2019-01-17-understanding-propensity-score-weighting/index.html#what-are-propensity-scores-good-for",
    "title": "Understanding propensity score weighting",
    "section": "What are propensity scores good for?",
    "text": "What are propensity scores good for?\nA lot of my research is in the observational study space. This basically mean that participants in the study were not randomly assigned treatments or exposures, but rather we just observe how a certain exposure affects an outcome. For example, in one of the studies I worked on, we were interested in whether a certain diabetes drug was associated with heart disease. Instead of randomly assigning some patients to take diabetes drug A and some to take a diabetes drug B, we evaluated the electronic health records of patients who were already taking the drugs and assessed their health after. There are some issues with this analysis - since we didn‚Äôt randomly assign patients to drug A and drug B, it is possible that doctors selected one drug over the other for certain reasons that reflect patient characteristics. For example, perhaps healthier patients are often prescribed drug A ‚Äì this could make it look like those who take drug B are more likely to have heart disease simply based on their pre-treatment characteristics. Propensity scores help to adjust for these pre-treatment characteristics."
  },
  {
    "objectID": "posts/2019-01-17-understanding-propensity-score-weighting/index.html#what-is-a-propensity-score",
    "href": "posts/2019-01-17-understanding-propensity-score-weighting/index.html#what-is-a-propensity-score",
    "title": "Understanding propensity score weighting",
    "section": "What is a propensity score?",
    "text": "What is a propensity score?\nA propensity score is the probability of being assigned to a certain treatment, conditional on pre-treatment (or baseline) characteristics. This can be estimated in different ways, but most commonly it is estimated using logistic regression. Using the simulated dataset from above, we can do this in R with the following code:\n\ndat &lt;- dat %&gt;%\n  mutate(\n    propensity_score = glm(treatment ~ x_1 + x_2, data = dat, family = \"binomial\") %&gt;%\n      predict(type = \"response\")\n  )\n\nNotice here I fit a logistic regression, predicting the treatment from the pre-treatment characteristics, x_1 and x_2 using the glm() function along with the family = \"binomial\" option. I then used the predict() function along with type = \"response\" to obtain the conditional probabilities of treatment assignment."
  },
  {
    "objectID": "posts/2019-01-17-understanding-propensity-score-weighting/index.html#what-are-we-trying-to-estimate",
    "href": "posts/2019-01-17-understanding-propensity-score-weighting/index.html#what-are-we-trying-to-estimate",
    "title": "Understanding propensity score weighting",
    "section": "What are we trying to estimate?",
    "text": "What are we trying to estimate?\nThe point of the propensity score is to allow you to estimate the treatment or exposure effect in an unbiased way. This works based on a few assumptions:\n\nYou have measured all of the necessary pre-treatment characteristics - this is sometimes known as ‚Äúno unmeasured confounding‚Äù\nAll participants have a non-zero probability of receiving either exposure / treatment\n\nUltimately, we are interested in some estimate of the treatment effect, for example we may want to know what the average treatment effect is across all participants, or we may want to know what the average treatment effect is among participants who received the treatment. I‚Äôll describe a few different causal quantities of interest.\n\nAverage Treatment Effect\nThe Average Treatment Effect (ATE) is generally the quantity estimated when running a randomized study. The target population is the whole population, both treated and controlled. While this is often declared as the population of interest, it is not always the medically or scientifically appropriate population. This is because estimating the ATE assumes that every participant can be switched from their current treatment to the opposite, which doesn‚Äôt always make sense. For example, it may not be medically appropriate for every participant who didn‚Äôt receive a treatment to receive it.\n\n\nAverage Treatment Effect Among the Treated\nThe Average Treatment Effect Among the Treated (ATT) estimates the treatment effect with the treated population as the target population.\n\n\nAverage Treatment Effect Among the Controls\nThe Average Treatment Effect Among the Controls (ATC) estimates the treatment effect with the controlled population as the target population.\n\n\nAverage Treatment Effect Among the Evenly Matchable\nThe Average Treatment Effect Among the Evenly Matchable (ATM) estimates the treatment effect with a matched population as the target population. The estimated population is nearly equivalent to the cohort formed by one-to-one pair matching.\n‚äïMy dissertation explores some nice properties of ATM and ATO weights if you are interested in a deeper dive!\n\n\nAverage Treatment Effect Among the Overlap Population\nThe Average Treatment Effect Among the Overlap Population (ATO) estimates the treatment effect very similar to the ATM, with some improved variance properties. Basically, if you estimated the probability of receiving treatment, the ‚Äúoverlap‚Äù population would consist of participants who fall in the middle - you‚Äôre estimating the treatment effect among those likely to have received either treatment or control. I‚Äôll include some graphs in the following sections to help better understand this causal quantity."
  },
  {
    "objectID": "posts/2019-01-17-understanding-propensity-score-weighting/index.html#how-do-we-incorporate-a-propensity-score-in-a-weight",
    "href": "posts/2019-01-17-understanding-propensity-score-weighting/index.html#how-do-we-incorporate-a-propensity-score-in-a-weight",
    "title": "Understanding propensity score weighting",
    "section": "How do we incorporate a propensity score in a weight?",
    "text": "How do we incorporate a propensity score in a weight?\nEach of these causal quantities has a weight associated with it. Brace yourself for a little bit of math (but I‚Äôll include R code immediate after so hopefully it won‚Äôt be too bad!).\nThe propensity score for participant \\(i\\) is defined here as \\(e_i\\) and the treatment assignment is \\(Z_i\\), where \\(Z = 1\\) indicates the participant received the treatment and \\(Z = 0\\) indicates they received the control.\n\\(w_{ATE} = \\frac{Z_i}{e_i} + \\frac{1 - Z_i}{1 - e_i}\\)\n\\(w_{ATT} = \\frac{e_iZ_i}{e_i} + \\frac{e_i(1-Z_i)}{1-e_i}\\)\n\\(w_{ATC} = \\frac{(1-e_i)Z_i}{e_i} + \\frac{(1-e_i) (1-Z_i)}{1 - e_i}\\)\n\\(w_{ATM} = \\frac{\\min\\{e_i, 1-e_i\\}}{Z_ie_i + (1- Z_i)(1-e_i)}\\)\n\\(w_{AT0} = (1-e_i)Z_i + e_i(1-Z_i)\\)\nPhew, okay let‚Äôs get this coded up!\n\ndat &lt;- dat %&gt;%\n  mutate(\n    w_ate = (treatment / propensity_score) + \n      ((1 - treatment) / (1 - propensity_score)),\n    w_att = ((propensity_score * treatment) / propensity_score) + \n      ((propensity_score * (1 - treatment)) / (1 - propensity_score)),\n    w_atc = (((1 - propensity_score) * treatment) / propensity_score) + \n      (((1 - propensity_score) * (1 - treatment)) / (1 - propensity_score)),\n    w_atm = pmin(propensity_score, 1 - propensity_score) / \n      (treatment * propensity_score + (1 - treatment) * (1 - propensity_score)),\n    w_ato = (1 - propensity_score) * treatment + \n      propensity_score * (1 - treatment)\n  )\n\nLet‚Äôs look at some graphs to better understand what these weights are doing. ‚äïI first saw these mirrored histograms in this context in a paper by Li & Greene and loved them!\nFirst, I am going to just plot a histogram of the propensity scores for the two populations, those who received treatment (treatment = 1), and those who received the control (treatment = 0). The histogram above the 0 line is the distribution of propensity scores among the treated. The histogram below the 0 line is the distribution of propensity scores among the controls.\n\nd &lt;- dat %&gt;%\n  tidyr::spread(treatment, propensity_score, sep = \"_p\")\n\nggplot(d) + \n  geom_histogram(bins = 50, aes(treatment_p1)) + \n  geom_histogram(bins = 50, aes(x = treatment_p0, y = -..count..)) + \n  ylab(\"count\") + xlab(\"p\") +\n  geom_hline(yintercept = 0, lwd = 0.5) +\n  scale_y_continuous(label = abs) \n\n\n\n\nExamining this, we can see in this simulated sample, more patients received the treatment than the control. We can also see that among those who received the treatment, propensity scores tend to be higher (we can see this based on the skew of the histogram, with more mass towards the right where the propensity scores are closer to 1). This makes sense, since the propensity score is the probability of receiving treatment, those who received it probably had a higher probability of doing so.\nNow, for each weight I am going to overlay the pseudo-population that is created via weighting. The ‚Äúpseudo-population‚Äù for the treated group is in green and the ‚Äúpseudo-population‚Äù for the control group is in blue.\n\nATE\n\nggplot(d) +\n  geom_histogram(bins = 50, aes(treatment_p1), alpha = 0.5) + \n  geom_histogram(bins = 50, aes(treatment_p1, weight = w_ate), fill = \"green\", alpha = 0.5) + \n  geom_histogram(bins = 50, alpha = 0.5, aes(x = treatment_p0, y = -..count..)) + \n  geom_histogram(bins = 50, aes(x = treatment_p0, weight = w_ate, y = -..count..), fill = \"blue\", alpha = 0.5) + \n  ylab(\"count\") + xlab(\"p\") +\n  geom_hline(yintercept = 0, lwd = 0.5) +\n  scale_y_continuous(label = abs) \n\n\n\n\n\n\nATT\n\nggplot(d) +\n  geom_histogram(bins = 50, aes(treatment_p1), alpha = 0.5) + \n  geom_histogram(bins = 50, aes(treatment_p1, weight = w_att), fill = \"green\", alpha = 0.5) + \n  geom_histogram(bins = 50, alpha = 0.5, aes(x = treatment_p0, y = -..count..)) + \n  geom_histogram(bins = 50, aes(x = treatment_p0, weight = w_att, y = -..count..), fill = \"blue\", alpha = 0.5) + \n  ylab(\"count\") + xlab(\"p\") +\n  geom_hline(yintercept = 0, lwd = 0.5) +\n  scale_y_continuous(label = abs) \n\n\n\n\nNotice here, the ‚Äúpseudo-population‚Äù for the treatment group is exactly the same as the actual population. For the ATT, we take the treatment population as it is and try to weight the control population to match it.\n\n\nATC\n\nggplot(d) +\n  geom_histogram(bins = 50, aes(treatment_p1), alpha = 0.5) + \n  geom_histogram(bins = 50, aes(treatment_p1, weight = w_atc), fill = \"green\", alpha = 0.5) + \n  geom_histogram(bins = 50, alpha = 0.5, aes(x = treatment_p0, y = -..count..)) + \n  geom_histogram(bins = 50, aes(x = treatment_p0, weight = w_atc, y = -..count..), fill = \"blue\", alpha = 0.5) + \n  ylab(\"count\") + xlab(\"p\") +\n  geom_hline(yintercept = 0, lwd = 0.5) +\n  scale_y_continuous(label = abs) \n\n\n\n\nNotice now this is the opposite of the previous graph. Now the target population is the control group, so this ‚Äúpseudo-population‚Äù exactly matches the population of controls and the treatment group is weighted to try to mimic this.\n\n\nATM\n\nggplot(d) +\n  geom_histogram(bins = 50, aes(treatment_p1), alpha = 0.5) + \n  geom_histogram(bins = 50, aes(treatment_p1, weight = w_atm), fill = \"green\", alpha = 0.5) + \n  geom_histogram(bins = 50, alpha = 0.5, aes(x = treatment_p0, y = -..count..)) + \n  geom_histogram(bins = 50, aes(x = treatment_p0, weight = w_atm, y = -..count..), fill = \"blue\", alpha = 0.5) + \n  ylab(\"count\") + xlab(\"p\") +\n  geom_hline(yintercept = 0, lwd = 0.5) +\n  scale_y_continuous(label = abs) \n\n\n\n\nThis ‚Äúpseudo-population‚Äù looks like a 1:1 matched population. In the regions where the treatment group is in the minority, on the left side of the graph, the controls are weighted to match the distribution of the treated. In the regions where the control group is in the minority, on the right side of the graph, the treated are weighted to match the distribution of the controls.\n\n\nATO\n\nggplot(d) +\n  geom_histogram(bins = 50, aes(treatment_p1), alpha = 0.5) + \n  geom_histogram(bins = 50, aes(treatment_p1, weight = w_ato), fill = \"green\", alpha = 0.5) + \n  geom_histogram(bins = 50, alpha = 0.5, aes(x = treatment_p0, y = -..count..)) + \n  geom_histogram(bins = 50, aes(x = treatment_p0, weight = w_ato, y = -..count..), fill = \"blue\", alpha = 0.5) + \n  ylab(\"count\") + xlab(\"p\") +\n  geom_hline(yintercept = 0, lwd = 0.5) +\n  scale_y_continuous(label = abs) \n\n\n\n\nThis looks a lot like the ATM graph, except shifted just a bit to improve the variance properties."
  },
  {
    "objectID": "posts/2019-01-17-understanding-propensity-score-weighting/index.html#estimating-the-treatment-effect",
    "href": "posts/2019-01-17-understanding-propensity-score-weighting/index.html#estimating-the-treatment-effect",
    "title": "Understanding propensity score weighting",
    "section": "Estimating the treatment effect",
    "text": "Estimating the treatment effect\nNow that we have a bit of an understanding of what these weights do, we can estimate the treatment effect in each of these ‚Äúpseudo-populations‚Äù. Mathematically, we can write this as follows, with the outcome for each individual defined as \\(Y_i\\), the treatment \\(Z_i\\) and the weight \\(w_i\\).\n\\(\\frac{\\sum{Y_i Z_i w_i}}{\\sum{Z_i w_i}} - \\frac{\\sum{Y_i(1-Z_i)w_i}}{\\sum{(1-Z_i)w_i}}\\)\nIn R, we can make a function to do this.\n\ntreatment_effect &lt;- function(treatment, outcome, weight) {\n  (sum(treatment * outcome * weight) / sum(treatment * weight)) - (sum((1 - treatment) * outcome * weight) / sum((1 - treatment) * weight))\n}\n\nFor example, to estimate the ATO for this population, we can do the following.\n\ntreatment_effect(dat$treatment, dat$outcome, dat$w_ato)\n\n[1] 2.006989\n\n\nAnd we did it! Hopefully this helped to elucidate what is going on with propensity score weighting - please let me know if there are any parts that can be clarified!"
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html",
    "title": "Wrangling Data Day Texas Slides",
    "section": "",
    "text": "Since twitter threads are excessively cumbersome to navigate, Ma√´lle asked me to relocate the list of #rstats Data Day Texas slides to a blog post, so here we are!\nThe titles link to the slides r emo::ji(\"dancers\")"
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#pilgrims-progress-a-journey-from-confusion-to-contribution",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#pilgrims-progress-a-journey-from-confusion-to-contribution",
    "title": "Wrangling Data Day Texas Slides",
    "section": "Pilgrim‚Äôs Progress: a journey from confusion to contribution",
    "text": "Pilgrim‚Äôs Progress: a journey from confusion to contribution\n\nMara Averick\nNavigating the data science landscape can be overwhelming. Luckily, you don‚Äôt have to do it alone! In fact, I‚Äôll argue shouldn‚Äôt do it alone. Whether it be by tweeting your latest mistake, asking a well-formed question, or submitting a pull request to a popular package, you can help others and yourselves by ‚Äúlearning out loud.‚Äù No matter how much (or little) you know, you can turn your confusion into contributions, and have a surprising amount of fun along the way."
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#making-causal-claims-as-a-data-scientist-tips-and-tricks-using-r",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#making-causal-claims-as-a-data-scientist-tips-and-tricks-using-r",
    "title": "Wrangling Data Day Texas Slides",
    "section": "Making Causal Claims as a Data Scientist: Tips and Tricks Using R",
    "text": "Making Causal Claims as a Data Scientist: Tips and Tricks Using R\n\nLucy D‚ÄôAgostino McGowan\nMaking believable causal claims can be difficult, especially with the much repeated adage ‚Äúcorrelation is not causation‚Äù. This talk will walk through some tools often used to practice safe causation, such as propensity scores and sensitivity analyses. In addition, we will cover principles that suggest causation such as the understanding of counterfactuals, and applying Hill‚Äôs criteria in a data science setting. We will walk through specific examples, as well as provide R code for all methods discussed."
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#statistics-for-data-science-what-you-should-know-and-why",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#statistics-for-data-science-what-you-should-know-and-why",
    "title": "Wrangling Data Day Texas Slides",
    "section": "Statistics for Data Science: what you should know and why",
    "text": "Statistics for Data Science: what you should know and why\n\nGabriela de Queiroz\nData science is not only about machine learning. To be a successful data person, you also need a significant understanding of statistics. Gabriela de Queiroz walks you through the top five statistical concepts every Data Scientist should know to work with data."
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#r-what-is-it-good-for-absolutely-everything",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#r-what-is-it-good-for-absolutely-everything",
    "title": "Wrangling Data Day Texas Slides",
    "section": "R, What is it good for? Absolutely Everything",
    "text": "R, What is it good for? Absolutely Everything\n\nJasmine Dumas\nGood does not mean great, but good is better than bad. When we try to compare programming languages we tend to look at the surface components (popular developer influence, singular use cases or language development & design choices) and sometimes we forget the substantive (sometimes secondary) components of what can make a programming language appropriate for use, such as: versatility, environment and inclusivity. I‚Äôll highlight each of these themes in the presentation to show and not tell of why R is good for everything!"
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#infer-an-r-package-for-tidy-statistical-inference",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#infer-an-r-package-for-tidy-statistical-inference",
    "title": "Wrangling Data Day Texas Slides",
    "section": "infer: an R package for tidy statistical inference",
    "text": "infer: an R package for tidy statistical inference\n\nChester Ismay\nHow do you code-up a permutation test in R? What about an ANOVA or a chi-square test? Have you ever been uncertain as to exactly which type of test you should run given the data and questions asked? The infer package was created to unite common statistical inference tasks into an expressive and intuitive framework to alleviate some of these struggles and make inference more intuitive. This talk will focus on the design principles of the package, which are firmly motivated by Hadley Wickham‚Äôs tidy tools manifesto. It will also discuss the implementation, centered on the common conceptual threads that link a surprising range of hypothesis tests and confidence intervals. Lastly, we‚Äôll walk through some examples of how to implement the code of the infer package. The package is aimed to be useful to new students of statistics as well as seasoned practitioners."
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#something-old-something-new-something-borrowed-something-blue-ways-to-teach-data-science-and-learn-it-too",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#something-old-something-new-something-borrowed-something-blue-ways-to-teach-data-science-and-learn-it-too",
    "title": "Wrangling Data Day Texas Slides",
    "section": "Something old, something new, something borrowed, something blue: Ways to teach data science (and learn it too!)",
    "text": "Something old, something new, something borrowed, something blue: Ways to teach data science (and learn it too!)\n\nAlbert Y. Kim\nHow can we help newcomers take their first steps into the world of data science and statistics? In this talk, I present ModernDive: An Introduction to Statistical and Data Sciences via R, an open source, fully reproducible electronic textbook available at ModernDive.com, co-authored by myself and Chester Ismay, Data Science Curriculum Lead at DataCamp. ModernDive‚Äôs authoring follows a paradigm of ‚Äúversions, not editions‚Äù much more in line with software development than traditional textbook publishing, as it is built using RStudio‚Äôs bookdown interface to R Markdown. In this talk, I will present details on our book‚Äôs construction, our approaches to teaching novices to use tidyverse tools for data science (in particular ggplot2 for data visualization and dplyr for data wrangling), how we leverage these data science tools to teach data modeling via regression, and preview the new infer package for statistical inference, which performs statistical inference using an expressive syntax that follows tidy design principles. We‚Äôll conclude by presenting example vignettes and R Markdown analyses created by undergraduate students to demonstrate the great potential yielded by effectively empowering new data scientists with the right tools."
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#building-shiny-apps-challenges-and-responsibilities",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#building-shiny-apps-challenges-and-responsibilities",
    "title": "Wrangling Data Day Texas Slides",
    "section": "Building Shiny Apps: Challenges and Responsibilities",
    "text": "Building Shiny Apps: Challenges and Responsibilities\n\nJessica Minnier\nR Shiny has revolutionized the way statisticians and data scientists distribute analytic results and research methods. We can easily build interactive web tools that empower non-statisticians to interrogate and visualize their data or perform their own analyses with methods we develop. However, ensuring the user has an enjoyable experience while guaranteeing the analyses options are statistically sound is a difficult balance to achieve. Through a case study of building START (Shiny Transcriptome Analysis Resource Tool), a shiny app for ‚Äúomics‚Äù data visualization and analysis, I will present the challenges you may face when building and deploying an app of your own. By allowing the non-statistician user to explore and analyze data, we can make our job easier and improve collaborative relationships, but the success of this goal requires software development skills. We may need to consider such issues as data security, open source collaborative code development, error handling and testing, user education, maintenance due to advancing methods and packages, and responsibility for downstream analyses and decisions based on the app‚Äôs results. With Shiny we do not want to fully eliminate the statistician or analyst ‚Äúmiddle man‚Äù but instead need to stay relevant and in control of all types of statistical products we create."
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#using-r-on-small-teams-in-industry",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#using-r-on-small-teams-in-industry",
    "title": "Wrangling Data Day Texas Slides",
    "section": "Using R on small teams in industry",
    "text": "Using R on small teams in industry\n\nJonathan Nolis\nDoing statistical analyses and machine learning in R requires many different components: data, code, models, outputs, and presentations. While one person can usually keep track of their own work, as you grow into a team of people it becomes more important to keep coordinated. This session discusses the work we do data science work at Lenati, a marketing and strategy consulting firm, and why R is a great tool for us. It covers the best practices we found for working on R code together over many projects and people, and how we handle the occasional instances where we must use other languages."
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#the-lesser-known-stars-of-the-tidyverse",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#the-lesser-known-stars-of-the-tidyverse",
    "title": "Wrangling Data Day Texas Slides",
    "section": "The Lesser Known Stars of the Tidyverse",
    "text": "The Lesser Known Stars of the Tidyverse\n\nEmily Robinson\nWhile most R programmers have heard of ggplot2 and dplyr, many are unfamiliar with the breath of the tidyverse and the variety of problems it can solve. In this talk, we will give a brief introduction to the concept of the tidyverse and then describe three packages you can immediately start using to make your workflow easier. The first package is forcats, designed for making working with categorical variables easier; the second is glue, for programmatically combining data and strings; and the third package is tibble, an alternative to data.frames. We will cover their basic functions so that, at the end of the talk, we will be able to use and learn more about the broader tidyverse."
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#text-mining-using-tidy-data-principles",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#text-mining-using-tidy-data-principles",
    "title": "Wrangling Data Day Texas Slides",
    "section": "Text Mining Using Tidy Data Principles",
    "text": "Text Mining Using Tidy Data Principles\n\nJulia Silge\nText data is increasingly important in many domains, and tidy data principles and tidy tools can make text mining easier and more effective. I will demonstrate how we can manipulate, summarize, and visualize the characteristics of text using these methods and R packages from the tidy tool ecosystem. These tools are highly effective for many analytical questions and allow analysts to integrate natural language processing into effective workflows already in wide use. We will explore how to implement approaches such as sentiment analysis of texts, measuring tf-idf, and measuring word vectors."
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#speeding-up-r-with-parallel-programming-in-the-cloud",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#speeding-up-r-with-parallel-programming-in-the-cloud",
    "title": "Wrangling Data Day Texas Slides",
    "section": "Speeding up R with Parallel Programming in the Cloud",
    "text": "Speeding up R with Parallel Programming in the Cloud\n\nDavid Smith\nThere are many common workloads in R that are ‚Äúembarrassingly parallel‚Äù: group-by analyses, simulations, and cross-validation of models are just a few examples. In this talk I‚Äôll describe several techniques available in R to speed up workloads like these, by running multiple iterations simultaneously, in parallel. Many of these techniques require the use of a cluster of machines running R, and I‚Äôll provide examples of using cloud-based services to provision clusters for parallel computations. In particular, I will describe how you can use the SparklyR package to distribute data manipulations using the dplyr syntax, on a cluster of servers provisioned in the Azure cloud."
  },
  {
    "objectID": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#making-magic-with-keras-and-shiny",
    "href": "posts/2018-01-28-wrangling-data-day-texas-slides/index.html#making-magic-with-keras-and-shiny",
    "title": "Wrangling Data Day Texas Slides",
    "section": "Making Magic with Keras and Shiny",
    "text": "Making Magic with Keras and Shiny\n\nNicholas Strayer\nThe web-application framework Shiny has opened up enormous opportunities for data scientists by giving them a way to bring their models and visualizations to the public in interactive applications with only R code. Likewise, the package keras has simplified the process of getting up and running with deep-neural networks by abstracting away much of the boiler-plate and book-keeping associated with writing models in a lower-level library such as tensorflow. In this presentation, I will demo and discuss the development of a shiny app that allows users to cast ‚Äòspells‚Äô simply by waving their phone around like a wand. The app gathers the motion of the device using the library shinysense and feeds it into a convolutional neural network which predicts spell casts with high accuracy. A supplementary shiny app for gathering data will be also be shown. These applications demonstrate the ability for shiny to be used at both the data-gathering and model-presentation steps of data science."
  },
  {
    "objectID": "posts/2018-04-09-network3d-demo/index.html",
    "href": "posts/2018-04-09-network3d-demo/index.html",
    "title": "network3d - a 3D network visualization and layout library",
    "section": "",
    "text": "network3d is a tiny R package built using the htmlwidgets package that takes network data in the form of a node and edge dataframes and performs a physics simulation to determine the optimal layout in three dimensions. There is a lot of customization you can do but attempts have been made to make it as simple to use as possible.\n‚äïI appologize in advance: this page will most likely be super slow at first. This is because I have un-wisely set three seperate one thousand node network visualizations to run and at the same time calculate their 3D layout all using our personal computer‚Äôs hardware. In practice you really would only use one so things shouldn‚Äôt be so sloppy. Here we‚Äôre doing a bunch so you can see some of the customization options available to you."
  },
  {
    "objectID": "posts/2018-04-09-network3d-demo/index.html#what-is-network3d",
    "href": "posts/2018-04-09-network3d-demo/index.html#what-is-network3d",
    "title": "network3d - a 3D network visualization and layout library",
    "section": "",
    "text": "network3d is a tiny R package built using the htmlwidgets package that takes network data in the form of a node and edge dataframes and performs a physics simulation to determine the optimal layout in three dimensions. There is a lot of customization you can do but attempts have been made to make it as simple to use as possible.\n‚äïI appologize in advance: this page will most likely be super slow at first. This is because I have un-wisely set three seperate one thousand node network visualizations to run and at the same time calculate their 3D layout all using our personal computer‚Äôs hardware. In practice you really would only use one so things shouldn‚Äôt be so sloppy. Here we‚Äôre doing a bunch so you can see some of the customization options available to you."
  },
  {
    "objectID": "posts/2018-04-09-network3d-demo/index.html#why-was-it-made",
    "href": "posts/2018-04-09-network3d-demo/index.html#why-was-it-made",
    "title": "network3d - a 3D network visualization and layout library",
    "section": "Why was it made?",
    "text": "Why was it made?\n\nNeed for 3d\nThere are plenty of excellent network visualization libraries in R such as ggraph, and igraph. My main issue with these libraries is that they are limited to two dimensional visualizations (or static 3d in the case of igraph). I have personally found that in many network data adding a third dimension allows for much clearer patterns to pop out. In my opinion often worth the sacrifices that are made in terms of perception when using 3d visualizations.\n\n\nNeed for on-the-fly layout calculation\nIn a research project I am working on I built a custom htmlwidget based on the javascript library to do interactive visualization of a rather large network. The problem that I ended up encountering was that frequently we wanted to resize the graph and see how it affected the structure, but to do that we needed to go back into R and recalculate the layout using igraph, which took forever. This prompted me to look into generating the layout right in the htmlwidget with javascript. Lo and behold, it was much faster and now allowed us to refresh the data underlying the graph in a shiny app and have it update much faster than before.\n\n\nNo dependencies\nThere is no need to generate an igraph object to plot. You simply need to be able to get your data into the form of a vertice list with an id column and optional name, color, and size columns and an edge list with two columns source and target being the ids of the vertices the edge connects. I personally found this nice especially when on a fresh computer where the process of installing igraph can be a painful one."
  },
  {
    "objectID": "posts/2018-04-09-network3d-demo/index.html#how-to-install",
    "href": "posts/2018-04-09-network3d-demo/index.html#how-to-install",
    "title": "network3d - a 3D network visualization and layout library",
    "section": "How to install",
    "text": "How to install\nThe package is not on Cran so you need to use the devtools package to install it.\n\ndevtools::install_github('nstrayer/network3d')\n\nNext we can load an included data set from the Stanford graph database. It is a network of the collaborations between researchers in general relativity and quantum cosmology.. Here the name column is essentially just an index of order of appearance.\n\ndata &lt;- collaboration_networks # comes from the package\n\ndata$vertices %&gt;% head() %&gt;% knitr::kable()\n\n\n\n\nid\nname\n\n\n\n\n3466\n1\n\n\n10310\n2\n\n\n5052\n3\n\n\n5346\n4\n\n\n15159\n5\n\n\n19640\n6\n\n\n\n\n\n\ndata$edges %&gt;% head() %&gt;% knitr::kable()\n\n\n\n\nsource\ntarget\n\n\n\n\n3466\n5233\n\n\n3466\n8579\n\n\n3466\n10310\n\n\n3466\n17038\n\n\n10310\n3466\n\n\n10310\n4583\n\n\n\n\n\nNow we‚Äôre ready to make the plot in the simplest way possible, just passing the data.\n\nnetwork3d(data$vertices, data$edges)\n\n\n\n\n\n‚äïLeft click and drag to pan the plot, right click and drag to rotate.\nCool, but we probably want to add some color to out nodes, let‚Äôs do that by adding a color column to the vertices dataframe. In addition, let‚Äôs modify the mouse over text for each node by adding the super helpful prefix of ‚Äònode‚Äô.\n\ndata$vertices &lt;- data$vertices %&gt;% \n  mutate(\n    color = 'steelblue',\n    name = paste('node', name)\n  )\n\nnetwork3d(data$vertices, data$edges)\n\n\n\n\n\nThat looks much better, but the nodes are kind of bunched together, to fix this let‚Äôs set our node size to be smaller and increase our edge opaity to make connections more obvious. In addition we should probably adjust the simulation parameters a bit. Don‚Äôt know how to change them? Go into the force_explorer mode‚Ä¶\nDrag around the different parameters to change the values. Max iterations starts at a default of 75 but for some reason the controls don‚Äôt reflect that, changes will register properly though.\n\nnetwork3d(\n  data$vertices, data$edges, \n  node_size = 0.05, \n  edge_opacity = 0.25,\n  force_explorer = TRUE)\n\n\n\n\n\nGreat, so after fiddling let‚Äôs choose a manybody_strength of 0.5 instead of the default of -1. This will cause the nodes to be attracted to each other instead of repelled (in addition to the forces exerted by their edges).\nOne last attempt to make things interesting: let‚Äôs change the background color to black and the nodes colors to a random selection from an Rcolorbrewer and node sizes to a random uniform draw between 0.01 and 0.1 (this is in the context of the world being a 2x2x2 cube).\n\ndata$vertices &lt;- data$vertices %&gt;% \n  mutate(\n    color =  RColorBrewer::brewer.pal(12, name = 'Paired') %&gt;% sample(size = n(), replace = TRUE),\n    size = runif(n(), min = 0.01, max = 0.1)\n  )\n\nnetwork3d(data$vertices, data$edges, \n          max_iterations = 100,\n          manybody_strength = 0.5, \n          background_color = 'black',\n          edge_opacity = 0.15)\n\n\n\n\n\n‚äïA warning: Network structure and visualization in general is super tricky and can be extremely easy to fiddle with to get the results you want. I would encourage any sort of inference around the structure revealed in these plots to be accompanied with layout-algorithm-free method. Also, the last plot should most likely never be made, but at least it‚Äôs interesting."
  },
  {
    "objectID": "posts/2018-04-09-network3d-demo/index.html#future-work",
    "href": "posts/2018-04-09-network3d-demo/index.html#future-work",
    "title": "network3d - a 3D network visualization and layout library",
    "section": "Future work",
    "text": "Future work\nRight now the package is very light in what it does. This is on purpose. This is mostly a packaging up of a one-off visualization I did for a project that some people may find useful. That being said a few things could be improved and I will if I find the time.\n\nPerformance\nSince the networks are rendered on the viewing computers GPU they will scale great on something like a beefy macbook pro with an integrated graphics card but on lesser devices there may be some hiccups. I have successfully rendered networks with ~6k nodes and 25k edges on my Google pixelbook chromebook with no integrated graphics.\nOne way of improving the performance goes with the interactivity. Right now when you have set the option interactive = TRUE (its default) the network will display whatever node you have moused over‚Äôs value in the name column of the vertices dataframe. The way this is calculated is a beam is projected out from the camera of the scene and every single node in your graph is checked to see if it intersects the node, if some do the closest node that was intersected is selected. This is obviously going to be a super slow process with large networks. One method of making it faster would be to use a spatially aware data structure such as an octtree to limit the nodes searched. Alternatively for faster performance when you don‚Äôt need interactivity set interactive = FALSE. The paradox here is that as the network gets larger the more needed interactivity is to explore.\n\n\nShiny Hooks\nCurrently the networks work well with shiny in that you can send new data to the network and it will auto redraw the network with the new data. However it would be nice to allow some sort of back into shiny communication about which node is selected etc. This wouldn‚Äôt be too hard, however I would want it to conform to the nice crosstalk format, which will necessitate changing a bit about how the library handles data."
  },
  {
    "objectID": "posts/2018-04-09-network3d-demo/index.html#questions-or-bugs",
    "href": "posts/2018-04-09-network3d-demo/index.html#questions-or-bugs",
    "title": "network3d - a 3D network visualization and layout library",
    "section": "Questions or bugs?",
    "text": "Questions or bugs?\nI am excited to see all the ways people use this library. More interestingly I am excited to see how people break it. If you find a bug or are confused about how to do something don‚Äôt hesitate to open up an issue on the github page. Alternatively, if twitter is more your jam, you can at me there (link in my bio card below)."
  },
  {
    "objectID": "posts/2017-01-24-javascript-r-3/index.html",
    "href": "posts/2017-01-24-javascript-r-3/index.html",
    "title": "Custom JavaScript visualizations in RMarkdown",
    "section": "",
    "text": "I happened to stumble upon the preview release page for RStudio recently and noticed something that made me exorbitantly happy.\n‚ÄúSupport for JavaScript and CSS chunks in R Notebooks‚Äù! As someone who loves using javascript for plotting (and secretly for manipulating) data this is massively exciting. Previously my workflow for generating an interactive graphic would be something like:\nThis workflow has served me very well, I did it probably 20 times a day when working at the New York Times and I am very fast with it. If you want to just make a stand alone visualization it‚Äôs fantastic, however, as a biostatistician who works with a lot of other biostatisticians, people tend to want to see where the data comes from.\nWith Javascript chunks in .Rmd files you can explicitly show the code for data gathering/cleaning etc. in a language that your collaborators can understand, along with making custom d3 charts with that data. All without ever leaving RStudio. I will show you my quick and dirty solution to doing so."
  },
  {
    "objectID": "posts/2017-01-24-javascript-r-3/index.html#getting-data-into-javascript",
    "href": "posts/2017-01-24-javascript-r-3/index.html#getting-data-into-javascript",
    "title": "Custom JavaScript visualizations in RMarkdown",
    "section": "Getting data into Javascript",
    "text": "Getting data into Javascript\nInstead of generating a csv file in R and then loading that into javascript we will instead send the data directly through the html to javascript. (Note: This wont work well with super large data).\nInspired by this medium post I wrote a little function that takes a dataframe and sends it to the html document in the .json format.\n\nlibrary(tidyverse)\nlibrary(jsonlite)\n\nsend_df_to_js &lt;- function(df){\n  cat(\n    paste(\n    '&lt;script&gt;\n      var data = ',toJSON(df),';\n    &lt;/script&gt;'\n    , sep=\"\")\n  )\n}\n\nTo illustrate how it works we will generate some random data into a dataframe.\n\n#Generate some random x and y data to plot\nn &lt;- 300\nrandom_data &lt;- data_frame(x = runif(n)*10) %&gt;% \n  mutate(y = 0.5*x^3 - 1.3*x^2 + rnorm(n, mean = 0, sd = 80),\n         group = paste(\"group\", sample(c(1,2,3), n, replace = T)))\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\n‚Ñπ Please use `tibble()` instead.\n\n\nNow we send a snippit of the dataframe into the function to see the output‚Ä¶\n\nrandom_data %&gt;% \n  head() %&gt;% \n  send_df_to_js()\n\n&lt;script&gt;\n      var data = [{\"x\":8.8553,\"y\":330.51,\"group\":\"group 3\"},{\"x\":4.5664,\"y\":-43.3795,\"group\":\"group 3\"},{\"x\":6.0234,\"y\":-14.7295,\"group\":\"group 1\"},{\"x\":6.1653,\"y\":60.0903,\"group\":\"group 3\"},{\"x\":8.3446,\"y\":231.5122,\"group\":\"group 2\"},{\"x\":0.2206,\"y\":38.3577,\"group\":\"group 3\"}];\n    &lt;/script&gt;\n\n\nBeautiful, we have our data, in json format, wrapped in a script tag. Now we can send the whole dataframe through. This time I am using the results = \"asis\" option in the code chunk ({r, results = \"asis\"}), to write the results directly to the html document and not to the output like we did above.\n#Initiate data transfer protocol one\nsend_df_to_js(random_data)\n\nNow our data is inside our page‚Äôs javascript scope and ready to be played with!"
  },
  {
    "objectID": "posts/2017-01-24-javascript-r-3/index.html#drawing-pretty-pictures",
    "href": "posts/2017-01-24-javascript-r-3/index.html#drawing-pretty-pictures",
    "title": "Custom JavaScript visualizations in RMarkdown",
    "section": "Drawing pretty pictures",
    "text": "Drawing pretty pictures\nLet‚Äôs make a super simple d3 scatter plot to see this randomly generated data. All I have to do is include my desired javascript libraries, make a div element for my visualization to go into and then put my {js} block in. RMarkdown will slot the javascript into the page and we‚Äôre good to go.\nIn this example I did‚Ä¶\n&lt;script src=\"https://code.jquery.com/jquery-3.1.1.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"https://d3js.org/d3.v4.min.js\"&gt;&lt;/script&gt;\n\n&lt;div id = \"viz\"&gt;&lt;/div&gt; \n\n` ` `{js}\n//code goes here\n` ` `"
  },
  {
    "objectID": "posts/2017-01-24-javascript-r-3/index.html#did-it-work",
    "href": "posts/2017-01-24-javascript-r-3/index.html#did-it-work",
    "title": "Custom JavaScript visualizations in RMarkdown",
    "section": "Did it work?",
    "text": "Did it work?\n\n\n\n\n\n\nvar point_vals = d3.select(\"#viz\")\n  .append(\"p\")\n  .attr(\"align\", \"center\")\n  .text(\"Mouseover some data!\");\n\n//Get how wide our page is in pixels so we can draw our plot in it\nvar page_width = $(\"#did-it-work\").width();\n  \n// set the dimensions and margins of the graph\nvar margin = 30,\n    width = page_width - 2*margin,\n    height = page_width*0.8 - 2*margin;\n    \n// Find max data values\nvar x_extent = d3.extent(data, d =&gt; d.x);\nvar y_extent = d3.extent(data, d =&gt; d.y);\n\n// Set the scales \nvar x = d3.scaleLinear()\n  .domain(x_extent)\n  .range([0, width]);\n  \nvar y = d3.scaleLinear()\n  .domain(y_extent)\n  .range([height, 0]);\n\n//Set up our SVG element\nvar svg = d3.select(\"#viz\").append(\"svg\")\n    .attr(\"width\", width + 2*margin)\n    .attr(\"height\", height + 2*margin)\n  .append(\"g\")\n    .attr(\"transform\",\n          \"translate(\" + margin + \",\" + margin + \")\");\n\nvar bounce_select = d3.transition()\n    .duration(1000)\n    .ease(d3.easeElastic.period(0.4));\n    \n// Add the scatterplot\nsvg.selectAll(\".dots\")\n    .data(data)\n  .enter().append(\"circle\")\n    .attr(\"class\", \"dots\")\n    .attr(\"fill\", d =&gt; d.group === \"group 1\"? \"steelblue\":\"orangered\")\n    .attr(\"fill-opacity\", 0.3)\n    .attr(\"r\", 5)\n    .attr(\"cx\", d =&gt; x(d.x) )\n    .attr(\"cy\", d =&gt; y(d.y) )\n    .on(\"mouseover\", function(d){\n       d3.selectAll(\".dots\").attr(\"r\", 5) //make sure all the dots are small\n       d3.select(this)\n        .transition(bounce_select)\n        .attr(\"r\", 10);\n      \n       point_vals.text(\"X:\" + d.x + \" Y:\" + d.y) //change the title of the graph to the datapoint\n    });\n    \n// Draw the axes    \n// Add the X Axis\nsvg.append(\"g\")\n    .attr(\"transform\", \"translate(0,\" + height + \")\")\n    .call(d3.axisBottom(x));\n\n// Add the Y Axis\nsvg.append(\"g\")\n    .call(d3.axisLeft(y));"
  },
  {
    "objectID": "posts/2017-01-24-javascript-r-3/index.html#why-when",
    "href": "posts/2017-01-24-javascript-r-3/index.html#why-when",
    "title": "Custom JavaScript visualizations in RMarkdown",
    "section": "Why/ When",
    "text": "Why/ When\nThis is a bad example of a visualization for this scenario as something like plotly could do this in much less effort. If you‚Äôre doing something more complicated/ bespoke then this is a great resource to have."
  },
  {
    "objectID": "posts/2017-01-24-javascript-r-3/index.html#addendum",
    "href": "posts/2017-01-24-javascript-r-3/index.html#addendum",
    "title": "Custom JavaScript visualizations in RMarkdown",
    "section": "Addendum",
    "text": "Addendum\nIf you‚Äôre interested, here‚Äôs the javascript code I included to make the above graph.\nvar point_vals = d3.select(\"#viz\").append(\"p\").text(\"Mouseover some data!\");\n\n//Get how wide our page is in pixels so we can draw our plot in it\nvar page_width = $(\"#did-it-work\").width();\n  \n// set the dimensions and margins of the graph\nvar margin = 20,\n    width = page_width - 2*margin,\n    height = page_width*0.8 - 2*margin;\n    \n// Find max data values\nvar x_extent = d3.extent(data, d =&gt; d.x);\nvar y_extent = d3.extent(data, d =&gt; d.y);\n\n// Set the scales \nvar x = d3.scaleLinear()\n  .domain(x_extent)\n  .range([0, width]);\n  \nvar y = d3.scaleLinear()\n  .domain(y_extent)\n  .range([height, 0]);\n\n//Set up our SVG element\nvar svg = d3.select(\"#viz\").append(\"svg\")\n    .attr(\"width\", width + 2*margin)\n    .attr(\"height\", height + 2*margin)\n  .append(\"g\")\n    .attr(\"transform\",\n          \"translate(\" + margin + \",\" + margin + \")\");\n\nvar bounce_select = d3.transition()\n    .duration(1000)\n    .ease(d3.easeElastic.period(0.4));\n    \n// Add the scatterplot\nsvg.selectAll(\".dots\")\n    .data(data)\n  .enter().append(\"circle\")\n    .attr(\"class\", \"dots\")\n    .attr(\"fill\", d =&gt; d.group === \"group 1\"? \"steelblue\":\"orangered\")\n    .attr(\"fill-opacity\", 0.3)\n    .attr(\"r\", 5)\n    .attr(\"cx\", d =&gt; x(d.x) )\n    .attr(\"cy\", d =&gt; y(d.y) )\n    .on(\"mouseover\", function(d){\n       d3.selectAll(\".dots\").attr(\"r\", 5) //make sure all the dots are small\n       d3.select(this)\n        .transition(bounce_select)\n        .attr(\"r\", 10);\n      \n       point_vals.text(\"X:\" + d.x + \" Y:\" + d.y) //change the title of the graph to the datapoint\n    });\n    \n// Draw the axes    \n// Add the X Axis\nsvg.append(\"g\")\n    .attr(\"transform\", \"translate(0,\" + height + \")\")\n    .call(d3.axisBottom(x));\n\n// Add the Y Axis\nsvg.append(\"g\")\n    .call(d3.axisLeft(y));"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " ",
    "section": "",
    "text": "Visual Diagnostic Tools for Causal Inference\n\n\n\n\n\n\n\nrstats\n\n\ncausal inference\n\n\nmirrored histograms\n\n\nbalance\n\n\npropensity scores\n\n\ntreatment effect heterogeneity\n\n\n\n\nHere we are going to look at several diagnostic plots that are helpful when attempting to answer a causal question. They can be used to visualize the target population, balance, and treatment effect heterogeneity.\n\n\n\n\n\n\nAug 4, 2023\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nVisual Diagnostic Tool for Causal Inference: Heterogeneous Treatment Effects\n\n\n\n\n\n\n\nrstats\n\n\ncausal inference\n\n\ntreatment heterogeneity\n\n\n\n\nA simple diagnostic plot to examine potential treatment heterogeneity ‚Äì what‚Äôs old is new!\n\n\n\n\n\n\nJul 26, 2023\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nAre two wrong models better than one? Missing data style\n\n\n\n\n\n\n\nrstats\n\n\nsimulations\n\n\nmissing data\n\n\n\n\nWould imputing + fitting an outcome model using the wrong variables be better than just fitting the wrong outcome model? Let‚Äôs investigate!\n\n\n\n\n\n\nApr 29, 2023\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nWhen is complete case analysis unbiased?\n\n\n\n\n\n\n\nrstats\n\n\nsimulations\n\n\nmissing data\n\n\n\n\nI have been thinking about scenarios under which it makes sense to use imputation for prediction models and am struggling to come up with a case. Yikes! Even for inference, as long as you do some doubly robust approach, I‚Äôm not sure I see the value (other than for precision, but then is no longer a question of bias and thus is a question for a different day!)\n\n\n\n\n\n\nApr 28, 2023\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nIt‚Äôs just a linear model: neural networks edition\n\n\n\n\n\n\n\nstatistical communication\n\n\ndata science pedoagogy\n\n\nneural-networks\n\n\n\n\nI created a little Shiny application to demonstrate that Neural Networks are just souped up linear models: https://lucy.shinyapps.io/neural-net-linear/\n\n\n\n\n\n\nApr 27, 2023\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nCausal Quartets\n\n\n\n\n\n\n\ncausal inference\n\n\nstatistical communication\n\n\ndata science pedoagogy\n\n\nconfounding\n\n\n\n\nOn this weeks episode of Casual Inference we talk about a ‚ÄúCausal Quartet‚Äù a set of four datasets generated under different mechanisms, all with the same statistical summaries (including visualizations!) but different true causal effects.\n\n\n\n\n\n\nApr 24, 2023\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nTransparency in Public Health\n\n\n\n\n\n\n\ntransparency\n\n\nstatistical communication\n\n\n\n\nTransparency in public health messaging matters. Hannah Mendoza and I looked at how providing transparent information about why a public health recommendation is being made can increase uptake in a randomized trial published today in Plos One.\n\n\n\n\n\n\nDec 14, 2022\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nMigrating from Hugo to Quarto\n\n\n\n\n\n\n\nrstats\n\n\nquarto\n\n\nblog\n\n\n\n\nWe have migrated our blog from Hugo to Quarto! Here are a few quick tips that made the transition a bit smoother.\n\n\n\n\n\n\nSep 19, 2022\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\ntipr: An R package for sensitivity analyses for unmeasured confounding\n\n\n\n\n\n\n\nrstats\n\n\n\n\nThe tipr R package has new updates!\n\n\n\n\n\n\nSep 8, 2022\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nThe Peril of Power when Prioritizing a Point Estimate\n\n\n\n\n\n\n\nCOVID-19\n\n\nnon-inferiority trials\n\n\nclinical trials\n\n\npower\n\n\n\n\nI recently noticed that the Pfizer immunobridging trials, presumably set up to demonstrate that their COVID-19 vaccines elicit the same antibody response in children as was seen in 16-25 year olds, for whom efficacy has previously been demonstrated, have a strange criteria for ‚Äúsuccess‚Äù.\n\n\n\n\n\n\nFeb 21, 2022\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nExploring the impacts of noninferiority trial thresholds\n\n\n\n\n\nThis post explores the impact of setting particular criteria for ‚Äòsuccess‚Äô in clinical trial designs, with a specific example from the recent vaccine immunobridging trials.\n\n\n\n\n\n\nFeb 20, 2022\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nWould seeing Spider-Man: No Way Home decrease COVID-19 Cases?\n\n\n\n\n\n\n\ncovid-19\n\n\nstatistics\n\n\ncausal inference\n\n\n\n\nIn SNL‚Äôs cold open last night, President Joe Biden suggested that the COVID-19 surge we are seeing in the US is due to people seeing Spider-Man: No Way Home. If people would just stop seeing this film, he argues, cases will go back down! Interesting hypothesis, let‚Äôs take a looksy at the data, shall we?\n\n\n\n\n\n\nJan 16, 2022\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nVaccine effectiveness and breakthrough cases\n\n\n\n\n\n\n\ncovid-19\n\n\nstatistics\n\n\n\n\nI‚Äôm seeing lots of confusion around the frequency of breakthrough cases and the effectiveness of vaccines (in fact, a recent interview I did resulted in a confusing headline on this topic!) so let‚Äôs dive in!\n\n\n\n\n\n\nAug 17, 2021\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nDenominators Matter\n\n\n\n\n\n\n\ncovid-19\n\n\nstatistics\n\n\n\n\nI‚Äôve seen a lot today about how effective the vaccines are; mistakes aside, lots of folks seem to be mixing up which denominators matter - good thing statisticians LOVE denominators\n\n\n\n\n\n\nJul 21, 2021\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nNYTimes Map How-to\n\n\n\n\n\n\n\nrstats\n\n\nnytimes\n\n\ncovid-19\n\n\n\n\nA quick how-to for a neat New York Times visualization, inspired by an IsoStat listserv conversation.\n\n\n\n\n\n\nApr 7, 2021\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nGrided: a web-app for building css-grid layouts\n\n\n\n\n\n\n\nCSS\n\n\nwebsite\n\n\n\n\nGrided is an app that lets you define CSS-Grid layouts in a simple GUI allowing you to see how your app will look while you define it.\n\n\n\n\n\n\nMar 1, 2021\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nHow to build a DIY Lightboard\n\n\n\n\n\n\n\ndiy\n\n\nlightboard\n\n\n\n\nLearn how to build a lightboard with 5 quick supplies for less than $100!\n\n\n\n\n\n\nOct 2, 2020\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nSo you want to learn R‚Ä¶\n\n\n\n\n\n\n\nrstats\n\n\n\n\nThere have been several twitter threads circulating with (free!) resources for learning R - I wanted to collect some of my favorites here.\n\n\n\n\n\n\nJul 2, 2020\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nSurvival Model Detective: Part 2\n\n\n\n\n\n\n\nrstats\n\n\ncovid-19\n\n\nsurvival analysis\n\n\ncompeting risks\n\n\n\n\nA paper by Grein et al.¬†was recently published in the New England Journal of Medicine examining a cohort of patients with COVID-19 who were treated with compassionate-use remdesivir. This paper had a flaw in it‚Äôs main statistical analysis. Let‚Äôs learn a bit about competing risks!\n\n\n\n\n\n\nMay 22, 2020\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nSurvival Model Detective: Part 1\n\n\n\n\n\n\n\nrstats\n\n\ncovid-19\n\n\nsurvival analysis\n\n\ncompeting risks\n\n\n\n\nA paper by Grein et al.¬†was recently published in the New England Journal of Medicine examining a cohort of patients with COVID-19 who were treated with compassionate-use remdesivir. This paper had a very cool figure - here‚Äôs how to recreate it in R!\n\n\n\n\n\n\nMay 21, 2020\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nGraph detective\n\n\n\n\n\n\n\nrstats\n\n\ncovid-19\n\n\ndata visualizations\n\n\n\n\nA plot has been floating around on Twitter from Georgia where the x-axis is all scampled. Let‚Äôs look into it and see if we can fix it!\n\n\n\n\n\n\nMay 17, 2020\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nThis one cool hack will‚Ä¶help you categorize Harry Potter characters!\n\n\n\n\n\n\n\nrstats\n\n\n\n\nInspired by the amazing Not So Standard Deviations, as usual, here is a fun way to categorize data using a left join instead of case_when or if/else statements!!!\n\n\n\n\n\n\nMay 16, 2020\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nFirst year as faculty\n\n\n\n\n\n\n\nhigher education\n\n\nthoughts\n\n\nwork-life harmony\n\n\n\n\nI just completed my first year as a faculty member - here is what I‚Äôve learned! I‚Äôll start by giving some context for where I am, what my university is like, etc. Then I‚Äôll describe four recommendations, summarized by: discover your institution‚Äôs culture, become a peer, find harmony, and build community!\n\n\n\n\n\n\nMay 15, 2020\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nMay the Fourth Be With You (#rstats style)\n\n\n\n\n\n\n\nrstats\n\n\nstar wars\n\n\nggplot2\n\n\ngganimate\n\n\n\n\nRafael Irizarry made a fabulous TIE fighter plot in R, Jake Thompson recreated it using ggplot2 and gganimate, I added some stars.\n\n\n\n\n\n\nMay 4, 2020\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nPrevalence of a disease plays an important role in your probability of having COVID-19 given you tested positive\n\n\n\n\n\n\n\nstatistics\n\n\nuncertainty\n\n\ncoronavirus\n\n\ncasual inference\n\n\nbayes theorem\n\n\n\n\nThe prevalence of a disease plays an important role in your probability of having it given you test positive.\n\n\n\n\n\n\nApr 13, 2020\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nBayes Theorem and the Probability of Having COVID-19\n\n\n\n\n\n\n\nstatistics\n\n\nuncertainty\n\n\ncoronavirus\n\n\ncasual inference\n\n\nbayes theorem\n\n\n\n\nI‚Äôve seen a few papers describing the characteristics of people who tested positive for SARS-CoV-2 and this is sometimes being interpreted as describing people with certain characteristic‚Äôs the probability of infection. Let‚Äôs talk about why that‚Äôs likely not true.\n\n\n\n\n\n\nApr 9, 2020\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nIHME Model Uncertainty: A quick explainer\n\n\n\n\n\n\n\nstatistics\n\n\nuncertainty\n\n\ncoronavirus\n\n\ncasual inference\n\n\n\n\nThere has been a lot of talk about the IHME Covid-19 projection model. Ellie Murray & I have a chat about it on Episode 10 of Casual Inference; here is a quick description of what is going on here with a focus on the uncertainty.\n\n\n\n\n\n\nApr 8, 2020\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nPulling co-authors for grant docs\n\n\n\n\n\n\n\nrstats\n\n\n\n\nI just submitted my first grant. It turns out you need tons of little things when you submit a grant (who knew!) and one of the little things is a list of all of the coauthors you‚Äôve published with in the past four years. Instead of tracking that down, I automated the process using R and then stuck the code here so I have it for next time!\n\n\n\n\n\n\nNov 14, 2019\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nPheWAS-ME, an app for exploration of multimorbidity patterns in PheWAS\n\n\n\n\n\n\n\nR\n\n\nshiny\n\n\njavascript\n\n\nd3\n\n\nEHR\n\n\nbiobank\n\n\n\n\nThis post is a longer-form and less-formal accompaniment to the manuscript ‚ÄúPheWAS-ME: A web-app for interactive exploration of multimorbidity patterns in PheWAS‚Äù and accompanying application. As the first of three papers that make up my PhD dissertation, the project represents a significant collaborative effort bringing together Electronic Health Records (EHR) and Biobank data using R and Shiny.\n\n\n\n\n\n\nOct 16, 2019\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nBuilding a data-driven CV with R\n\n\n\n\n\n\n\ncareer\n\n\nR\n\n\nRMarkdown\n\n\nCSS\n\n\n\n\nUpdating your CV or Resume can be a pain. It usually involves lots of copying and pasting along and then if you decide to tweak some style you may need to repeat the whole process. I decided to switch things up and design my CV so the format is just a wrapper around the underlying data. This post will help you do the same.\n\n\n\n\n\n\nSep 4, 2019\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nExtending the analogy: The boy who cried wolf was p-hacking!\n\n\n\n\n\n\n\n\n\n\nDuring my postdoc with Jeff Leek, we worked on a few p-value, study design, and p-hacking ‚Äúexplainers‚Äù. Two of these were incorporated into TED-Ed cartoons (The totally ironically named (NOT BY ME) This one weird trick will help you spot clickbait and the less ironic Can you spot the problem with these headlines?), but the analogy written about here was never used, so here it is!\n\n\n\n\n\n\nAug 24, 2019\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nUsing AWK and R to parse 25tb\n\n\n\n\n\n\n\nbig data\n\n\nawk\n\n\ndata cleaning\n\n\n\n\nRecently I was tasked with parsing 25tb of raw genotype data. This is the story of how I brought the query time and cost down from 8 minutes and $20 to a tenth of a second and less than a penny, plus the lessons learned along the way.\n\n\n\n\n\n\nJun 4, 2019\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nUnderstanding propensity score weighting\n\n\n\n\n\n\n\npropensity scores\n\n\ncausal inference\n\n\n\n\nCome enjoy a graphical exploration of various propensity score weighting schemes.\n\n\n\n\n\n\nJan 17, 2019\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nOne year to dissertate\n\n\n\n\n\n\n\nphd\n\n\none year dissertation\n\n\n\n\nI‚Äôve compiled some resources that I used when completing my dissertation and I wanted to share them with YOU! Throughout this post, I link to a bunch of different templates that I used throughout my process. You can find them all in a GitHub repo. This how-to has gotten a biiiiiit long. This post contains the whole kit-and-kaboodle, but I will also be releasing these in a series of smaller posts over the next couple of weeks.\n\n\n\n\n\n\nSep 14, 2018\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\np-value thoughts: A twitter follow up\n\n\n\n\n\n\n\nrstats\n\n\np-values\n\n\n\n\nA conversation about how ‚Äúconvincing‚Äù various studies were based on sample size and p-values led me to post a poll on twitter. Here I discuss some thoughts that came up based on these results. tl;dr: p-values are hard, twitter is a fun way to spur stats convos!\n\n\n\n\n\n\nAug 21, 2018\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nShinyviewr: camera input for shiny\n\n\n\n\n\n\n\nshiny\n\n\nimages\n\n\ndeeplearning\n\n\n\n\nA brief intro to, and tutorial for, the new function in the shinysense packages: shinyviewr. This function allows you to take photos using the camera on your computer or phone and directly send them into your shiny applications.\n\n\n\n\n\n\nJul 22, 2018\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nR release names (Updated)\n\n\n\n\n\n\n\nrstats\n\n\n\n\nI always love discussions about R release names and their origin. I have been working on this list for a while ‚Äì with the release of ‚ÄúShort Summer‚Äù today, I thought it‚Äôd be a good time to post!\n\n\n\n\n\n\nApr 23, 2018\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nnetwork3d - a 3D network visualization and layout library\n\n\n\n\n\n\n\nvisualization\n\n\nnetworks\n\n\n\n\nRecently, I have found myself needing to visualize networks. There are plenty of lovely options in R for visualizing networks in 2d, but I have found that many of the networks I want to visualize work much better when done in 3d and here the options are much smaller. This has prompted me to build the package network3d. This post will be a brief intro to using it.\n\n\n\n\n\n\nApr 9, 2018\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nThe United States of Seasons\n\n\n\n\n\n\n\nvisualization\n\n\nmaps\n\n\nclimate\n\n\n\n\nHow different is the warmest day from the coldest day all around the country? Using readings from 7,000+ NOAA weather stations across the country we can find out.\n\n\n\n\n\n\nFeb 12, 2018\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nWrangling Data Day Texas Slides\n\n\n\n\n\n\n\nrstats\n\n\n\n\nSince twitter threads are excessively cumbersome to navigate, Ma√´lle asked me to relocate the list of #rstats Data Day Texas slides to a blog post, so here we are!\n\n\n\n\n\n\nJan 28, 2018\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nA set.seed() + ggplot2 adventure\n\n\n\n\n\n\n\nrstats\n\n\nggplot2\n\n\n\n\nRecently I tweeted a small piece of advice re: when to set a seed in your script. Jenny pointed out that this may be blog post-worthy, so here we are!\n\n\n\n\n\n\nJan 22, 2018\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nA year as told by fitbit\n\n\n\n\n\n\n\nvisualization\n\n\nwearables\n\n\ntime series\n\n\n\n\nOf all of the important things that happened in 2017, probably the most impactful on the world is that I managed to wear a fitbit the entire year. Here I download my entire years worth of heart rate and step data to see what my 2017 looked like, in terms of heart beats and steps.\n\n\n\n\n\n\nDec 27, 2017\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nLeveraging uncertainty information from deep neural networks for disease detection - a summary\n\n\n\n\n\n\n\ndeep learning\n\n\nalgorithms\n\n\nuncertainty\n\n\nbayesian\n\n\n\n\nI was recently sent this fantastic paper on using uncertainty in deep neural networks. In it the authors demonstrate a practical use of approximate bayesian inference by dropout in the context of massively complicated computer vision models for diagnosing disease. The paper, while well written, is very long. Here I summarize it into its main points and comment on their impactfulness.\n\n\n\n\n\n\nDec 24, 2017\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nSecret Sampling\n\n\n\n\n\n\n\nrstats\n\n\nholiday cheer\n\n\n\n\n‚ÄôTis the season for white elephant / ◊í◊û◊ì ◊ï◊¢◊†◊ß / Yankee swap / secret santa-ing! We thought it‚Äôd be particularly fun to do it #rstats style.\n\n\n\n\n\n\nNov 15, 2017\n\n\nSarah Lotspeich and Lucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nThanksgiving Gantt Chart\n\n\n\n\n\n\n\nrstats\n\n\nthankyou\n\n\n\n\nThanksgiving r emo::ji(\"turkey\") is right around the corner r emo::ji(\"tada\") ‚Äì this year we are hosting 17 people r emo::ji(\"scream\"). If you too are hosting way more than your kitchen normally cooks for, perhaps this will be of interest!\n\n\n\n\n\n\nNov 12, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nLSTM neural nets as told by baseball\n\n\n\n\n\n\n\ndeep learning\n\n\nalgorithms\n\n\nbaseball\n\n\n\n\nOne thing I always found confusing when learning what an LSTM does is understanding intuitively why it‚Äôs doing what it does. Here I attempt to give an example of how a LSTM hidden layer can be thought of through baseball.\n\n\n\n\n\n\nNov 8, 2017\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nMCMC and the case of the spilled seeds\n\n\n\n\n\n\n\ninteractive\n\n\nalgorithms\n\n\nvisualization\n\n\n\n\nFor a long time I was confused by MCMC. I didn‚Äôt understand what it was, how it worked, and why we needed to do it. In this post I attempt to clear up those questions and allow you to play with the Metroplis Haystings algorithm as it attempts to find a posterior to help solve a mystery of two messy birds.\n\n\n\n\n\n\nOct 14, 2017\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nR release names\n\n\n\n\n\n\n\nrstats\n\n\n\n\nI always love discussions about R release names and their origin. I have been working on this list for a while ‚Äì with the release of ‚ÄúShort Summer‚Äù today, I thought it‚Äôd be a good time to post!\n\n\n\n\n\n\nSep 28, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nThe traveling metallurgist\n\n\n\n\n\n\n\ninteractive\n\n\nalgorithms\n\n\nvisualization\n\n\n\n\nHere I attempt to explain the concepts behind the optimization technique simulated annealing and the combinatorial optimization problem of the traveling salesman. First in words, and then more excitingly in an interactive visualization.\n\n\n\n\n\n\nSep 25, 2017\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nCommentary and follow up to p&lt;0.005 suggestion\n\n\n\n\n\n\n\n\n\n\nA recent paper, Redefine Statistical Significance by 72 co-authors, has caused quite a stir in the statistical community. Our student-run journal club at Vanderbilt will be discussing this contribution at our meeting led by Nathan James this week, so I‚Äôve attempted to create a list of significant responses/commentary that have come out since this paper was posted on PsyArXiv.\n\n\n\n\n\n\nSep 25, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nA Simple Slack Bot With Plumber\n\n\n\n\n\n\n\ncatslaps\n\n\ngifs\n\n\nslack\n\n\nplumber\n\n\napis\n\n\n\n\nI‚Äôve been excited about the R package Plumber ever since hearing about it for the first time as useR2017. So when I finally found an application that would allow me to use it, sending cat and dog photos over slack, I jumped at the opportunity.\n\n\n\n\n\n\nSep 3, 2017\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nThe Exponential Power Series\n\n\n\n\n\n\n\nstatistics\n\n\nvisualization\n\n\ninteractive\n\n\n\n\nI find series expansions fascinating. I also find any math envolving e to be fascinating. Here I explain some of the facets of the exponential power series and its connection to my favorite distribution, the Poisson.\n\n\n\n\n\n\nAug 14, 2017\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nWhy you maybe shouldn‚Äôt care about that p-value\n\n\n\n\n\n\n\np-values\n\n\nstatistics\n\n\nsushi-cat\n\n\ntaco-tuesday\n\n\n\n\nRecently, there seems to have been an uptick in citations of studies or statistics about this or that in the news and on the internet. Often these studies claim validity on the basis of a p-value. Through a small contrived example I make the point that in some situations we may want to ignore the forest and focus on the trees.\n\n\n\n\n\n\nAug 14, 2017\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nHow to make an R Markdown website (with RStudio!)\n\n\n\n\n\n\n\nrstats\n\n\nwebsite\n\n\ntutorial\n\n\n\n\nInterested in creating your personal website with R Markdown? We‚Äôve updated our R Markdown website tutorial to depend on RStudio for simplicity, making website building easy as üç∞!\n\n\n\n\n\n\nAug 8, 2017\n\n\nNick Strayer & Lucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nTwitter trees\n\n\n\n\n\n\n\nrstats\n\n\ntwitter\n\n\n\n\nA little over a week ago, Hilary Parker tweeted out a poll about sending calendar invites that generated quite the repartee. It was quite popular ‚Äì so much so that I couldn‚Äôt possible keep up with all of the replies! I personally am quite dependent on my calender, but I was intrigured to see what others had to say. This inspired me to try out some swanky R packages for visualizing trees.\n\n\n\n\n\n\nJul 24, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nThe making of ‚ÄúWe R-Ladies‚Äù\n\n\n\n\n\n\n\nrstats\n\n\nrladies\n\n\n\n\nMa√´lle and I created a mosaic of R-Ladies for the JSM Data Art Show. Here is a quick tutorial if you are interested in trying something similar!\n\n\n\n\n\n\nJul 18, 2017\n\n\nLucy D‚ÄôAgostino McGowan and Ma√´lle Salmon\n\n\n\n\n\n\n\n\nHappy World Emoji Day: an analysis of rOpenSci‚Äôs Slack emojis\n\n\n\n\n\n\n\nrOpenSci\n\n\nrstats\n\n\nemojis\n\n\n\n\nHAPPY world emoji day! In honor of this momentous occasion, I have decided to analyze the emojis used on rOpenSci‚Äôs Slack.\n\n\n\n\n\n\nJul 17, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nIntroducing the tuftesque blogdown theme\n\n\n\n\n\n\n\nrstats\n\n\n\n\nIf you like the way our blog looks, you too can have your own blogdown driven site just like it! In this post I walk through how to set up an RMarkdown driven blog from scratch using blogdown and the tuftesque theme constructed for Live Free Or Dichotomize.\n\n\n\n\n\n\nJul 13, 2017\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nuseR!2017 digressions\n\n\n\n\n\n\n\nrstats\n\n\nconferences\n\n\ntravel\n\n\n\n\nWe both recently attended useR!2017 in Brussels. It was a blast to say the least. Here we will cover our favorite things about things about the conference and the lessons we learned.\n\n\n\n\n\n\nJul 12, 2017\n\n\nNick Strayer & Lucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nrunconf17, an analysis of emoji use\n\n\n\n\n\n\n\nrOpenSci\n\n\nrstats\n\n\nconferences\n\n\nemojis\n\n\n\n\nI had such a delightful time at rOpenSci‚Äôs unconference. Not only was it extremely productive (21 packages were produced!), but in between the crazy productivity was some epic community building.\n\n\n\n\n\n\nJun 4, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nENAR in words\n\n\n\n\n\n\n\nENAR\n\n\ntidytext\n\n\nconferences\n\n\nrstats\n\n\n\n\nI had an absolutely delightful time at ENAR this year. Lots of talk about the intersection between data science & statistics, diversity, and great advancements in statistical methods. Since there was quite a bit of twitter action, I thought I‚Äôd do a quick tutorial in scraping twitter data in R.\n\n\n\n\n\n\nMar 16, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nIntroducing shinyswipr: swipe your way to a great Shiny UI\n\n\n\n\n\n\n\njavascript\n\n\nvisualization\n\n\n\n\nRecently we have been working on a shiny app that mimics tinder for preprints. One of the more exciting things we‚Äôve done in this app is implimented a swiping input. Now you can to with the package shinyswipr.\n\n\n\n\n\n\nMar 12, 2017\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nIntro to GMD\n\n\n\n\n\n\n\ncollaboration\n\n\nrstats\n\n\nGoogle Docs\n\n\n\n\nLucy and I have made a simple package that allows you to pull down a collaborative google doc directly into an RMD file on your computer. Hopefully speeding up the process of writing collaborative statistical documents.\n\n\n\n\n\n\nFeb 24, 2017\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nThe dire consequences of tests for linearity\n\n\n\n\n\n\n\nrstats\n\n\nrms\n\n\ntype 1 error\n\n\nnonlinearity\n\n\n\n\nThis is a tale of the dire (type 1 error) consequences that occur when you test for linearity üò±\n\n\n\n\n\n\nFeb 18, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nThe prevalence of drunk podcasts\n\n\n\n\n\n\n\nNSSD\n\n\nrstats\n\n\nemojis\n\n\n\n\nFor today‚Äôs rendition of I am curious about everything, in Hilary Parker & Roger Peng‚Äôs Not So Standard Deviations Episode 32, Roger suggested the prevalence of drunk podcasting has dramatically increased - so I thought I‚Äôd dig into it.\n\n\n\n\n\n\nFeb 9, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nYoga for modeling\n\n\n\n\n\nA New Year‚Äôs resolution for all of our models: get more flexible! By flexible, we mean let‚Äôs be more intential about fitting nonlinear parametric models.\n\n\n\n\n\n\nJan 27, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nCatterPlot thank you note\n\n\n\n\n\n\n\nthankyou\n\n\nrstats\n\n\nemojis\n\n\n\n\nLara Harmon has put in countless hours to build and uplift the ASA Student community. We are SO grateful.\n\n\n\n\n\n\nJan 25, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nCustom JavaScript visualizations in RMarkdown\n\n\n\n\n\n\n\njavascript\n\n\nvisualization\n\n\n\n\nRecently RStudio added JavaScript chunks to RMarkdown. This makes many exciting things possible. Among these things is making your own custom JavaScript visualizations of data managed in R, all without leaving the .Rmd document. This is a quick walkthrough of doing just that.\n\n\n\n\n\n\nJan 24, 2017\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nRegression modeling strategies: a student‚Äôs perspective\n\n\n\n\n\n\n\nrms\n\n\nrstats\n\n\n\n\nNick and I are starting a series following Frank Harrell‚Äôs Regression Modeling Strategies course. Get ready for some crazy fun.\n\n\n\n\n\n\nJan 18, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\ndplyr thank you note\n\n\n\n\n\n\n\nthankyou\n\n\nrstats\n\n\n\n\nIt‚Äôs that post-holiday time of year to write some thank yous! I‚Äôm getting excited to attend rstudio::conf next week, so in that spirit, I have put together a little thank you using dplyr\n\n\n\n\n\n\nJan 7, 2017\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\n\n\nWait, what are P-values?\n\n\n\n\n\nP-Values are annoying, let‚Äôs understand them so we dont get beaten by them.\n\n\n\n\n\n\nDec 24, 2016\n\n\nNick Strayer\n\n\n\n\n\n\n\n\nHill for the data scientist: an xkcd story\n\n\n\n\n\n\n\ndata-science\n\n\nepidemiology\n\n\nxkcd\n\n\nNSSD\n\n\n\n\nThis was inspired by Hilary Parker & Roger Peng‚Äôs Not So Standard Deviations Episode 28. It was suggested that it would be useful to lay out Hill‚Äôs criterion for data scientists, I agree!\n\n\n\n\n\n\nDec 15, 2016\n\n\nLucy D‚ÄôAgostino McGowan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2017-07-12-useR2017-digresssion/index.html",
    "href": "posts/2017-07-12-useR2017-digresssion/index.html",
    "title": "useR!2017 digressions",
    "section": "",
    "text": "We both recently attended useR!2017 in Brussels. It was a blast to say the least. We‚Äôre going to tag team to cover our favorite things & the lessons we learned while adventuring across the Atlantic."
  },
  {
    "objectID": "posts/2017-07-12-useR2017-digresssion/index.html#location",
    "href": "posts/2017-07-12-useR2017-digresssion/index.html#location",
    "title": "useR!2017 digressions",
    "section": "Location",
    "text": "Location\nLucy: Brussels was incredible. The r emo::ji(\"beer\") was delectable and the people were delightful.\n\n\nThis proved to be particularly useful when we temporally misplaced our travel buddy Jacquelyn on our way from the r emo::ji(\"airplane\") - we conveniently followed the signs to wifi!\nIt was quite simple to navigate, and as an added bonus the conference had set up lovely bread-crumb-esque signs from the train station to the main site at the Wild Gallery, making it even easier for the directionally challenged or GPS-less conference attendees.\n\n\n   Magritte, 1929, Les mots et les images, p.¬†32\nAs an added bonus, the Magritte Museum is in Brussels. After Aussie Nick rocked the conference‚Äôs closing remarks, we were able to pay homage to the original %&gt;% master himself. Magritte created some of my favorite surrealism pieces, but here we were also able to see some beautiful works from his brief Impressionist phase - they were incredibly charming! The Magritte Museum also houses Les Mots et Les Images, Magritte‚Äôs article in the surrealist journal La r√©volution surr√©aliste on words and images, a piece that speaks straight to my r emo::ji(\"heart\") given my present emoji-craze. r emo::ji(\"construction\") A potential blog post coming on this soon r emo::ji(\"construction\").\nNick: Brussels and Belgium are located about as conveniently in Europe as one could hope. Bordered by the Netherlands, Germany, France, and Luxembourg and just short train ride from all of them, one couldn‚Äôt ask for a better location. This centrality seemed to enable lots of people to attended that might not otherwise have. Due to this, and the fact that all other countries seem to have realized Americans won‚Äôt learn their languages and thus have learned English, I was able to interact with people from all over the globe. Sitting at dinner having a chat with someone from the Netherlands on one side, France on another and across from an Aussie is a pretty nice experience. While this may be obvious, it is easy to get subsumed into your own little bubble at your institution. Hearing about the amazing work being done by others elsewhere, motivated by their own unique cultural or environmental issues, was great for my sheltered Tennessee mind and inspiring.\nSome may be annoyed by the fact that the conference won‚Äôt be back in the US next year but I personally think this is beneficial.\n\n\nBesides, Nick Tierney may have told me that conference transportation next year will be provided by Kangaroos and Ostriches, so like, that‚Äôs worth it.\nAs Angela Merkel has similarly expressed recently, the US is slipping a bit in its hold on the ‚Äúgo-to‚Äù place.\n\n\n\n  \n\n\nThis big network diagram had champagne at the bottom, so that was cool."
  },
  {
    "objectID": "posts/2017-07-12-useR2017-digresssion/index.html#favorite-talks",
    "href": "posts/2017-07-12-useR2017-digresssion/index.html#favorite-talks",
    "title": "useR!2017 digressions",
    "section": "Favorite Talks",
    "text": "Favorite Talks\nNick:\n\n\nWe both gave talks, Lucy spoke about a shiny app we created called papr and Nick spoke about a new package for shiny development called shinysense.\nI have never been to a conference where almost every single talk I went to was as high-quality and fantastic as useR. I truly learned something at every single one and can‚Äôt wait to apply the concepts to my own R and statistics related projects. Obviously I can‚Äôt write up a thing on 30 different talks so I‚Äôll include a randomly selected few that stuck out to me.\n\nshiny.collections: Google Docs-like live collaboration in Shiny\nby Marek Rogala:\nShiny is fantastic for getting people with no web programming experience up and running with web-apps in minutes, but because of that often some of the more advanced features of web-apps such as reactive database connections can be nearly impossible to add (even for people like me with more web experience). The shiny.collections package addresses this by making it amazingly easy to spin up an app with an auto-updating database back-end. Believe me when I say this is not easy to do in a traditional web programming setup. This is fantastically exciting because the tool is easy enough to use for beginners but legitimately very helpful for all skill ranges. The number of applications I have dreamed up to use with this is vast. In addition, in a testament to his bravery and also the crazy ease of using the package, Marek did a live coding demo while on stage in front of more than 100 people! Having just spoken on that stage prior I can say he is much braver than I.\n\n\nCan you keep a secret?\nby Andrie de Vries:\nThe second you start working with other people on any project that involves sensitive information (api-keys etc) you realize the limitations of a public git-based workflow. You can‚Äôt commit all_my_sensitive_info.txt to the repo (although that certainly hasn‚Äôt stopped me) so you end up having to engage in an intricate and delicate dance of figuring out how to .gitignore the right files and also share their format between all your collaborators.\n\n\nAgain a live coding demo was performed showing just how easy it is to use. Apparently useRs are abnormally brave people.\nEssentially it‚Äôs miserable and I don‚Äôt recommend it. secret is another mind-blowingly easy to use R package for committing this info straight to your public repo while keeping is secure via encryption. You just need to gather public ssh keys from your collaborators. Then once the collaborator clones the repo they just need to run the code like normal and it works, no dangerous .gitignore dance to be performed, especially if you do something like change a secret. It can not be stressed helpful this will be in my future workflow.\n\n\nbsplus: Using Twitter Bootstrap to extend your Shiny app\nby Ian Lyttle:\n‚ÄúThe user benefits from me doing as little as possible‚Äù was a quote Ian emphasized in his talk, but I think this is incorrect. Perhaps a more accurate statement would be ‚Äúthe user benefits from me knowing how to take the best parts of bootstrap and port them in an easy-to-use and efficient way to R‚Äù. bsplus is a series of lightweight wrappers around the firmly established web-styling framework bootstrap by Twitter. A great UI for shiny or rmarkdown reports is not only a way to look professional but also important in encouraging new users to the platform. If you can make a shiny app in twenty minutes that takes an experienced web-developer a week to implement you‚Äôre making a pretty darn strong case for the adoption of shiny into your workflow. bsplus is a fantastic contribution to continuing shiny‚Äôs charge forward as the go-to way to make professional quality apps fast and easy.\nLucy: Nick and I shared many favorites! A few others that warrant mentioning:\n\n\nHow we built a Shiny App for 700 users?\nby Olga Mierzwa-Sulima\nOlga walked through a Shiny application she built for a client that has 700+ daily users! She highlighted that the prototype for this app was delivered in just one day, with a working demo deployed in 2 weeks - this is one of my favorite things about Shiny - the ability to get something in the r emo::ji(\"open_hands\") of a collaborator quickly! She demonstrated the shiny.semantic package - a semantic UI wrapper for Shiny. It creates simply beautiful output. Between this and shiny.connections, the Appsilon team are creating awesome Shiny resources.\n\n\nR-Ladies Global Community\nby Alice Daish and Hannah Frick\n\n\n   VERY pleased that #rchickenladies is out in the wild.\nThe R-Ladies Global team did an excellent job presenting the current state of this organization. Beginning in 2012 in San Francisco with Gabby, we‚Äôve grown to:\nr emo::ji(\"globe\") 20+ countries\nr emo::ji(\"cityscape\") 45+ cities\nr emo::ji(\"dancing_women\") 6000+ members\nr emo::ji(\"globe_with_meridians\") a website rladies.org\nr emo::ji(\"speaking_head\") a repository for #RLadies speakers\nIt was such a delight to be able to meet so many women I‚Äôve collaborated online with!\n\n\nAMAZING #RLadies at #useR2017 üíúüåç inspiring #rstats work around the world pic.twitter.com/pIPEorlkyl\n\n‚Äî R-Ladies Global (@RLadiesGlobal) July 5, 2017\n\n\nOkay, I had to do an update after the recordings were released to briefly highlight a few more exquisite talks we missed!\n\n\nThe R6 Class System\nby Winston Chang\n\n\nonly 508 lines of code!\nWinston gave a fantastic talk on R6, a lightweight package that allows for encapsulated/classical object-oriented programming in R. This is something I am quite interested in learning more about, so it was delightful to hear him explain it!\n\n\nFFTrees: An R package to create, visualise and use fast and frugal decision trees\nby Nathaniel Phillips\nThis talk goes a bit against the name of our blog, but I am quite interested in decision theory and this talk was very well delivered! My favorite part was the final slide - a tree that describes how you should decide whether a fast and frugal tree is good for your problem domain.\n\n\n\n  \n\n\nv meta - a decision tree about when to use a decision tree.\n\n\n\n\n\n\n\nR in Minecraft\nby David Smith\nI would be remiss not to mention the Minecraft talk - complete with slides built in Minecraft! David & crew spent the rOpenSci unconf building this awesome package and associated book to help teach kids how to use R."
  },
  {
    "objectID": "posts/2017-07-12-useR2017-digresssion/index.html#take-away",
    "href": "posts/2017-07-12-useR2017-digresssion/index.html#take-away",
    "title": "useR!2017 digressions",
    "section": "Take Away",
    "text": "Take Away\nLucy: It is always so fun to to get together in person with the visionaries of this language I love! Not only is it an incredible opportunity to learn through the presentations, but also through the small conversations over drinks in the evenings, or over r emo::ji(\"coffee\") during the breaks, or on the r emo::ji(\"train\") to a fun destination. Aside from the planned content, I learned about things such as Miles‚Äô initiative to start a community-wide discussion about interactive visualizations, Jim‚Äôs suggestions on some better git aliases, and that Ian knows every word to R.E.M‚Äôs It‚Äôs the End of the World as We Know It. It is seriously such a special experience to all be in the same place, even if just for a few days.\nNick: This was my first R related conference and I can with 100% certainty say it will not be my last. I have had my feet dipped in both the R and Javascript communities for a while now, while never fully diving into either. Now I feel much more comfortable diving full on into the R community. UseR helped humanize all of the people I follow on twitter/ other places and in doing so was an overwhelmingly positive experience. I have never seen a community more uniformly kind, thoughtful and humble. No one blew off the obscure graduate student coming up to talk to them about their work, they were all willing to provide advice or just have philosophical talks about statistics and R‚Äôs role in the world. This certainly is correlated with the opensource nature of the language, but adjusting for this in my mental model still leaves a good proportion of variation to be explained by whatever latent variable I have not yet thought of. Seeing the selfless work that these people do and the impacts it has had on the world of data-science, statistics and scientific communication is inspiring and I believe endows R with a non-tangible but powerful advantage over many other languages."
  },
  {
    "objectID": "posts/2017-07-12-useR2017-digresssion/index.html#non-conference-fun",
    "href": "posts/2017-07-12-useR2017-digresssion/index.html#non-conference-fun",
    "title": "useR!2017 digressions",
    "section": "Non-Conference Fun",
    "text": "Non-Conference Fun\nNick: As we found out quickly, one does not simply go to Brussels without taking a trip to Bruges. While at the train station Lucy, Jacquelyn, Nick Tierney and I were surprised by Ian Lyttle randomly gracing our presence having (lucky for us) just missed the earlier train. We all loaded into the clean smooth train and set off to Bruges. While there I set aside all desire to remain ‚Äúcool‚Äù and went full tourist mode and it was fantastic.\n\n\n\n \n\n\nA typical view down the street in Bruges.\n\n\n\n\n\n\n\n \n\nCover photo for our upcoming album \"Last Train to Brussels\"\n\n\n\n\n\n\n \n\n\nI mean, it‚Äôs Belgium.\n\n\n\n\n\n\nLucy: After Bruges, we ventured off to Iceland r emo::ji(\"iceland\") for a few days. It was absolutely magical. Icelandair has a great stopover program where you can book a flight with an extended stopover for the same cost as an ordinary layover ‚Äì it was marvelous ‚Äì this may become a new tradition for my European travel.\n\n\n\n \n\n\nGlacier SELFIE!\n\n\n\n\n\n\n\n \n\n\nIceland, where the (double) rainbow starts and ends."
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html",
    "href": "posts/2019-09-04-data-driven-cv/index.html",
    "title": "Building a data-driven CV with R",
    "section": "",
    "text": "A confession: I always put off updating my CV and Resume. Every time I go to update it, I end up deciding that I don‚Äôt like the style and then start from scratch. Upon starting over, I need to manually extract all relevant information for each position and conform it to my new layout. I don‚Äôt have a long CV yet, but even now, this takes much too long.\nA programmer would look at a problem like this and say, ‚Äúany scenario where I repeat the same actions I should write a script.‚Äù Since I self-identify as a ‚Äúdata scientist‚Äù - an unholy mashup of programmer and statistician - I figured I could write a script to do the formatting and store all my information in‚Ä¶ spreadsheets! ‚äïIn a hurry and just want to know how you can build your CV or resume this way? Skip to here\nThe goal of this post is to walk you through the techniques I used and help you do the same.\nFirst, let‚Äôs talk about the framework that allows this."
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#rmarkdown-pagedown",
    "href": "posts/2019-09-04-data-driven-cv/index.html#rmarkdown-pagedown",
    "title": "Building a data-driven CV with R",
    "section": "RMarkdown & Pagedown",
    "text": "RMarkdown & Pagedown\n‚äïRMarkdown is just one of a number of tools that allow you to use the literate programming paradigm, invented/popularized by the computer scientist Donald Knuth. If you are familiar with the world of R programming, you will likely have heard of Rmarkdown. RMarkdown is a file-format/package that takes a standard markdown document and allows the code chunks within to be executed and their outputs recorded.\nPagedown is a new R package that formats the final output from your Rmarkdown into paginated HTML. ‚äïI had the pleasure of sitting in the very back of a packed conference hall at RStudio::conf 2019 when Yihui Xie introduced Pagedown. You can watch the presentation from a much better vantage point than I had on the RStudioConf presentations site. This HTML can then be easily turned into a PDF by using the print function in your browser. Previously, if you wanted to generate a PDF from RMarkdown, you had two options. You could just print unpaginated HTML (ugly), or you could compile to Latex, which meant you losing out on a lot of niceties that are provided by CSS formatting. Personally, I know a lot more about formatting with CSS than I do with Latex, so this was a perfect fit for me.\n‚äïIt‚Äôs beautiful \nIncluded in the Pagedown package is a template called ‚ÄòResume.‚Äô ‚äïThe result of the un-touched resume template:  This template generates a beautiful resume with the main content section on the left and a sidebar for supplementary information on the right. The main section displays your positions in an elegant timeline format.\nTo add an entry for a position, you write markdown like so‚Ä¶\n### PhD. Candidate, Biostatistics\n\nVanderbilt University\n\nNashville, TN\n\n2011-2015\n\n- Working on Bayesian network models & interactive visualization platforms\n- University Graduate Fellow"
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#resultsasis",
    "href": "posts/2019-09-04-data-driven-cv/index.html#resultsasis",
    "title": "Building a data-driven CV with R",
    "section": "results=‚Äòasis‚Äô",
    "text": "results=‚Äòasis‚Äô\n‚äïYou can set this options for all chunks with knitr::opts_chunk$set(results=‚Äòasis‚Äô) in the first chunk of the RMD.\nIn RMarkdown, the output from code chunks is typically used to print tables or plots. However, by using the chunk command results = 'asis', we can also generate text that looks the same to pandoc as if we typed it ourselves.\nThis means that we can use R to write our document for us. ‚äïI have been doing this for a while with my personal website. My ‚Äòprojects‚Äô section is stored in a JSON file that gets rendered to HTML by some simple R functions. The trouble is, how do we write the code do to this?"
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#held-together-by-glue",
    "href": "posts/2019-09-04-data-driven-cv/index.html#held-together-by-glue",
    "title": "Building a data-driven CV with R",
    "section": "Held together by glue",
    "text": "Held together by glue\nUp until recently, if you wanted to generate long text output with R from multiple variables, you had to use the paste function. ‚äïTechnically the sprintf() function was an option as well, but that was just annoying. As its name implies, paste() paste‚Äôs its arguments together into a character string. It‚Äôs great if you want to join two or so values together, but when you start writing longer, more complicated output, it gets a bit hairy.\n\n\nMaking a CV/ Resume is something I always put off. But thanks to the {pagedown}, {purrr}, and {glue} packages I can store my positions in lists and build the output programatically. This way I tell myself I'm doing 'data-aggregation' and 'presentation' for my career. #rstats pic.twitter.com/VDgMAqWmQ8\n\n‚Äî Nick Strayer (@NicholasStrayer) September 26, 2019\n\n\nThis sticking point was one of the inspirations for the package glue, by the brilliant Jim Hester. Glue works by allowing you to write your output string in a natural template format. You tell R where you want each variable placed by writing the variable name wrapped in curly braces in the desired position in your template. Then when glue() evaluates, it pastes the values of each variable into its respective curly-braced location. ‚äïYou can change what character is used to wrap variables with the arguments .open and .close. In case you want a curly brace in your output.\nThis means we can write the markdown for a resume position by encoding all the necessary details about the position in variables and passing those variables to glue()‚Ä¶\n\ntitle &lt;- \"PhD. Candidate, Biostatistics\"\nloc &lt;- \"Nashville, TN\"\ninstitution &lt;- \"Vanderbilt University\"\ntimeline &lt;- \"2011-2015\"\ndescription_bullets &lt;- \"- Working on Bayesian network models & interactive visualization platforms\n- University Graduate Fellow\n\"\n\nglue(\n  \"### {title}\",\n  \"\\n\\n\",   # Add two new lines\n  \"{loc}\",\n  \"\\n\\n\",\n  \"{institution}\",\n  \"\\n\\n\",\n  \"{timeline}\", \n  \"\\n\\n\",\n  \"{description_bullets}\"\n)\n\n### PhD. Candidate, Biostatistics\n\nNashville, TN\n\nVanderbilt University\n\n2011-2015\n\n- Working on Bayesian network models & interactive visualization platforms\n- University Graduate Fellow"
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#bring-in-the-spreadsheets",
    "href": "posts/2019-09-04-data-driven-cv/index.html#bring-in-the-spreadsheets",
    "title": "Building a data-driven CV with R",
    "section": "Bring in the spreadsheets!",
    "text": "Bring in the spreadsheets!\nSo we have a pagedown template, and we know how to turn variables into an entry. Now we need to decide how to store those variables for all the positions.\n‚äïA little secret: however, much data scientists love to talk about database systems, scraping frameworks, or distributed file stores, we still all wish everything came in CSVs. I decided to do one final copy and paste of my positions into a CSV.\n\nFor each ‚Äòposition,‚Äô I had columns describing the primary details: title, loc(ation), institution, start (date), and end (date).\nI also added a column for the section the entry fell into. For me this consists of 'education', 'research_positions', 'industry_positions', 'data_science_writings', etc..\nLast, I needed a way to store the description for the positions. This was a tiny bit tricky as I didn‚Äôt always want the same number of description bullets. Some positions just get one, some may have three‚Ä¶"
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#enter-tidyrpivot_longer",
    "href": "posts/2019-09-04-data-driven-cv/index.html#enter-tidyrpivot_longer",
    "title": "Building a data-driven CV with R",
    "section": "Enter tidyr::pivot_longer()",
    "text": "Enter tidyr::pivot_longer()\nTo deal with this, I reached into the shiny new tidyr V1 functions toolbox for the function pivot_longer.\n\n\nü§¶ ‚ôÄÔ∏è this worked much better in my imagination‚Ä¶üò¨ tidyr::pivot_longer()-ing/* for actual good #rstats animations, see @grrrck's tidyexplain repo https://t.co/CL9IEuUlXe */ pic.twitter.com/ohgGXlyCOA\n\n‚Äî Mara Averick (@dataandme) September 20, 2019\n\n\nI made multiple columns for description and creatively titled them description_1, description_2, and description_3. ‚äïIf I had an entry with four description bullet points, I would have just added a description_4, and so on.I filled in the columns for each entry until all bullets had been pasted and then left the remaining columns empty.\nFor the rest of this post, I will use the actual position dataframe that my CV is using to demonstrate, it‚Äôs loaded currently as positions.\n\n# Helper function to print table without overwhelming output\nprint_head &lt;- function(data, size = 5){\n  head(data, size) %&gt;% \n    knitr::kable()\n}\n\n# Add an id to keep track of each entry\nposition_data &lt;- positions %&gt;% \n  mutate(id = 1:n()) \n\nposition_data %&gt;% \n  select(\n    id, \n    description_1, \n    description_2, \n    description_3 ) %&gt;%\n  print_head()\n\n\n\n\n\n\n\n\n\n\nid\ndescription_1\ndescription_2\ndescription_3\n\n\n\n\n1\nWorking on Bayesian network models & interactive visualization platforms\nUniversity Graduate Fellow\nNA\n\n\n2\nThesis: An agent based model of Diel Vertical Migration patterns of Mysis diluviana\nNA\nNA\n\n\n3\nIndependently analyzed and constructed statistical models for large data sets pertaining to carbon decomposition rates.\nNA\nNA\n\n\n4\nDeveloped mathematical model to predict the transport of sulfur through the environment with applications in waste cleanup.\nNA\nNA\n\n\n5\nAnalyzed and visualized data for CATOS fish tracking project.\nHead of data mining project to establish temporal trends in population densities of Mysis diluviana (Mysis).\nRan project to mathematically model the migration patterns of Mysis (honors thesis project.)\n\n\n\n\n\nUsing pivot_longer, I targeted the description columns using tidyr::starts_with('description') and transformed each individual position row into a series of rows with each description stored as 'description' and its value as 'description_num'. All the other columns were simply repeated.\n\npivoted_positions &lt;- position_data %&gt;% \n  pivot_longer(\n     starts_with('description'),\n     names_to = 'description_num',\n     values_to = 'description',\n     values_drop_na = TRUE\n   )\n\npivoted_positions %&gt;% \n  select(\n    title, \n    description_num, \n    description) %&gt;% \n  print_head()\n\n\n\n\n\n\n\n\n\ntitle\ndescription_num\ndescription\n\n\n\n\nPhD. Candidate, Biostatistics\ndescription_1\nWorking on Bayesian network models & interactive visualization platforms\n\n\nPhD. Candidate, Biostatistics\ndescription_2\nUniversity Graduate Fellow\n\n\nB.S., Mathematics, Statistics (minor C.S.)\ndescription_1\nThesis: An agent based model of Diel Vertical Migration patterns of Mysis diluviana\n\n\nResearch Assistant\ndescription_1\nIndependently analyzed and constructed statistical models for large data sets pertaining to carbon decomposition rates.\n\n\nUndergraduate Researcher\ndescription_1\nDeveloped mathematical model to predict the transport of sulfur through the environment with applications in waste cleanup.\n\n\n\n\n\nOnce this was completed, I got back to a single row per entry by using group_by() and mutate() to generate a list column that contained every description for a given position. Finally, I just kept the first row for each entry. ‚äïI am almost sure there‚Äôs a more elegant way to do this whole step, but I couldn‚Äôt figure it out. Know a better way? Please let me know!\n\npos_w_descrip_list &lt;- pivoted_positions %&gt;% \n  group_by(id) %&gt;% \n  # Wrap all descriptions into a list column\n  mutate(descriptions = list(description) ) %&gt;% \n  ungroup() %&gt;% \n  # Only keep first row of each expanded position rows\n  filter(description_num == 'description_1') %&gt;% \n  # We don't need these columns any more\n  select(-description_num, -description)\n\npos_w_descrip_list %&gt;% \n  select(id, title, descriptions) %&gt;% \n  print_head()\n\n\n\n\n\n\n\n\n\nid\ntitle\ndescriptions\n\n\n\n\n1\nPhD. Candidate, Biostatistics\nWorking on Bayesian network models & interactive visualization platforms, University Graduate Fellow\n\n\n2\nB.S., Mathematics, Statistics (minor C.S.)\nThesis: An agent based model of Diel Vertical Migration patterns of Mysis diluviana\n\n\n3\nResearch Assistant\nIndependently analyzed and constructed statistical models for large data sets pertaining to carbon decomposition rates.\n\n\n4\nUndergraduate Researcher\nDeveloped mathematical model to predict the transport of sulfur through the environment with applications in waste cleanup.\n\n\n5\nUndergraduate Researcher\nAnalyzed and visualized data for CATOS fish tracking project. , Head of data mining project to establish temporal trends in population densities of Mysis diluviana (Mysis)., Ran project to mathematically model the migration patterns of Mysis (honors thesis project.)\n\n\n\n\n\nNow I had the data in a nice tibble with all the information I need for a given position in the row in a format I can run through glue."
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#glue_data-to-the-rescue",
    "href": "posts/2019-09-04-data-driven-cv/index.html#glue_data-to-the-rescue",
    "title": "Building a data-driven CV with R",
    "section": "glue_data to the rescue",
    "text": "glue_data to the rescue\nglue::glue_data() is a sister-function to glue::glue() that allows you to use a glue template in a dplyr pipe. You write out the column name in your curly braces, and glue runs row-by-row through your tibble, building a string for each.\nBefore I could do this, I needed to take care of a few things. One was the dates. Some positions didn‚Äôt have both start and end date, or they were less than a year-long. In these scenarios, I just wanted to print a single date rather than a range. To do this, I used a mutate and an ifelse to build a timeline column.\n\n# If missing start or start is same as end\n# date, just use end date. otw build range\npositions_w_timeline &lt;- pos_w_descrip_list %&gt;% \n  mutate(\n    timeline = ifelse(\n      is.na(start) | start == end,\n      end,\n      glue('{end} - {start}')\n    )\n  )\n\npositions_w_timeline %&gt;% \n  select(title, timeline) %&gt;% \n  print_head(8)\n\n\n\n\ntitle\ntimeline\n\n\n\n\nPhD. Candidate, Biostatistics\n2020 - 2015\n\n\nB.S., Mathematics, Statistics (minor C.S.)\n2015 - 2011\n\n\nResearch Assistant\n2013 - 2012\n\n\nUndergraduate Researcher\n2014 - 2013\n\n\nUndergraduate Researcher\n2015 - 2013\n\n\nHuman Computer Interaction Researcher\n2015\n\n\nGraduate Research Assistant\n2020 - 2015\n\n\nData Science Researcher\n2018 - 2017\n\n\n\n\n\nSecond, I needed to collapse my list column of descriptions into the one-bullet-per-line format that the Pagedown resume template uses. I used purrr::map_chr and the paste function.\n‚äïEvery time I write a quoted dash character, I always feel like it‚Äôs a non-plussed face that is judging me: ‚Äò-‚Äô\n\npositions_collapsed_bullets &lt;- positions_w_timeline %&gt;% \n  mutate(\n     description_bullets = map_chr(\n       descriptions, \n       ~paste('-', ., collapse = '\\n')),\n  )\n\npositions_collapsed_bullets %&gt;% \n  pull(description_bullets) %&gt;% \n  head(3)\n\n[1] \"- Working on Bayesian network models & interactive visualization platforms\\n- University Graduate Fellow\"                 \n[2] \"- Thesis: An agent based model of Diel Vertical Migration patterns of Mysis diluviana\"                                    \n[3] \"- Independently analyzed and constructed statistical models for large data sets pertaining to carbon decomposition rates.\"\n\n\nLast, the template will ignore parts of an entry if it is passed as 'N/A'. Unfortunately, printing an na value in R returns 'na', so I used mutate_all() to turn every missing value in the dataframe into the string 'N/A'.\n\npositions_no_na &lt;- positions_collapsed_bullets %&gt;% \n  mutate_all(~ifelse(is.na(.), 'N/A', .))\n\nAfter all that, we just plop our glue template into glue_data and pipe in our newly modified positions dataframe.\n\npositions_no_na %&gt;% \n  head(2) %&gt;% \n  glue_data(\n   \"### {title}\",\n   \"\\n\\n\",\n   \"{loc}\",\n   \"\\n\\n\",\n   \"{institution}\",\n   \"\\n\\n\",\n   \"{timeline}\", \n   \"\\n\\n\",\n   \"{description_bullets}\",\n   \"\\n\\n\\n\"\n )\n\n### PhD. Candidate, Biostatistics\n\nVanderbilt University\n\nNashville, TN\n\n2020 - 2015\n\n- Working on Bayesian network models & interactive visualization platforms\n- University Graduate Fellow\n\n\n### B.S., Mathematics, Statistics (minor C.S.)\n\nUniversity of Vermont\n\nBurlington, VT\n\n2015 - 2011\n\n- Thesis: An agent based model of Diel Vertical Migration patterns of Mysis diluviana\n\n\nI wrapped all this code in a function that takes a position‚Äôs dataframe and the name of the section I want it to print and then runs all the transformations above and prints.\nHere‚Äôs what the final function looks like. ‚äïYou may notice a function strip_links_from_cols() in this that I haven‚Äôt mentioned yet. We will get to that!\n\n\n\n\n\nNow I can just go through and draft my resume by dropping in an R chunk with output asis. If I end up wanting to change the layout in the future, I just need to rearrange my glue template."
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#building-a-resume",
    "href": "posts/2019-09-04-data-driven-cv/index.html#building-a-resume",
    "title": "Building a data-driven CV with R",
    "section": "Building a resume",
    "text": "Building a resume\nI used this new-layout flexibility almost immediately. As some people doing hiring may not want - or have time - to scan through multiple pages of a CV, I built a template for a single-page resume. All this took was adding a single column to the spreadsheet that contained a boolean indicating if I wanted the position in my resume or not.\n‚äïThe fact I had an extra curly brace to start {pagedown} in this tweet has caused me large quantities of distress.\n\n\nAdded a 1 page resume to my #rstats {{pagedown}-built CV repo. To filter what gets into resume I just modify a column in my positions spreadsheet. No more starting from scratch when I want a new template! gh: https://t.co/NYJNg7daj7 pic.twitter.com/J0JQJz5Xpb\n\n‚Äî Nick Strayer (@NicholasStrayer) September 30, 2019"
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#dealing-with-links",
    "href": "posts/2019-09-04-data-driven-cv/index.html#dealing-with-links",
    "title": "Building a data-driven CV with R",
    "section": "Dealing with links",
    "text": "Dealing with links\nOne of the beautiful things about HTML is that you can interact with it. The most common type of interaction is clicking on links. In my positions, I liberally added links to things for people to click on. This works great when people view my CV on the web, but what if someone wants to print it out and read it in real life? After all, pagedown is paginated for precisely that purpose.\nIn the standard pagedown format, there is a YAML header option (links-to-footnotes: true) ‚äïFor all the available options, check out the excellent docs for pagedown. that replaces every link with a footnote and then adds a superscript to the link text guiding the reader to the correct reference. This is a great solution. Unfortunately, the option isn‚Äôt available for the resume template.\n‚äïThere is a pandoc processing function for parsing links that is passed to the standard paged format but is not passed to the Resume template. I tried to add it by modifying the Pagedown package itself could never get it to respect the sidebar layout. If I had more time, I would like to fix this at the package-level to make it more elegant.\nSince I liked the resume template, I decided to roll my own solution.\nI wrote a function sanitize_links() that takes a chunk of text as input and identifies the markdown links in it with a regular expression. The function then replaces the markdown link with plain text and adds a sequential superscript. Each link it replaces is placed in a bookkeeping array: links. At the very end of the document, links is printed out as a numbered list.\n‚äïI just found out you can write comments in your regular expressions, and it makes me very happy.\n\nlibrary(stringr)\n# Regex to locate links in text\nfind_link &lt;- regex(\"\n  \\\\[   # Grab opening square bracket\n  .+?   # Find smallest internal text as possible\n  \\\\]   # Closing square bracket\n  \\\\(   # Opening parenthesis\n  .+?   # Link text, again as small as possible\n  \\\\)   # Closing parenthesis\n  \",\n comments = TRUE)\n\n# Function that removes links from text and replaces them with superscripts that are \n# referenced in an end-of-document list. \nsanitize_links &lt;- function(text){\n  str_extract_all(text, find_link) %&gt;% \n    pluck(1) %&gt;% \n    walk(function(link_from_text){\n      title &lt;- link_from_text %&gt;% \n        str_extract('\\\\[.+\\\\]') %&gt;% \n        str_remove_all('\\\\[|\\\\]') \n      \n      link &lt;- link_from_text %&gt;% \n        str_extract('\\\\(.+\\\\)') %&gt;% \n        str_remove_all('\\\\(|\\\\)')\n      \n      # add link to links array\n      links &lt;&lt;- c(links, link)\n      \n      # Build replacement text\n      new_text &lt;- glue('{title}&lt;sup&gt;{length(links)}&lt;/sup&gt;')\n      \n      # Replace text\n      text &lt;&lt;- text %&gt;% \n        str_replace(fixed(link_from_text), new_text)\n    })\n  \n  text\n}\n\nTo demonstrate how this works, let‚Äôs ‚Äòsanitize‚Äô some sample text with two links in it.\n\nlinks &lt;- c() \ntext_w_links &lt;- \"This is some [text](www.with_links.com). It would be nice if it was just [plain](www.and_without_links.com).\"\n\ntext_wo_links &lt;- text_w_links %&gt;% sanitize_links()\n\ntext_wo_links %&gt;% print()\n\n[1] \"This is some text&lt;sup&gt;1&lt;/sup&gt;. It would be nice if it was just plain&lt;sup&gt;2&lt;/sup&gt;.\"\n\nlinks %&gt;% print()\n\n[1] \"www.with_links.com\"        \"www.and_without_links.com\"\n\n\nNotice the use of the &lt;&lt;- assignment for links in the function. ‚äïUsually, I am very against mutating variables, especially out of scope, but this seemed like an acceptable use of the technique since its just a static text document and the code isn‚Äôt running live somewhere. This is because by default, the link array that the function is appending to will only get modified within the function scope, and thus we won‚Äôt have the output for later. &lt;&lt;- lets R know that it should modify the variable reference outside of the current scope.\nI wrapped sanitize_links() in another function that returns the positions tibble with the links all sanitized in order of their appearance on the page.\n\n# Take entire positions section and removes the links \nstrip_links_from_cols &lt;- function(data, cols_to_strip){\n  for(i in 1:nrow(data)){\n    # Remove by position so numbers stay together\n    for(col in cols_to_strip){\n      data[i, col] &lt;- sanitize_links(data[i, col])\n    }\n  }\n  # Give back now sanitized position data\n  data\n}\n\nI wrapped links in the free text section in sanitize_links() so they would get stripped as well. This means that the links sequentially decrease as the page goes down. Is this actually important? Probably not.\nNow when I want to generate the CV for printing to PDF, I just flip a boolean PDF_EXPORT at the top of the RMD, and all the links are stripped out.\n\nsanitize_links &lt;- function(text){\n  # Only do stuff to text if we're exporting PDF\n  if(PDF_EXPORT){\n    ...\n  }\n  text\n}"
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#context-aware-content",
    "href": "posts/2019-09-04-data-driven-cv/index.html#context-aware-content",
    "title": "Building a data-driven CV with R",
    "section": "Context-aware content",
    "text": "Context-aware content\nI took advantage of this PDF export value to add a conditional text entry at the top of the document. This tells readers of the PDF version that there is an HTML version and gives readers of the HTML version a link to download the PDF version.\n\nif(PDF_EXPORT){\n  cat(\"View this CV online with links at _nickstrayer.me/cv_\")\n} else {\n  cat(\"[&lt;i class='fas fa-download'&gt;&lt;/i&gt; Download a PDF of this CV](https://github.com/nstrayer/cv/raw/master/strayer_cv.pdf)\")\n}"
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#customizing-the-css",
    "href": "posts/2019-09-04-data-driven-cv/index.html#customizing-the-css",
    "title": "Building a data-driven CV with R",
    "section": "Customizing the CSS",
    "text": "Customizing the CSS\nPreviously we focused on how to get the positions in, but one of the more compelling aspects of a CV is how you can stand out by making it unique. Because of this, I didn‚Äôt want to just use the default (but beautiful) resume template; so I modified the CSS using the CSS override option available in RMarkdown.\nThis involves just writing your new styles in a .css file somewhere in the same directory as your markdown and then referencing it in the YAML header:\n‚äïI left in the original ‚Äòresume‚Äô CSS file so I didn‚Äôt lose all the niceness it provides and my styles just stack on top of it.\n---\ntitle: \"Nick Strayer's CV\"\noutput:\n  pagedown::html_resume:\n    css: ['css/styles.css', 'resume']\n---\nFor instance, I decided to change the fonts to a pair I really love from Google Fonts‚Ä¶\n\n@import url(\"https://fonts.googleapis.com/css?family=Montserrat|Playfair+Display&display=swap\");\n\n/* Main text is monserrat*/\nbody {\n  font-family: \"Montserrat\", sans-serif;\n  font-weight: 300;\n  line-height: 1.3;\n  color: #444;\n}\n\n/* Give headers playfair font */\nh1,\nh2,\nh3 {\n  font-family: \"Playfair Display\", serif;\n  color: #000;\n}\n\nThis blog is not one you‚Äôre probably reading to get CSS tips, so I will not go into the specifics. ‚äïIf you are interested in learning more about CSS I really like Kevin Powell‚Äôs youtube channel. It transformed the way I view and write CSS for the better. I mainly went in and made the sidebar a bit smaller and changed the way some of the lists were formatted. If you want to see all the CSS I used for my CV check out the main style file on github. The singe-paged resume also has an additional set of styles that stack on top of both the default style sheet and my CV styles."
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#want-to-build-your-cv-this-way",
    "href": "posts/2019-09-04-data-driven-cv/index.html#want-to-build-your-cv-this-way",
    "title": "Building a data-driven CV with R",
    "section": "Want to build your CV this way?",
    "text": "Want to build your CV this way?\nOkay, we can dispense with the formalities. The quickest and easiest way to build your CV this way is to:\n\nCopy the files from my CV\nReplace the positions data with your own data\nPersonalize the text main document text‚Ä¶\n\n\nCopy the files\nIf you are familiar with git/ github you can fork the repo, if you just want the files, here‚Äôs a link to download a zip of all the files needed.\n\n\n\nReplace position data\nThe main file you need to change is positions.csv. Once you have all your positions in then go into the main index.Rmd if you‚Äôre making a CV or resume.Rmd if you‚Äôre making a resume.\nThen fill in your sections. If my sections don‚Äôt fit your profile (likely) you can add a new section in positions.csv (, say 'serious_workthings'.) You can include this new section by placing the following in the document.\n‚äïA huge thanks to Mark Scheuerell, who showed me how to properly render meta-rmarkdown!\nSerious Work Things {data-icon=book}\n--------------------------------------------------------------------------------\n\n```{r, results = 'asis'}\nposition_data %&gt;% print_section('serious_workthings')\n```\n\n\nPersonalize text\nLast, change the personalized text for skills and header text etc..\n‚äïI didn‚Äôt put everything into a spreadsheet.\n\n\n\n\n\nFinally, knit your results by pressing ‚ÄòKnit‚Äô at the top of the .Rmd in RStudio. ‚äïNot using RStudio? Then just run rmarkdown::render(‚Äòindex.Rmd‚Äô)."
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#the-giants-upon-whos-shoulders-i-stand",
    "href": "posts/2019-09-04-data-driven-cv/index.html#the-giants-upon-whos-shoulders-i-stand",
    "title": "Building a data-driven CV with R",
    "section": "The giants upon who‚Äôs shoulders I stand",
    "text": "The giants upon who‚Äôs shoulders I stand\nI want to emphasise that I have done nothing particularly special here. Everything is possible because of a series of amazing contributions by the R community.\nIn particular for this you should thank:\n\nYihui Xie: For markdown, and pagedown (, and blogdown that I am using to write this blog post.)\nRomain Lesur: For pagedown\nMara Averick: For somehow keeping on top of every new R package and giving demos on how to use them\nJim Hester: For glue\nHadley Wickham and Lionel Henry: for purrr\nMany more that I have neglected to mention.\n\nAlso, because I have no original thoughts. The ‚Äòdata-driven CV‚Äô concept has been done before by others. For instance:\n\nLucy D‚ÄôAgostino-McGowan (aka the better half of this blog) has a data-driven academic website.\nThe Vitae package by Mitchell O‚ÄôHara-Wild and Rob J Hyndman."
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#share-your-product",
    "href": "posts/2019-09-04-data-driven-cv/index.html#share-your-product",
    "title": "Building a data-driven CV with R",
    "section": "Share your product!",
    "text": "Share your product!\nIf you end up building your CV or resume using these techniques let me know! Tweet it out and let me know and I will gladly retweet it!\n\n\nnew version my CV, https://t.co/tWYjP6hxKf many thanks to @NicholasStrayer\n\n‚Äî Guangchuang Yu (@guangchuangyu) September 29, 2019\n\n\n\n\nI already had an resume made from pagedown. But thanks to this tweet and the code you made available, now it's really easy to update the 2 versions (long and short). Thank you so much! This is my resume (thanks to your code): https://t.co/QQ5NO5T9mJ\n\n‚Äî Beatriz Milz (@BeaMilz) October 6, 2019\n\n\n\n\nThanks @NicholasStrayer ! I use your your framework to build my new CV, and it looks great ! https://t.co/zyOTEalcqK\n\n‚Äî F√©lix M. (@felix_mil_) October 8, 2019\n\n\n‚äïCheck out the slick skills visualization!\n\n\nThanks very much! Here my cv in R created with pagedown #rstats #RLover pic.twitter.com/jcbokatnyM\n\n‚Äî Antony.Barja (@AntonyBarja1) October 8, 2019"
  },
  {
    "objectID": "posts/2019-09-04-data-driven-cv/index.html#questions",
    "href": "posts/2019-09-04-data-driven-cv/index.html#questions",
    "title": "Building a data-driven CV with R",
    "section": "Questions",
    "text": "Questions\nThis was a rather high-level skim over the process that left out some details. If any of those details I ommited was more important than I realized and you‚Äôre stuck. Don‚Äôt hesitate to leave a comment here, message me on Twitter, or email me."
  },
  {
    "objectID": "posts/2022-12-14-transparency-in-public-health/index.html",
    "href": "posts/2022-12-14-transparency-in-public-health/index.html",
    "title": "Transparency in Public Health",
    "section": "",
    "text": "Transparency in public health messaging matters. Hannah Mendoza and I looked at how providing transparent information about why a public health recommendation is being made can increase uptake in a randomized trial published today in Plos One"
  },
  {
    "objectID": "posts/2022-12-14-transparency-in-public-health/index.html#what-did-we-do",
    "href": "posts/2022-12-14-transparency-in-public-health/index.html#what-did-we-do",
    "title": "Transparency in Public Health",
    "section": "What did we do?",
    "text": "What did we do?\nWe conducted a randomized controlled trial to assess whether disclosing elements of uncertainty in an initial public health statement will change the likelihood that participants will accept new, different advice that arises as more evidence is uncovered.\nWe came up with a hypothetical health scenario and began by asking participants ‚Äúhow likely are you to sanitize your mobile phone?‚Äù\n(26% said they were likely / very likely)\n298 participants were randomized to treatment and 298 control.\ncontrols saw statement 1A, a (hypothetical) recommendation from public health experts that you don‚Äôt need to sanitize your mobile phone\ntreated saw 1B, the same recommendation with added transparent information\n After seeing these statements they were asked again their likelihood to sanitize their mobile phone. Then all participants were shown a second, üÜï statement ‚Äî this new statement said based on new information public health experts recommend you should sanitize your phone\n\nAll participants were again asked after seeing this second statement how likely they were to sanitize their phone (this is our primary endpoint ‚úÖ)\nWe fit proportional odds models stratified by the baseline likelihood to agree with the final advice.\n\nAmong participants who were more likely to agree with the final recommendation at baseline, those who were initially shown uncertainty had a 46% lower odds of being more likely to agree with the final recommendation compared to those who were not (OR: 0.54, 95% CI: 0.27-1.03).\nAmong participants who were less likely to agree with the final recommendation at baseline, those who were initially shown uncertainty had 1.61 times the odds of being more likely to agree with the final recommendation compared to those who were not (OR: 1.61, 95% CI: 1.15-2.25)\nThis has implications for public health leaders when assessing how to communicate a recommendation, suggesting communicating uncertainty influences whether someone will adhere to a future recommendation. Presenting public health recommendations with transparency can both ‚¨ÜÔ∏è and ‚¨áÔ∏è adherence to future recommendations in the presence of changing evidence, depending on the individuals‚Äô baseline likelihood to do what is being recommended prior to seeing the recommendation. Many public health recommendations, particularly those being made during an ever changing pandemic, have a high probability of changing in the future, making this result highly relevant. For example, the recommendation to üò∑ wear masks will come and go depending on the prevalence and impact of the infectious disease of concern in the community. When initially communicating this new recommendation to the public, an explanation of the reasoning for the recommendation as well as any potential uncertainty may have resulted in ‚¨ÜÔ∏è adherence among those who would not have been likely to wear a mask in the recommended settings. 12% of US respondents in March 2020 reported masking to protect themselves. At the time, the recommendation from the CDC was: ‚ÄúIf you are NOT sick: You do not need to wear a face mask unless you are caring for someone who is sick (and they are not able to wear a face mask)‚Äù. Based on our results, if the initial recommendation to avoid wearing face masks had been presented with the uncertainty communicated, we might expect some of this 12% to be less likely to follow the recommendation ‚Äî however! 88% were not likely to wear masks a priori; our results suggest this population may have ultimately had higher compliance with the subsequent recommendation to wear face masks had the initial communication been made with transparency explaining the ‚Äúwhy‚Äù behind it. This study has lots of limitations (it was only hypothetical!) but I think is a good first step for trying to quantify the impact of the way communications can impact future decisions.\nCheck out the full article here"
  },
  {
    "objectID": "posts/2021-07-21-denominators-matter/index.html",
    "href": "posts/2021-07-21-denominators-matter/index.html",
    "title": "Denominators Matter",
    "section": "",
    "text": "I‚Äôve seen a lot today about how effective the vaccines are; mistakes aside, lots of folks seem to be mixing up which denominators matter - good thing statisticians LOVE denominators!\n\nIf you see something like x% of the sick/hospitalized/deceased were vaccinated, the better the vaccine uptake the scarier this number will seem! It is using the wrong denominator. For example, here is a scenario with 90% vaccination, 4 people got sick: 2 vaccinated 2 unvaccinated:\n\nIn this scenario, 50% of the sick were vaccinated, but this is the wrong metric to look at! It is using the wrong denominator. It doesn‚Äôt take into account that 90% of the population is vaccinated (yay!). r tufte::margin_note('Even Bill Gates made this mistake when he said he\\'d [rather encounter a shark than a mosquito](https://twitter.com/BillGates/status/1118196606975787008).')This is called flipping the conditional (and is a problem that we see all the time!). Instead of looking at the probability of being vaccinated given you are sick, you want to look at the probability of being sick given you are vaccinated.\n What you need to do is look at the rates among the vaccinated and unvaccinated separately, and then compare them. Here 11% of the vaccinated got sick, 100% of the unvaccinated got sick.\n\n\nWe calculate vaccine efficacy as\n(risk among unvaccinated - risk among vaccinated) / risk among unvaccinated\nso in this case, 89% (yay!)\n\nSo in sum, denominators matter! When scrolling past headlines, be sure to think about what denominators are in play!"
  },
  {
    "objectID": "posts/2021-04-07-nytimes-map-how-to/index.html",
    "href": "posts/2021-04-07-nytimes-map-how-to/index.html",
    "title": "NYTimes Map How-to",
    "section": "",
    "text": "There was a recent email thread in the IsoStat listserv about a cool visualization that recently came out in the New York Times showing COVID-19 cases over time. This sparked a discussion about whether this was possible to recreate in R with ggplot, so of course I gave it a try!\n\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(geofacet)\nlibrary(zoo)\n\nThe plot shows cases per 100,000 by state, so I first needed to pull population data. To do that I used the tidycensus package. (If you don‚Äôt have an API key, you can get one here)\n\ncensus_api_key(\"YOUR API KEY\")\n\nI pulled the population by state from 2019.\n\npop &lt;- get_acs(geography = \"state\", variables = \"B01003_001\", year = 2019)\n\nThen I pulled the cases in from the New York Times GitHub repo.\n\ncases &lt;- read_csv(\"https://github.com/nytimes/covid-19-data/raw/master/us-states.csv\")\n\nThese need to be wrangled a bit:\n\nThe data come in as cumulative cases, and we want cases per day, so I create a new variable case for this purpose\nThere is a weirdo data point in Missouri on March 8th (it looks like there were 50,000 cases!) so I just removed that\nI merged in the state populations that I pulled from the census\nI created a 7 day rolling average\nI created a variable for 7 day average per 100,000 people - this is the main variable used in the plot\nI filtered to the range used in the original visualization - from Februrary 1st to April 4th\nI merged in state abbreviations to make the plot easier to read\n\n\nd &lt;- cases %&gt;%\n  group_by(state) %&gt;%\n  mutate(case = c(cases[1], diff(cases))) %&gt;%\n  ungroup() %&gt;%\n  filter(!(date == as.Date(\"2021-03-08\") & state == \"Missouri\")) %&gt;% \n  left_join(pop, by = c(\"fips\" = \"GEOID\")) %&gt;%\n  group_by(state) %&gt;%\n  arrange(date) %&gt;%\n  mutate(\n    case_7 = rollmean(case, k = 7, fill = NA),\n    case_per_100 = (case_7 / estimate) * 100000) %&gt;%\n  ungroup() %&gt;%\n  filter(date &gt; as.Date(\"2021-01-31\"), date &lt; as.Date(\"2021-04-05\"))\n\nstates &lt;- tibble(state = state.name,\n                 state_ = state.abb) %&gt;%\n   add_row(state = \"District of Columbia\", state_ = \"DC\")\n\nd &lt;- left_join(d, states, by = \"state\") %&gt;%\n  filter(!is.na(state_))\n\nThis plot had a neat feature that it filled in the area from the lowest point onward; to replicate this I found the date with the minimum cases per 100,000 and created a variable col to indicate any date after this point.\n\nd &lt;- d %&gt;% \n  group_by(state) %&gt;%\n  slice_min(case_per_100) %&gt;%\n  slice(1) %&gt;%\n  mutate(min_date = date) %&gt;%\n  select(min_date, state) %&gt;%\n  left_join(d, by = \"state\") %&gt;%\n  mutate(col = ifelse(date &gt;= min_date, \"yes\", \"no\"))\n\nNow time to plot! The x-axis is date, the y-axis is case_per_100 and voila!\n\nggplot(d, aes(x = date, y = case_per_100)) +\n  geom_line(color = \"#BE2D22\") + \n  geom_area(aes(alpha = col), fill = \"#BE2D22\") +\n  scale_alpha_discrete(range = c(0, 0.7)) +\n  facet_geo(~state_) + \n  theme_minimal() +\n  labs(x = \"\", \n       y = \"\",\n       title = \"Cases per 100,000\",\n       subtitle = \"Feb 1 - Apr 4, Red area indicates rise since lowest point of 2021\",\n       caption = \"Note: Shows seven-day average\") +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.grid.major.x = element_blank(),\n        legend.position = \"none\")"
  },
  {
    "objectID": "posts/2017-09-03-plumber-slack-bot/index.html",
    "href": "posts/2017-09-03-plumber-slack-bot/index.html",
    "title": "A Simple Slack Bot With Plumber",
    "section": "",
    "text": "Or you know, a graduate student could write an api for finding cat/dog photos on the internet and sending them to slack channels. Both equally valuable use cases.\nPlumber is an R package that allows you to create web apis in R. This is fantastic because it allows you to take your R code (models, database access, etc) and make them easy to access from anything capable of making http requests. This means a data scientist working at a tech company who has developed a fancy model using all of the tools at R‚Äôs disposal can make it available to their company‚Äôs app without needing to have a team of engineers port the model to whatever language the company uses for their back-end."
  },
  {
    "objectID": "posts/2017-09-03-plumber-slack-bot/index.html#intro-to-plumber",
    "href": "posts/2017-09-03-plumber-slack-bot/index.html#intro-to-plumber",
    "title": "A Simple Slack Bot With Plumber",
    "section": "",
    "text": "Or you know, a graduate student could write an api for finding cat/dog photos on the internet and sending them to slack channels. Both equally valuable use cases.\nPlumber is an R package that allows you to create web apis in R. This is fantastic because it allows you to take your R code (models, database access, etc) and make them easy to access from anything capable of making http requests. This means a data scientist working at a tech company who has developed a fancy model using all of the tools at R‚Äôs disposal can make it available to their company‚Äôs app without needing to have a team of engineers port the model to whatever language the company uses for their back-end."
  },
  {
    "objectID": "posts/2017-09-03-plumber-slack-bot/index.html#intro-to-slack-apps",
    "href": "posts/2017-09-03-plumber-slack-bot/index.html#intro-to-slack-apps",
    "title": "A Simple Slack Bot With Plumber",
    "section": "Intro To Slack Apps",
    "text": "Intro To Slack Apps\nThere are actually many ways to build apps for Slack, but the way we will be focusing on are called ‚Äúslash commands‚Äù. If you‚Äôre a slack power user you most likely know what these are, if you‚Äôre not (like me) this is what prevents you from ever starting a message with ‚Äú/‚Äù. Basically, you can wire up Slack to send out a simple HTTP POST request (don‚Äôt worry, we‚Äôll get to these in two seconds) when a user types /&lt;your command&gt;. In this demo we will be wiring up our slash command to send a request to our plumber driven api."
  },
  {
    "objectID": "posts/2017-09-03-plumber-slack-bot/index.html#http-requests",
    "href": "posts/2017-09-03-plumber-slack-bot/index.html#http-requests",
    "title": "A Simple Slack Bot With Plumber",
    "section": "HTTP Requests",
    "text": "HTTP Requests\n\n\nThere are more than just GET and POST requests (see here for a more thorough runthrough of them), but for the purposes of this tutorial you can stop at these.\nHTTP requests are the lingua-franca of the web. Every time you access a website your web browser is engaging in a conversation with the server hosting the site conducted in these requests. For instance, when you decided to load this website your browser sent a GET request to the server hosting it, saying ‚Äúhey, can I please get the files for the site?‚Äù. In response the server sent the raw files to your browser which then assembled into what you‚Äôre looking at now. If you decided to comment on this article, after you‚Äôve typed in your comment and pressed send, your browser would send off a POST request to the discus servers that contains the text of your comment, upon receiving the payload the server would send back a message acknowledging successful delivery (or unsuccessful)."
  },
  {
    "objectID": "posts/2017-09-03-plumber-slack-bot/index.html#reddit-api",
    "href": "posts/2017-09-03-plumber-slack-bot/index.html#reddit-api",
    "title": "A Simple Slack Bot With Plumber",
    "section": "Reddit API",
    "text": "Reddit API\n\n\nYou‚Äôll notice that we add the line User-Agent = woofbot 2000 in the GET request. This is because reddit gets suspicious whenever its api gets hit by a client not introducing itself and will only allow a request every minute or so. When we introduce our app with user-agent reddit will send us as many photo links as our heart desires.\nIf one is developing an app to get photos of cute animals, a natural place to go to find photos is Reddit. Conveniently they also have a fantastically easy to use API for getting information about the posts on a given subreddit. Say you want to get the ‚Äòhot‚Äô posts for the subreddit ‚Äòr/catslaps‚Äô. To do this you simply put together a url as if you were going to visit the subreddit in your browser and append .json at the end of it. Really, it‚Äôs that easy. So in the case of catslaps: www.reddit.com/r/catslaps/hot/.json. (We‚Äôll also limit the number of posts we get back to 100 with ?limit=100 to keep things speedy) Let‚Äôs demo this really quick:\n\nlibrary(listviewer)\nlibrary(httr)\n\nredditTopPosts &lt;- function(subreddit){\n  query &lt;- sprintf(\n    'https://www.reddit.com/r/%s/hot/.json?limit=100',\n    subreddit\n  )\n  \n  GET(url = query, add_headers(`User-agent` = 'woofbot 2000')) %&gt;% \n    content('text') %&gt;% \n    jsonlite::fromJSON() \n}\n\n\ncatslapsTop &lt;- redditTopPosts(subreddit = 'catslaps')\n\n\n# View it\njsonedit(catslapsTop)\n\n\n\n\n\n\nWe get back a big hairy list of data on the top posts. What we are after (the links to the images) is in the path data -&gt; children -&gt; data. So we can write a function to get that out of our api response and simplify our life in the future:\n\n# Takes api response from redditTopPosts() and \n# returns a dataframe with post urls and titles\ngetURLS &lt;- function(response){\n  response$data$children$data %&gt;% \n    select(url, title)\n}\n\ncatslapsTop %&gt;% \n  getURLS() %&gt;% \n  head() %&gt;% \n  knitr::kable()\n\n\n\n\n\n\n\n\nurl\ntitle\n\n\n\n\nhttps://v.redd.it/ch9759t4kwwa1\nSmol cat not sure if he is friend\n\n\nhttps://v.redd.it/zflcu1qriywa1\nMurder.exe fully online\n\n\nhttps://v.redd.it/xjrng84h8vwa1\nMoonpie is so not okay with me blowdrying my hair\n\n\nhttps://v.redd.it/v2fmw4k58twa1\nNeither of them comprehend their size\n\n\nhttps://v.redd.it/zmym8p82fpwa1\nWAKE UP\n\n\nhttps://v.redd.it/p8avmnuo4twa1\nI call her Tina Tyson ü§£ (sound on)\n\n\n\n\n\nWe also want to make sure we‚Äôre exclusively getting images and not albums or whatnot, so let‚Äôs filter these results to just images and then display one to make sure we‚Äôre getting what we want.\n\n# Use regular expressions to get the links that have the correct file extensions\njustImages &lt;- function(links) {\n  links %&gt;% filter(grepl(\"\\\\.jpg|\\\\.gif|\\\\.png\", url))\n}\n\n\nimagePosts &lt;- catslapsTop %&gt;% \n  getURLS() %&gt;% \n  justImages()\n\nimagePosts %&gt;% \n  head() %&gt;% \n  knitr::kable()\n\n\n\n\n\n\n\n\nurl\ntitle\n\n\n\n\nhttps://i.redd.it/zhtxwb5mkvua1.png\nWe thought they could be friends, but turns out the cat is always the boss.\n\n\nhttps://i.redd.it/3nfaq8e6jeta1.jpg\nGot slapped\n\n\nhttps://i.redd.it/v6nqzxggyata1.gif\nCat hunts and slaps toothbrush\n\n\nhttps://i.redd.it/qk4c4375d9ta1.png\n[OC] This cat does not negotiate with insects\n\n\nhttps://i.redd.it/85wpi21wicta1.png\ndon‚Äôt touch me! slap\n\n\nhttps://i.redd.it/7qa6a95z99ta1.png\nA never ending battle\n\n\n\n\n\nTesting one out\n\n\nThat‚Äôs one angry gatito.\n\n\"No! Can't you see I'm busy?\"\n\n\nLooks like we‚Äôre all set with the image source, now let‚Äôs just setup plumber to send off one of these images when called."
  },
  {
    "objectID": "posts/2017-09-03-plumber-slack-bot/index.html#putting-it-together",
    "href": "posts/2017-09-03-plumber-slack-bot/index.html#putting-it-together",
    "title": "A Simple Slack Bot With Plumber",
    "section": "Putting It Together",
    "text": "Putting It Together\nBefore we dive into actually implementing our bot logic, it‚Äôs important to note that the Slack slash command api requires responses to be sent to it in a specific JSON form. Rather than dive deeply into the specifics of this I will just demonstrate how to send a single image with a caption, but know that you can do much more than this by investigating the official docs.\nFirst we start by assembling our response object, then we will wire it up to plumber.\nIn the same script (or a new one that sources the functions we have already written), add the function sendToSlack\n\nsendToSlack &lt;- function(){\n\n  photoLink &lt;- redditTopPosts(subreddit = 'catslaps') %&gt;% \n    getURLS() %&gt;% \n    justImages() %&gt;% \n    .$url %&gt;% \n    sample(1) # pick photo at random.\n  \n  # photos must be sent as \"attachments\" to slack\n  attachments &lt;- data_frame(\n    fallback = \"uh oh, the image didn't load, bad omen\",\n    image_url = photoLink, \n    thumb_url = photoLink\n  )\n  \n  return(\n    list(\n      response_type = unbox(\"in_channel\"),\n      text = unbox(\"This cat likes to slap!\"),\n      unfurl_media = unbox(TRUE),\n      attachments =  attachments \n    )\n  )\n}\n\n\n\nThe unbox() that wrapping some fields is a function from jsonlite that lets plumber know how to properly format its response. Otherwise it will try and turn the single responses into vectors of length one, which Slack doesn‚Äôt know how to handle.\nTo break this down a tiny bit: first we are grabbing a random photo from reddit using our functions we wrote earlier, then we are putting that photo into a data frame called attachments and putting that in a list that contains a field response_type = 'in_channel which tells slack to show response to everyone and not just the sender, text which is self explanatory, and unfurl_media = true which tells Slack to load the image immediately and not require the user to click expand to see it (sometimes if the image is really large they still will have to).\nWe can test this really quick to see how it looks.\n\nsendToSlack() %&gt;% jsonedit()\n\n\n\n\n\n\n\nLooks like it‚Äôs formatting correctly! Now let‚Äôs wire up plumber! Buckle in, this takes a while‚Ä¶.\n\n#* @post /catslap\nfunction(){\n  sendToSlack()\n}\n\n\n\nThere are plenty of standard functions in plumber too, just we arent using them for our relatively simple app here.\n‚Ä¶and we‚Äôre done. Yup, it‚Äôs (almost) that simple. Plumber is much like roxygen in that it operates mainly through special comments that tell it what to do. In this case it‚Äôs saying, watch for POST requests coming through at our server‚Äôs url slash catslap. All we need to do now is setup a process that actually runs this to see if it works.\nCreate a separate script in the same directory as the one with sendToSlack in it and put the following..S.\n\nlibrary(plumber)\nr &lt;- plumb('&lt;file with sendToSlack&gt;.R')\nr$run(port = 4000)\n\nThat‚Äôs it. After executing the script we just wrote, your computer will be actively watching for requests coming in and will respond with a photo of a cat.\nYou can test out that it‚Äôs working by trying a POST request to localhost:4000/catslap using httr and you should get back the same thing you sent out.\n\nhttr::POST(url = \"localhost:4000/meow\") %&gt;% \n  httr:content('text') %&gt;% \n  jsonlite::fromJSON()\n# &gt; Returns the same thing that sendToSlack() does.\n\nYou‚Äôre all good to go. The only problem is localhost (or it may be 127.0.0.1 or something along those lines) is not accessible outside of your computer, and even if it was, it would have to be continuously running for it to be of any use to your slack channel."
  },
  {
    "objectID": "posts/2017-09-03-plumber-slack-bot/index.html#hosting-it",
    "href": "posts/2017-09-03-plumber-slack-bot/index.html#hosting-it",
    "title": "A Simple Slack Bot With Plumber",
    "section": "Hosting It",
    "text": "Hosting It\nWhat you need to do is get your app hosted. This is where this tutorial will invariably fall short, and I apologize for that. There are about a million ways to host something like this and it all depends on your situation. For instance, I do all of my R computing from RStudio Server which is running on a Digital Ocean droplet. For me, since the droplet is already accessable from the general internet, by running the last command I am already hosting my plumber app for the world and am ready to go, but most likely you wont be.\n\n\nIn terms of cloud providers I‚Äôm all for Digital Ocean because of ease of use and transparent pricing. I‚Äôve used Amazon AWS before and had some unexplainably high bills.\nI will defer to the excellent plumber docs on this instance. The hosting section does a fantastic job at describing how to take a plumber app like we just built and get it onto the internet for anyone to see. Plumber makes it super easy to roll out your app onto a Digital Ocean server using custom-built docker containers that can be deployed directly from the R command line.\nI will describe what I did so users in a similar situation to me will at least skip the headaches I went through. My situation applies to anyone who has a server open to the internet on at least some ports and can be easily ssh‚Äôd into. To make my app run in the background I followed the instructions from the plumber docs on using the tool pm2 to integrate hosting in my environment.\nAfter installing pm2 as per the instructions I just run the command pm2 start --interpreter=\"Rscript\" plumbServer.R and my app is instantly running in the background and I can forget about it."
  },
  {
    "objectID": "posts/2017-09-03-plumber-slack-bot/index.html#setup-the-slack-app",
    "href": "posts/2017-09-03-plumber-slack-bot/index.html#setup-the-slack-app",
    "title": "A Simple Slack Bot With Plumber",
    "section": "Setup the Slack app",
    "text": "Setup the Slack app\nI promise it‚Äôs almost all over. Just navigate to https://api.slack.com/apps and log in. From there click the button that says ‚ÄúCreate New App‚Äù.\nNext fill in the form with your app‚Äôs desired info:\n\nAfter you‚Äôve filled that info in, navigate to the ‚Äúslash commands‚Äù section of the app page.\n\nIn the slash command creation screen you can name your command whatever you desire (here I‚Äôm calling it catslaps).\n\nAfter completing all this, in the app‚Äôs main page, go the left hand menu bar and under settings click ‚ÄòInstall App‚Äô (you can see this setting in the upper left of the first two screenshots) and accept the terms given and boom, you have your very own slack bot/app. Let‚Äôs test it out.\n\n\n\nOh, she‚Äôs got her face in the danger zone.\nYay! All the world‚Äôs problems are now solved!"
  },
  {
    "objectID": "posts/2017-09-03-plumber-slack-bot/index.html#wrap-up",
    "href": "posts/2017-09-03-plumber-slack-bot/index.html#wrap-up",
    "title": "A Simple Slack Bot With Plumber",
    "section": "Wrap-up",
    "text": "Wrap-up\nLike I said before, I wish I could provide a better section on hosting. Ultimately though everyone‚Äôs situation will be a little bit different and the documentation provided with plumber is absolutely fantastic for getting your app hosted. If you have issues, please feel free to send me a tweet (see my profile card below) or leave a comment. The world needs more cat and dog images in it so anyway I can assist I will."
  },
  {
    "objectID": "posts/2017-02-09-the-prevalence-of-drunk-podcasts/index.html",
    "href": "posts/2017-02-09-the-prevalence-of-drunk-podcasts/index.html",
    "title": "The prevalence of drunk podcasts",
    "section": "",
    "text": "For today‚Äôs rendition of I am curious about everything, in Hilary Parker & Roger Peng‚Äôs Not So Standard Deviations Episode 32, Roger suggested the prevalence of drunk podcasting has dramatically increased - so I thought I‚Äôd dig into it üößüë∑.\nI pulled the iTunes API for the term drunk in podcasts & plotted the results over time. I also finally found an excuse to use emoGG.\nlibrary(dplyr)\nlibrary(ggplot2)\nreq &lt;- httr::GET(url = \"https://itunes.apple.com/search\",\n          query = list(\n            term = \"drunk\",\n            media = \"podcast\",\n            limit = 200\n          ))\n\nitunes &lt;- jsonlite::fromJSON(httr::content(req))$results\nThis resulted in 200 podcasts, which I grouped by month/year released.\nitunes %&gt;%\n  mutate(date = as.Date(releaseDate),monyear = zoo::as.yearmon(date)) %&gt;%\n  group_by(monyear) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(date = zoo::as.Date(monyear)) %&gt;%\n  ggplot(aes(x = date,y=n)) +\n    scale_x_date() +\n    emoGG::geom_emoji(emoji=\"1f37a\") + \n    ylab(\"Number of 'Drunk' podcasts released\")\nIt looks like Roger may be onto something.\nI tried to find the number of podcasts on iTunes by month over the past couple of years to adjust for this, but to no avail. If you have that data, please send it my way, so I can complete this very crucial analysis. In the meantime, I‚Äôll pretend it doesn‚Äôt matter: While it is certainly true that the number of podcasts in general has absolutely increased over this time period, I would be surprised if the increase is as dramatic as the increase in the number of ‚Äúdrunk‚Äù podcasts.\nHere is a little shout out to my favorite drunk podcast Drunk Monk, with the lovely Keiko Agena (Gilmore Girls‚Äô Lane herself!).\nCheers! üçª"
  },
  {
    "objectID": "posts/2017-02-09-the-prevalence-of-drunk-podcasts/index.html#update",
    "href": "posts/2017-02-09-the-prevalence-of-drunk-podcasts/index.html#update",
    "title": "The prevalence of drunk podcasts",
    "section": "Update!",
    "text": "Update!\nEthan pointed out the curve was looking a little drunk‚Ä¶\n\n\n@LucyStats @rdpeng @NSSDeviations Wait ‚Ä¶ that curve is starting to wrap around. Is it drunk? pic.twitter.com/YBAkAMLDM0\n\n‚Äî Ethan Jewett (@esjewett) February 9, 2017\n\n\nIndeed! We‚Äôve left in February‚Äôs data (despite the fact that February is not even half way over üôà).\n\nitunes %&gt;%\n  mutate(date = as.Date(releaseDate),monyear = zoo::as.yearmon(date)) %&gt;%\n  filter(date &lt; as.Date('2017-02-01')) %&gt;% #add a filter!\n  group_by(monyear) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(date = zoo::as.Date(monyear)) %&gt;%\n  ggplot(aes(x = date,y=n)) +\n    scale_x_date() +\n    emoGG::geom_emoji(emoji=\"1f37a\") + \n    ylab(\"Number of 'Drunk' podcasts released\")\n\n\n\n\nIf you are interested in other things NSSD has inspired me to do, check out this, or this."
  },
  {
    "objectID": "posts/2021-03-01-grided/index.html",
    "href": "posts/2021-03-01-grided/index.html",
    "title": "Grided: a web-app for building css-grid layouts",
    "section": "",
    "text": "Grided is an app that lets you define CSS-Grid layouts in a simple GUI allowing you to see how your app will look while you define it. The app is hosted at nickstrayer.me/grided and the source is on github."
  },
  {
    "objectID": "posts/2021-03-01-grided/index.html#summary",
    "href": "posts/2021-03-01-grided/index.html#summary",
    "title": "Grided: a web-app for building css-grid layouts",
    "section": "",
    "text": "Grided is an app that lets you define CSS-Grid layouts in a simple GUI allowing you to see how your app will look while you define it. The app is hosted at nickstrayer.me/grided and the source is on github."
  },
  {
    "objectID": "posts/2021-03-01-grided/index.html#background",
    "href": "posts/2021-03-01-grided/index.html#background",
    "title": "Grided: a web-app for building css-grid layouts",
    "section": "Background",
    "text": "Background\nI have recently started a job at RStudio on the Shiny team. One of the things that I was brought on to do is ‚Äúhelp Shiny user‚Äôs build more beautiful apps.‚Äù Because of this I have been working on a project allowing users to layout dashboard apps easily with the CSS-Grid specification. CSS-Grid is a super powerful way of naturally declaring layouts but if you‚Äôre like me, the initial setup of getting your app going can be a bit cumbersome. Because of this I almost always reach for a tool like Sarah Drasner‚Äôs CSS grid generator and grid.layoutit.\nInspired by both of these resources, I began working on a similar tool that can let Shiny user‚Äôs draw their desired layout and then get the code for that layout. After working on this project for a while, however, I realized that what I had built had a few features that I found useful compared to existing tools so I decided to spin off a purely JS/CSS version."
  },
  {
    "objectID": "posts/2021-03-01-grided/index.html#goal",
    "href": "posts/2021-03-01-grided/index.html#goal",
    "title": "Grided: a web-app for building css-grid layouts",
    "section": "Goal",
    "text": "Goal\nThe goal of Grided is to simplify the very first part of building a grid-layout. You specify using a (hopefully) straightforward GUI how you want your app laid out and you get code to implement the structure you defined."
  },
  {
    "objectID": "posts/2021-03-01-grided/index.html#usage",
    "href": "posts/2021-03-01-grided/index.html#usage",
    "title": "Grided: a web-app for building css-grid layouts",
    "section": "Usage",
    "text": "Usage\n\nThe page starts with an empty grid layout. From here you can adjust the number of rows and columns along with the gap between those rows and columns. As you do this the faux app on the right updates in real time. Each column and row can be resized by a control form. r tufte::margin_note(\"A fun hidden feature is that when you use the pixel-sizing a drag interface appears for resizing.\") For instance you can have a fixed-width sidebar.\n\nOnce you have your desired grid, you add your elements by clicking and dragging them on the grid. Once you have placed an element you provide its ID. Messed up where you placed it? Just drag it around with the supplied interaction handles to move it. r tufte::margin_note(\"A little dashed box shows you the extent of your drag, while the element snaps to the grid.\")\n\nDecide you actually want a footer on your page after declaring a bunch of other stuff? No worries, simply add another row, your elements will stay where they are. Decide you don‚Äôt want a footer? Just remove a a row. If elements reside in the deleted row/column you will be asked how you want to update them in a popup.\n Happy with your layout and ready to get building your site? Click the ‚ÄúGet code‚Äù button in the top right and you will be provided the CSS and HTML code necessary to recreate your layout on your site.\n r tufte::margin_note(\"For the Shiny users, the main version of the app will give you R/Shiny code instead of CSS and HTML.\")"
  },
  {
    "objectID": "posts/2021-03-01-grided/index.html#tech-specs",
    "href": "posts/2021-03-01-grided/index.html#tech-specs",
    "title": "Grided: a web-app for building css-grid layouts",
    "section": "Tech-Specs",
    "text": "Tech-Specs\nThere are a few fun facts about the app:\n\nZero dependencies. The app is all just vanilla javascript and CSS. It even uses system fonts. The result: the whole app is only 47.5kb.\nWritten in typescript. This is my very first typescript project and it worked pretty dang well.\nFast: After the resources are loaded it takes less than 10ms to completely render app to starting point."
  },
  {
    "objectID": "posts/2021-03-01-grided/index.html#limitations",
    "href": "posts/2021-03-01-grided/index.html#limitations",
    "title": "Grided: a web-app for building css-grid layouts",
    "section": "Limitations",
    "text": "Limitations\nThere are a few (to be kind) limitations right now:\n\nOnly a few CSS units are supported. These are fr, rem, and px. CSS-Grid has some cool fancy options like minmax() but I couldn‚Äôt figure out how to build an interface like this for these dynamic layouts. r tufte::margin_note(\"If you can think of one, let me know!\")\nNo nested grids. There‚Äôs no reason in grid you cant but a grid-in-a-grid. My lame cop-out here is to just build your other grid-layout with the app and nest the html within the parent-layout‚Äôs exported code.\nNo mobile mode: If you want to change the layout for your app on mobile you basically have to build it twice and then use media queries (or the more gridy options like repeat(autofit,...))."
  },
  {
    "objectID": "posts/2017-09-24-simulated-annealing/index.html",
    "href": "posts/2017-09-24-simulated-annealing/index.html",
    "title": "The traveling metallurgist",
    "section": "",
    "text": "TLDR: I made a thing that moves and has sliders and stuff. It‚Äôs down here.\nCurrently I am taking a class titled ‚ÄúAdvanced Statistical Computing‚Äù taught here at Vanderbilt by Chris Fonnesbeck. The class is a fantastic whirlwind tour so far of some common optimization algorithms used in statistical computing. One of the algorithms I have found particularly fascinating is the ‚Äúsimulated annealing‚Äù algorithm. ‚äïThe whole algorithm is explained much better than I can do in the freely available Jupyter notebook for the lecture. The inspiration for simulated annealing is from, unsurprisingly, annealing. Which is the act of heating up a metal and then slowly cooling it down to allow the atoms inside to find their optimal arrangement for strength before being locked into place. ‚äï Why annealing is valuable in metullurgy. Source.\nSimulated annealing attempts to replicate this process but for optimizing some loss function. A loss function in this context is any measurable outcome/performance of a model. For instance this could be the performance of a model via AIC, or the log-likelihood, or it could be the number of mistakes a robot makes in navigating a maze. The types of problems simulated-annealing is typically used for are those who have so many possible solutions that brute force figuring out the solution would be essentially impossible.\nHow the algorithm works in a very broad sense is that a starting state is chosen (often at random) and the performance is measured via your loss function of choice. After this, the solution (such as parameters included in your model) is mutated randomly and the performance of the new mutated model is calculated. If it is better than your previous solution, you ‚Äúaccept‚Äù the new solution. We could stop right here and have a good method for optimization, the problem is that this will have a tendency to get stuck in a local minimum or a solution that is better than any solution ‚Äòclose by‚Äô or within range of a single mutation, but not necessarily the best overall solution. ‚äï Source What simulated annealing does to combat this is it will sometimes accept new permutations of the solution that perform worse than the previous. By doing this it explores the solution space and has the potential to work its way out of those pesky local minimum. Again though, there is a problem, if we always are willing to accept worse solution, how will we ever narrow down and find the best solution once we do get nearby the true global minimum? This is where the inspiration really takes hold. What the algorithm does is it slowly, as the generations of model permutation progress, gets more and more conservative with the solutions it accepts. For instance, at the beginning almost any solution will be accepted, causing the algorithm to explore a large portion of the solution space, but then it slowly dials back this willingness to bounce around with the hopes that it has bounced into the vicinity of the true global minimum. The way that this willingness to bounce is controlled is called the ‚Äòtemperature.‚Äô"
  },
  {
    "objectID": "posts/2017-09-24-simulated-annealing/index.html#setting-an-itinerary",
    "href": "posts/2017-09-24-simulated-annealing/index.html#setting-an-itinerary",
    "title": "The traveling metallurgist",
    "section": "Setting an Itinerary",
    "text": "Setting an Itinerary\nOne of the classic problems that is infeasible to compute exactly is the traveling salesman problem. A salesman has a list of cities he needs to visit to do salesman things in, but he wants to minimize the amount of time he spends traveling between all the cities so he sets out trying to figure out the optimal route that will minimize his traveling distance. Two cities is easy, there‚Äôs just one path so no choice, adding another city is also fine, there‚Äôs only two ways to do this, 1,2,3,1, or 1,3,2,1 (he lives in city 1 so he always starts and returns there.) Even four cities isn‚Äôt bad, you just throw the fourth city into one of three slots in the previous paths ({1,-,-,4,1}, {1,-,4,-,1}, {1,4,-,-,1}) for a total of 6 possible routes. Five cities starts to get a little bit trickier, you have to add the fifth city into one of 4 possible slots in the six previous routes, this continues such that if you have \\(n\\) cities and add an \\((n + 1)^{th}\\) city, the possible number of routes you have to consider is \\(n!\\), which is a large number past 5. ‚äï\\(25! = 1.55 * 10^{25}\\) Even though our computers are super duper fast, they can‚Äôt do that many calculations in the human (or universe‚Äôs) lifespan.\nSimulated annealing is a way of solving this problem. ‚äïThe best ways tend to approach it as a graph problem, but this works too Essentially what you can do is start with a random route that passes through every city, then randomly choose a given city in the route path and move it. So say your route was 1,2,3, 4 ,5, you could take city 4 and randomly place it somewhere else to get a new route such as 1, 4, 2, 3, 5. The loss function in this case is total distance traveled.\nThis is all fun and easy to write/read about, but I find these algorithms make more sense when I can get in and watch them work and fiddle with them, which is why I have made an interactive that allows you to watch simulated annealing try and solve the traveling salesman problem.\n\n\n\n\n\n‚äïClick to add or remove cities. Use slider to take manual control of the temperature schedule. To slow down the visualization increase the step delay.\nIf you prefer you can just let it run and it should (but not always) do a good job of finding a sensible route through the randomly generated cities, but I encourage you to poke and prod it and ask questions. Here are just a few that I‚Äôve asked myself while building it:\n\nCan you fool the algorithm by adding (click or tap anywhere without a city) or deleting cities (click or tap on an existing city)?\nTry deleting cities until you have a perimeter, or an easy to solve problem, and then see if you can fool the algorithm by putting a city smack in the middle.\nCan you come up with a cooling routine that allows the algorithm to converge faster than the default (exponential) one?\nThe cooling function will auto-run to cool down exponentially, but you can take control of it.\n\nWhat happens if you pulse the temperature?\nWhat is the relationship between a decrease in the cooling rate and the model performance?\nShould the model try and flip two cities each generation or one, or five?\nWhy would increasing the number of flips each generation not necessarily make the model converge faster?\nWhen might it be a good decision?"
  },
  {
    "objectID": "posts/2023-04-27-its-just-a-linear-model-neural-nets/index.html",
    "href": "posts/2023-04-27-its-just-a-linear-model-neural-nets/index.html",
    "title": "It‚Äôs just a linear model: neural networks edition",
    "section": "",
    "text": "I created a little Shiny application to demonstrate that Neural Networks are just souped up linear models: https://lucy.shinyapps.io/neural-net-linear/\n\nThis application has a neural network fit to a dataset with one predictor, x, and one outcome, y. The network has one hidden layer with three activations. You can click a ‚ÄúPlay‚Äù button to watch how the neural network fits across 300 epochs. You can also click on the nodes of the neural network diagram to highlight each of the individual activations across the applciation (along with their corresponding coefficients)."
  },
  {
    "objectID": "posts/2017-01-25-catterplot-thank-you-note/index.html",
    "href": "posts/2017-01-25-catterplot-thank-you-note/index.html",
    "title": "CatterPlot thank you note",
    "section": "",
    "text": "Lara Harmon has put in countless hours to build and uplift the ASA Student community. We are SO grateful. We know she loves cats, as evidenced by\n\n\n\n\n\nOn the off-chance you have not felt judged by a cat today, my cat can help you out. pic.twitter.com/sApgbyYztD\n\n‚Äî Lara Harmon (@Amstat_Lara) December 27, 2016\n\n\nand\n\n\nYou'll notice that we have yet to be in a city where I can convince our design team cats would be 100% on-theme. /life goals\n\n‚Äî Lara Harmon (@Amstat_Lara) January 24, 2017\n\n\nand\n\n\nStats Cat's back from her first business trip. Looks about as jetlagged as I am! #JMM17 pic.twitter.com/JLzSbMKj3I\n\n‚Äî Lara Harmon (@Amstat_Lara) January 10, 2017\n\n\nso we decided to write her an appropriate thank you note, using CatterPlots.\n\n\nIf you haven‚Äôt installed this purrrrfect package, you can install it from GitHub.\n\ndevtools::install_github(\"Gibbsdavidl/CatterPlots\")\n\n\nlibrary(CatterPlots)\n\n\n\nStack Overflow taught me how to plot a heart in R (perhaps they will be the subject of my next thank you note)‚Äú)\n\nt &lt;- seq(0, 2 * pi, by = 0.1)\nxhrt &lt;- function(t) {\n  16 * sin(t) ^ 3\n}\nyhrt &lt;- function(t) {\n  13 * cos(t) - 5 * cos(2 * t) - 2 * cos(3 * t) - cos(4 * t)\n}\ny &lt;- yhrt(t)\nx &lt;- xhrt(t)\n\nThank you, Lara! We appreciate you!\n\nmeow &lt;- multicat(xs = x, ys = y,\n                 cat = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),\n                 canvas = c(-0.1, 1.1, -0.1, 1.1),\n                 xlab = \"some cats\",\n                 ylab = \"more cats\",\n                 main = \"Thank you, Lara!\")"
  },
  {
    "objectID": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html",
    "href": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html",
    "title": "Would seeing Spider-Man: No Way Home decrease COVID-19 Cases?",
    "section": "",
    "text": "In SNL‚Äôs cold open last night, ‚ÄúPresident Joe Biden‚Äù suggested that the COVID-19 surge we are seeing in the US is due to people seeing Spider-Man: No Way Home. If people would just stop seeing this film, he argues, cases will go back down! Interesting hypothesis, let‚Äôs take a looksy at the data, shall we?\nI pulled the domestic box office data from the-numbers.com and put them in spiderman.csv (if you want to code along, you can do the same! My .csv has two columns: date and domestic_box_office) and US COVID-19 cases from the NY Times GitHub repository. Here‚Äôs a little code-along.\nlibrary(tidyverse)\n\nspiderman &lt;- read_csv(\"spiderman.csv\",\n                      col_types = cols(\n                        date = col_date(format = \"%m/%d/%y\"),\n                        domestic_box_office = col_double()\n                      ))\ncases &lt;- read_csv(\"https://github.com/nytimes/covid-19-data/raw/master/us.csv\")\n\n# get daily cases from cumulative cases\ncases &lt;- cases %&gt;%\n  mutate(case = c(cases[1], diff(cases)))\n\nd &lt;- spiderman %&gt;%\n  left_join(cases)\nNow, let‚Äôs look at the relationship between daily domestic box office and new COVID-19 cases in the US.\nlm(case ~ domestic_box_office, data = d)\n\n\nCall:\nlm(formula = case ~ domestic_box_office, data = d)\n\nCoefficients:\n        (Intercept)  domestic_box_office  \n          3.559e+05           -3.691e-03\nOh my heavens! Not only does it not look like ticket sales for Spider-Man: No Way Home are increasing COVID-19 cases‚Ä¶it looks like the opposite. According to our very silly model, every $1,000 dollar increase in domestic box office sales results in a decrease of 3-4 COVID-19 cases! Pandemic solved! We just need to buy more Spider-Man: No Way Home tickets!! Since we‚Äôre looking at a silly analysis, let‚Äôs do something horrible, shall we? A double y-axis!\nlibrary(geomtextpath)\n\n\nd %&gt;%\n  pivot_longer(cols = c(domestic_box_office, case)) %&gt;%\n  filter(!is.na(value)) %&gt;%\n  mutate(value = ifelse(name == \"case\", value * 120, value),\n         name = ifelse(name == \"domestic_box_office\", \n                       \"Spiderman daily domestic box office\", \n                       \"US daily COVID-19 cases\")) %&gt;%\n  ggplot(aes(x = date, y = value, color = name)) +\n  geom_point(alpha = 0.2) +\n  geom_textsmooth(aes(label = name), hjust = 0.05,\n                  method = \"loess\", formula = y ~ x) +\n  scale_y_continuous(\"Spiderman Daily Domestic Box Office\", \n                     label = scales::dollar,\n                     sec.axis = sec_axis(\n                       ~ . / 120, \n                       label = scales::comma, \n                       name = \"Daily new COVID-19 cases in the US\")\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(caption = \"@LucyStats\")  +\n  scale_x_date(\"Date\", limits = c(as.Date(\"2021-12-07\"), NA))\nThe actual SNL sketch claimed there was a one week lag ‚Äì that Spider-Man sales a week ago caused cases 7 days later. Let‚Äôs check that too!\ncases &lt;- cases %&gt;%\n  mutate(case_lag = lead(case, 7)) \n\nspiderman %&gt;%\n  left_join(cases) %&gt;%\n  pivot_longer(cols = c(domestic_box_office, case_lag)) %&gt;%\n  filter(!is.na(value)) %&gt;%\n  mutate(value = ifelse(name == \"case_lag\", value * 120, value),\n         name = ifelse(name == \"domestic_box_office\", \n                       \"Spiderman daily domestic box office\", \n                       \"US daily COVID-19 cases (7 days later)\")) %&gt;%\n  ggplot(aes(x = date, y = value, color = name)) +\n  geom_point(alpha = 0.2) +\n  geom_textsmooth(aes(label = name), hjust = .95,\n                  method = \"loess\", formula = y ~ x) +\n  scale_y_continuous(\"Spiderman Daily Domestic Box Office\", \n                     label = scales::dollar,\n                     sec.axis = sec_axis(\n                       ~ . / 120, \n                       label = scales::comma, \n                       name = \"Daily new COVID-19 cases in the US (7 days later)\")\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(caption = \"@LucyStats\")  +\n  scale_x_date(\"Date\", limits = c(as.Date(\"2021-12-07\"), NA))\nWell there you have it. It looks like SNL was onto something, but got it backwards! Just kidding.\nSo new claim! a higher Spider-Man: No Way Home domestic box office would lead to fewer COVID-19 cases. Let‚Äôs talk about how we can think about this claim from a causal perspective if it was presented as I have done here. There are lots of ways to introduce causal thinking; in my class, one of the first ways I like to get my students thinking about causal assumptions is via Hill‚Äôs Criteria. These are 9 things to think about when trying to assess whether an association is causal; it is not meant to be a checklist (and the list is not exhaustive!) but I find it is a nice way to dip your toe into causal thinking."
  },
  {
    "objectID": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#strength",
    "href": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#strength",
    "title": "Would seeing Spider-Man: No Way Home decrease COVID-19 Cases?",
    "section": "Strength",
    "text": "Strength\nThe first criteria is what is the strength of the association. Our goofy linear model suggested that 16% of the variability in the number of daily cases in the US could be explained by the Spider-Man: No Way Home domestic box office. Is this strong? The coefficient in our linear model suggests that every $1,000 dollar increase in domestic box office sales results in a decrease of 3-4 COVID-19 cases! That sounds pretty strong? I‚Äôm not sure this criteria is helping me assess whether this relationship is causal so I‚Äôm going to move on to the next one and rate strength as ü§∑."
  },
  {
    "objectID": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#consistency",
    "href": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#consistency",
    "title": "Would seeing Spider-Man: No Way Home decrease COVID-19 Cases?",
    "section": "Consistency",
    "text": "Consistency\nHave other studies shown a similar association? I‚Äôm going to say no‚Ä¶consistency is going to get rated as üëé."
  },
  {
    "objectID": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#specificity",
    "href": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#specificity",
    "title": "Would seeing Spider-Man: No Way Home decrease COVID-19 Cases?",
    "section": "Specificity",
    "text": "Specificity\nSpecificity means the exposure of interest (in this case Spider-Man domestic box office) only causes one thing (and that thing is a decrease in COVID-19 cases in this case). I‚Äôm going to go with no again. üëé."
  },
  {
    "objectID": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#temporality",
    "href": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#temporality",
    "title": "Would seeing Spider-Man: No Way Home decrease COVID-19 Cases?",
    "section": "Temporality",
    "text": "Temporality\nDoes the cause precede the effect? Kind of? It looks like sales are decreasing rapidly from Dec 7th through Dec 20 and cases appear to dramatically increase thereafter? But wait! Could the opposite be true? What if the surge in COVID-19 cases is causing a decrease in box office sales? (Acutally there may be some merit to this, but also we can just look at box office trends and see that they tend to decrease over time since lots of excited people go the first weekend and fewer as time goes on but I DIGRESS!) I‚Äôm going to give this a ü§∑."
  },
  {
    "objectID": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#biological-gradient",
    "href": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#biological-gradient",
    "title": "Would seeing Spider-Man: No Way Home decrease COVID-19 Cases?",
    "section": "Biological gradient",
    "text": "Biological gradient\nDo we see a dose effect? Indeed we do! As domestic box office decreases, COVID-19 cases increase, so can we infer the opposite, if domestic box office sales were to increase, would we see fewer COVID-19 cases? From our little window of data it looks like we can‚Äôt know (but reality check, if we had started the analysis from the day before, we actually have box office going from 0 to $121,964,712 and we don‚Äôt see a precipitous drop in COVID-19 cases, so I‚Äôm going to give this one a üëé)."
  },
  {
    "objectID": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#plausibility",
    "href": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#plausibility",
    "title": "Would seeing Spider-Man: No Way Home decrease COVID-19 Cases?",
    "section": "Plausibility",
    "text": "Plausibility\nIs this plausible? No, it‚Äôs silly. üëé"
  },
  {
    "objectID": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#coherence",
    "href": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#coherence",
    "title": "Would seeing Spider-Man: No Way Home decrease COVID-19 Cases?",
    "section": "Coherence",
    "text": "Coherence\nIs there a coherent argument to be made? Again, no. üëé"
  },
  {
    "objectID": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#experiment",
    "href": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#experiment",
    "title": "Would seeing Spider-Man: No Way Home decrease COVID-19 Cases?",
    "section": "Experiment",
    "text": "Experiment\nWas there a randomized trial // some attempt at estimating a causal effect to explore this relationship? Nope. üëé"
  },
  {
    "objectID": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#analogy",
    "href": "posts/2022-01-16-would-seeing-spider-man-no-way-home-decrease-covid19-cases/index.html#analogy",
    "title": "Would seeing Spider-Man: No Way Home decrease COVID-19 Cases?",
    "section": "Analogy",
    "text": "Analogy\nIs there strong evidence for a similar exposure (something like Spider-Man box office sales) to cause a similar outcome (a decrease in COVID-19 cases in the US)? Nope. üëé\nOk, so using these criteria, let‚Äôs assess the overall likelihood of a causal effect:\nStrength ü§∑\nConsistency üëé\nSpecificity üëé\nTemporality ü§∑\nBiological gradient üëé\nPlausibility üëé\nCoherence üëé\nExperiment üëé\nAnalogy üëé\nI‚Äôm going to go ahead and rate this one üëé not likely causal! Sorry! Looks like increasing box office sales of Spider-Man: No Way Home is not likely to get us out of the pandemic."
  },
  {
    "objectID": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html",
    "href": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html",
    "title": "A year as told by fitbit",
    "section": "",
    "text": "I managed to wear a fitbit the entirety of 2017, this is exciting for a few reasons: one I have commitment problems, and two: it‚Äôs a lot of data that I have to play with. While fitbit‚Äôs app has some nice pretty graphs, they make it rather hard to actually dump all of your data into something nice like a csv.\nOver the summer I worked on a project with Jeff Leek at the Johns Hopkins Data Science Lab to crack open this fitbit data resource and get out nice tidy csvs of your data. While we have yet to fully release what we worked on I figured this post would be a demonstration of what one can do with the data from their fitbit and perhaps inspire others to explore their own data."
  },
  {
    "objectID": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#loading-the-data",
    "href": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#loading-the-data",
    "title": "A year as told by fitbit",
    "section": "Loading the data",
    "text": "Loading the data\nI dumped out six csv‚Äôs of my data because the fitbit API has a 150 query per hour limit, so I would go in and grab two months worth of heart rate and steps \\(30 * 2 * 2 = 120\\) in a go, drink some coffee and repeat. ‚äïIf I was smart I would have written a script that waited this time for me, but, I was already three timepoints in before realizing I should have and at that point premature optimization is the root of all evil\n\ndata_path &lt;- function(months) paste0('../../media/fitbit_year/my_fitbit_data_', months, '.csv')\nfitbit_data &lt;- read_csv(data_path('nov-dec')) %&gt;% \n  bind_rows(read_csv(data_path('sep-oct'))) %&gt;% \n  bind_rows(read_csv(data_path('july-aug'))) %&gt;% \n  bind_rows(read_csv(data_path('may-june'))) %&gt;% \n  bind_rows(read_csv(data_path('march-april'))) %&gt;% \n  bind_rows(read_csv(data_path('jan-feb'))) %&gt;% \n  select(-1) %&gt;% \n  mutate(\n    day_of_week = lubridate::wday(date),\n    week_of_year = lubridate::week(date)\n  )\n\nfitbit_data %&gt;% \n  head() %&gt;% \n  knitr::kable()\n\n\n\n\ntime\nvalue\ntype\ndate\nday_of_week\nweek_of_year\n\n\n\n\n0\n57\nheart rate\n2017-12-26\n3\n52\n\n\n60\n56\nheart rate\n2017-12-26\n3\n52\n\n\n120\n57\nheart rate\n2017-12-26\n3\n52\n\n\n180\n57\nheart rate\n2017-12-26\n3\n52\n\n\n240\n56\nheart rate\n2017-12-26\n3\n52\n\n\n300\n57\nheart rate\n2017-12-26\n3\n52\n\n\n\n\n\nThe data we are looking to visualize has come to us in a tidy format, with a time column representing seconds into the day (date), and a numeric value corresponding to the measurement of a given type.\nLet‚Äôs first look at some brief summaries of the data to make sure nothing is wrong.\n\nfitbit_data %&gt;% \n  group_by(type) %&gt;% \n  summarise(\n    total = sum(value),\n    minimum = min(value),\n    mean = mean(value),\n    median = median(value),\n    maximum = max(value),\n    max_time = max(time)\n  ) %&gt;% \n  knitr::kable(digits = 3)\n\n\n\n\ntype\ntotal\nminimum\nmean\nmedian\nmaximum\nmax_time\n\n\n\n\nheart rate\n35396998\n35\n73.300\n69\n192\n86340\n\n\nsteps\n3255136\n0\n6.287\n0\n222\n86340\n\n\n\n\n\nSo from this simple summary we can see that our time column does appear to be what it should, starting at 0 and ending at \\(60*60*(24 - 1) = 86340\\), or seemingly every minute of a given day. Mean heart rate is 73 and mean steps per minute is 6. Median heart rate is 69 bpm and median steps are zero, or more than half the time I‚Äôm not moving at all.\nLet‚Äôs take a quick look at the distributions of the contiguous variables to get a better picture than a simple mean and median can show.\n\nannotations &lt;- data_frame(\n  x = c(115, 165),\n  y = c(0.01,0.007),\n  label = c('walking pace', 'running pace'),\n  type = c('steps', 'steps')\n)\n\nfitbit_data %&gt;% \n  ggplot(aes(value)) + \n  geom_density(fill = 'orangered', color = 'orangered') +\n  geom_text(data = annotations, aes(x = x, y = y, label = label), angle = -30, hjust = 1) +\n  facet_grid(.~type, scales = 'free_x') + \n  labs(title = 'Density of per-minute values for heart beats and steps over two months',\n       subtitle = 'gathered from fitbit activity tracker')\n\n\n\n\nInterestingly the distribution for heart rate is semi-heavily right skewed. This is most likely due to the fact that most of the time when I am sleeping my heart rate will be in the lower part of the range, where as the rest of the day is more spread out from workout highs in the 160-180 range and sitting in the 60 range.\nFor the steps some more cool stuff pops out. For one, you can see I am very lazy, with most of the time I am not moving (steps per min = 0), but then we have a small body of low step numbers (0 - 50). These are most likely when I walk for only part of the minute to grab coffee or something. Much more interesting though is the two little bumps that occur further down the range. ‚äïOne potentially cool thing to do with these data is implement an expectation maximization algorithm to identify the different step modalities present in one‚Äôs day to day existance. The first and larger one is right around 115, this is my walking/ hiking pace. This spike is populated by the times that I am walking contiguously for a the full minute measured. The next spike is right at around 165 (I know this because I run to music at 165 bpm). This is populated by the times that I am running contiguously."
  },
  {
    "objectID": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#a-years-heart-rate-in-one-plot",
    "href": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#a-years-heart-rate-in-one-plot",
    "title": "A year as told by fitbit",
    "section": "A year‚Äôs heart rate in one plot",
    "text": "A year‚Äôs heart rate in one plot\nNext, we can use a raster plot to plot all 500,000 heart rate measurements for the entire year at once. Since, sometimes the heart rate is missing for a minute or I took the fitbit off I have filled in the missing minutes with my median heart rate for the entire year.\n\nannotations &lt;- data_frame(\n  rotation = c(0, 0, 0, 0, 90),\n  x = c(4, 5.9, 5.7, 5.15, 17) * 3600,\n  y = as.Date(c(\"2017-07-15\", \"2017-02-05\", \"2017-04-20\", \"2017-11-10\", \"2017-11-01\")),\n  label = c(\"useR in Brussles\", \"alarm switch to 6am\", \"alarm switch to 5:45\", \"alarm switch to 5:15\", \"Flurry of biking to and from work\")\n)\n\nfitbit_data %&gt;%\n  select(-day_of_week, -week_of_year) %&gt;%\n  spread(type, value) %&gt;%\n  mutate(`heart rate` = ifelse(is.na(`heart rate`), median(`heart rate`, na.rm = T), `heart rate`)) %&gt;%\n  gather(type, value, -time, -date) %&gt;%\n  filter(type == \"heart rate\") %&gt;%\n  ggplot(aes(x = time, y = date)) +\n  geom_raster(aes(fill = value)) +\n  geom_text(data = annotations, \n            aes(x = x, y = y, label = label, angle = rotation), \n            hjust = 1, size = 3) +\n  scale_fill_gradientn(guide = \"legend\", colours = brewer.pal(n = 9, name = \"Oranges\")) +\n  theme_minimal() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  scale_y_date(date_labels = \"%b\", expand = c(0, 0)) +\n  scale_x_continuous(breaks = c(6, 12, 18) * 3600, labels = c(\"6am\", \"noon\", \"6pm\"), expand = c(0, 0)) +\n  labs(title = \"Heart rate by minute January 1 to December 27, 2017\")\n\n\n\n\nA few things pop out immediately here. First, you can see me slowly inching my alarm earlier as the year goes on. It started at around 6:30 and I managed to get it down to 5:15 by the end of the year. You can see the weekends as breaks in the hard alarm-time.\nThere is a general band of activity that happens just after noon and then another just after six. This usually corresponds to lunch and then leaving school for the day. Of course all my diligence for alarm times goes to heck when the useR 2017 happened in Brussels."
  },
  {
    "objectID": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#exercise-patterns-throughout-the-year",
    "href": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#exercise-patterns-throughout-the-year",
    "title": "A year as told by fitbit",
    "section": "Exercise patterns throughout the year",
    "text": "Exercise patterns throughout the year\nGenerally I workout sometime between 6 and 9 am Monday through Friday, let‚Äôs see if we can observe this in the data. To do so we will restrict our data to be between 6 and 8:30 am and only look at the weekdays (since I don‚Äôt go to the gym on the weekends.)\nThis time we will leave in the step data and use a line plot for heart rate. We will be pushing the amount of data we should display this way but it may be interesting.\n\nbetween_six_nine &lt;- function(time) time &gt; 6*60*60 & time &lt; 8.5*60*60\nis_weekday &lt;- function(day_of_week) day_of_week %in% 2:6\n\nfitbit_data %&gt;% \n  filter(between_six_nine(time) & is_weekday(day_of_week)) %&gt;% \n  spread(type, value) %&gt;% \n  mutate(day_of_week = factor(day_of_week, \n    labels = c('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'),\n    levels = 2:6)) %&gt;% \n  ggplot(aes(x = time)) +\n  geom_bar(aes(y = steps), color = 'orangered', alpha = 0.3, stat = 'identity') + \n  geom_line(aes(y = `heart rate`), color = 'steelblue', size = 0.8) + \n  facet_grid(week_of_year ~ day_of_week) +\n  theme_void() \n\n\n\n\nYou can see when I switched my run days (seen as the large red blocks of steps) from Wednesday to Tuesday and Thursday. In addition, if you look closely you can see the distinct pattern of my steps to the gym from my car.\nAnother type of workout my girlfriend and I like to do is hikes on the weekends. We started doing this much more frequently as the year went on. Again let‚Äôs subset our data to see if we can observe this.\n\nbetween_seven_two &lt;- function(time) time &gt; 7*60*60 & time &lt; 14*60*60\n\nfitbit_data %&gt;% \n  filter(between_seven_two(time) & !(is_weekday(day_of_week))) %&gt;% \n  spread(type, value) %&gt;% \n  mutate(\n  day_of_week = factor(day_of_week, \n    labels = c('Saturday', 'Sunday'),\n    levels = c(7,1))) %&gt;% \n  ggplot(aes(x = time)) +\n  geom_bar(aes(y = steps), color = 'orangered', alpha = 0.3, stat = 'identity') + \n  geom_line(aes(y = `heart rate`), color = 'steelblue', size = 0.8) + \n  facet_grid(week_of_year ~ day_of_week) +\n  theme_void()\n\n\n\n\nYou can see that in the later half of the year we started doing hikes much more frequently. ‚äï In addition, in week 37 we did a hike at the fiery gizzard trail in South Tennessee."
  },
  {
    "objectID": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#did-i-get-fitter-over-the-year",
    "href": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#did-i-get-fitter-over-the-year",
    "title": "A year as told by fitbit",
    "section": "Did I get fitter over the year?",
    "text": "Did I get fitter over the year?\nLooking at the path of median or mean heart rate over a year really only tells part of the story for fitness. For instance, if you exercised a lot you would have a higher average (and potentially median) heart rate but may have a lower resting heart rate. This is a great instance where visual inspection of a distribution can give a better picture than any statistical summary could. Ideally what I would like to see is the development of a bi modal distribution with the larger and lower modality getting lower (resting heart rate is decreasing) and a smaller higher bump getting larger as the year goes on (I‚Äôm exercising more).\nTo look at this we will use the wonderful ggridges package to look at the distribution of heart rate readings over the year by week.\n\nlibrary(ggridges)\nfitbit_data %&gt;% \n  mutate(`week of year` = fct_rev(as.factor(week_of_year))) %&gt;% \n  filter(type == 'heart rate') %&gt;% \n  ggplot(aes(x= value, y = `week of year`)) +\n  geom_density_ridges(scale = 2.5) +\n  xlim(30, 175) +\n  theme_ridges()\n\n\n\n\nSo, I do see a slight decrease in the lower level and perhaps a bit more bimodality happening as the year goes on. I added more cardiovascular workouts in the later half of the year which explains part of this. Still though, it would be nice if the signal was a little stronger. Oh well, next year."
  },
  {
    "objectID": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#what-day-of-the-week-was-my-most-most-active",
    "href": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#what-day-of-the-week-was-my-most-most-active",
    "title": "A year as told by fitbit",
    "section": "What day of the week was my most most active?",
    "text": "What day of the week was my most most active?\n\n\nstep_counts &lt;- fitbit_data %&gt;% \n  filter(type == 'steps') %&gt;% \n  group_by(day_of_week) %&gt;% \n  summarise(\n    type = last(type), \n    avg_num_steps = sprintf('avg num steps: %3.0f', sum(value)/52)\n  )\n\nlibrary(ggridges)\nfitbit_data %&gt;% \n  mutate(\n  `day of week` = fct_rev(factor(day_of_week, \n    labels = c('Sunday', 'Monday', 'Tuesday', 'Wednesday', \n               'Thursday', 'Friday', 'Saturday'),\n    levels = 1:7))) %&gt;% \n  ggplot(aes(x= value, y = `day of week`)) +\n  geom_density_ridges(scale = 2.5) +\n  geom_text(data = step_counts, nudge_y = 0.15, hjust = 0, \n            aes(x = 85, y = day_of_week, label = avg_num_steps)) +\n  facet_grid(.~type, scales = \"free\") +\n  theme_ridges() + \n  labs(x = '')\n\n\n\n\nSo in terms of steps, there is no noticeably difference noticeably changes, however, we do see that both Saturday and Sunday have by far the most unimodal (see lazy) distribution of heart rates. Thursday also surprisingly has a rather unimodal shape. Usually this is a day that I would do lower intensity cardio work, perhaps I need to up the intensity a bit."
  },
  {
    "objectID": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#takeaway",
    "href": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#takeaway",
    "title": "A year as told by fitbit",
    "section": "Takeaway",
    "text": "Takeaway\nIt‚Äôs easy to dismiss the amount of data we gather about ourselves, but by wearing a fitbit for a year I have managed to gather one million observations of data for just heart rate and steps counts. ‚äïEven more if I had pulled at the second level for heart rate.\nThe amount of data that is generated by any one of is is astounding, and it‚Äôs easy to not see any of it. In 2018 one of my goals will be to try and ‚Äòtake back‚Äô this data on myself and explore it for personal gain. For instance, this year, while I worked out much more consistently than in years past, I didn‚Äôt make massive improvements in my fitness levels. This is something that I wouldn‚Äôt notice if I just looked at the limited graphs that fitbit gives you in their app."
  },
  {
    "objectID": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#want-to-do-this-yourself",
    "href": "posts/2017-12-27-a-year-in-fitbit.Rmd/index.html#want-to-do-this-yourself",
    "title": "A year as told by fitbit",
    "section": "Want to do this yourself?",
    "text": "Want to do this yourself?\nWhile this post was rather self-centered the point was more to show others what they can do with their own data. If you own a fitbit and are interested in easily gathering this data for your own activity stay tuned to my and Jeff Leek‚Äôs twitters. We will soon be launching an application that allows you to easily download your own fitbit data and if you want, donate it for research purposes (think DNA Land but for fitbits).\nIf you are a more hands on person, there is a sparsely documented API that you can use at the fitbit dev site. In addition, the underlying library that Jeff and I are using for our application is hosted on the Johns Hopkins Data Science Lab github. ‚äïFair warning: our library has even worse documentation than the fitbit api."
  },
  {
    "objectID": "posts/2017-01-07-dplyr-thank-you-note/index.html",
    "href": "posts/2017-01-07-dplyr-thank-you-note/index.html",
    "title": "dplyr thank you note",
    "section": "",
    "text": "It‚Äôs that post-holiday time of year to write some thank yous! I‚Äôm getting excited to attend rstudio::conf next week, so in that spirit, I have put together a little thank you using dplyr.\n\nlibrary(\"dplyr\")\n\n\ndear &lt;-function(input, you = \"you\") {\n  cat(input, \"\\nDear \", you, \",\", sep = \"\")\n}\n\nthank_you &lt;- function(input, for_the = \"gift\") {\n  cat(input, \"\\n Thank you for the\", for_the)\n}\n\npersonal_touch &lt;- function(input, from_the_heart = \"I hope you have a lovely New Year!\") {\n  cat(input, from_the_heart)\n}\n\nfrom &lt;- function(input, us = \"us\"){\n  cat(input, \"\\n Cheers,\\n\", us)\n}\n\n\n\"A dplyr Thank You Note\" %&gt;%\n  dear(you = \"R Studio\") %&gt;%\n  thank_you(for_the = \"opportunity to attend rstudio::conf.\") %&gt;%\n  personal_touch(from_the_heart = \"We are SO looking forward to learning from and with you all.\") %&gt;%\n  from(us = \"Lucy, Jacquelyn, and Alli\")\n\nA dplyr Thank You Note\nDear R Studio, \n Thank you for the opportunity to attend rstudio::conf. We are SO looking forward to learning from and with you all. \n Cheers,\n Lucy, Jacquelyn, and Alli"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html",
    "href": "posts/2018-04-23-r-release-names/index.html",
    "title": "R release names (Updated)",
    "section": "",
    "text": "I always love discussions about R release names and their origin. I have been working on this list for a while ‚Äì with the release of ‚ÄúShort Summer‚Äù today, I thought it‚Äôd be a good time to post!\nAll of the release names are references to Peanuts strips/films. I think this is just so delightful! A few months ago, I attempted to get permission to embed the strips on our blog, but unfortunately it was denied (unless with a limited audience and password protection r emo::ji(\"woman_shrugging\")), so I‚Äôve just linked to the comics ‚Äì Enjoy!"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#r-devel-unreleased-development-version-unsuffered-consequences",
    "href": "posts/2018-04-23-r-release-names/index.html#r-devel-unreleased-development-version-unsuffered-consequences",
    "title": "R release names (Updated)",
    "section": "r-devel (unreleased development version) Unsuffered Consequences",
    "text": "r-devel (unreleased development version) Unsuffered Consequences\nReference: Peanuts August 17, 1967"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#great-pumpkin",
    "href": "posts/2018-04-23-r-release-names/index.html#great-pumpkin",
    "title": "R release names (Updated)",
    "section": "2.14.0 (2011-10-31) Great Pumpkin",
    "text": "2.14.0 (2011-10-31) Great Pumpkin\nReference: Peanuts October 29, 1973"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#december-snowflakes",
    "href": "posts/2018-04-23-r-release-names/index.html#december-snowflakes",
    "title": "R release names (Updated)",
    "section": "2.14.1 (2011-12-22) December Snowflakes",
    "text": "2.14.1 (2011-12-22) December Snowflakes\n\n\n\n\nReference: A Charlie Brown Christmas\nThis is very close to the Peanuts January 5, 1960, however they mention January snowflakes rather than December. The ‚ÄúDecember Snowflakes‚Äù quote is from A Charlie Brown Christmas."
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#gift-getting-season",
    "href": "posts/2018-04-23-r-release-names/index.html#gift-getting-season",
    "title": "R release names (Updated)",
    "section": "2.14.2 (2012-02-29) Gift-Getting Season",
    "text": "2.14.2 (2012-02-29) Gift-Getting Season\nReference: This is a line Lucy says in the short film It‚Äôs the Easter Beagle, Charlie Brown! ‚Äì referring to Easter as the ‚Äúgift-getting season‚Äù."
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#easter-beagle",
    "href": "posts/2018-04-23-r-release-names/index.html#easter-beagle",
    "title": "R release names (Updated)",
    "section": "2.15.0 (2012-03-30) Easter Beagle",
    "text": "2.15.0 (2012-03-30) Easter Beagle\nReference: Peanuts April 11, 1971\nThis also likely references the It‚Äôs the Easter Beagle, Charlie Brown!."
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#roasted-marshmallows",
    "href": "posts/2018-04-23-r-release-names/index.html#roasted-marshmallows",
    "title": "R release names (Updated)",
    "section": "2.15.1 (2012-06-22) Roasted Marshmallows",
    "text": "2.15.1 (2012-06-22) Roasted Marshmallows\nReference: Peanuts June 6, 1987"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#trick-or-treat",
    "href": "posts/2018-04-23-r-release-names/index.html#trick-or-treat",
    "title": "R release names (Updated)",
    "section": "2.15.2 (2012-10-26) Trick or Treat",
    "text": "2.15.2 (2012-10-26) Trick or Treat\nReference: Peanuts October 31, 1969"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#security-blanket",
    "href": "posts/2018-04-23-r-release-names/index.html#security-blanket",
    "title": "R release names (Updated)",
    "section": "2.15.3 (2013-03-01) Security Blanket",
    "text": "2.15.3 (2013-03-01) Security Blanket\nReference: Peanuts October 23, 1965"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#masked-marvel",
    "href": "posts/2018-04-23-r-release-names/index.html#masked-marvel",
    "title": "R release names (Updated)",
    "section": "3.0.0 (2013-04-03) Masked Marvel",
    "text": "3.0.0 (2013-04-03) Masked Marvel\nReference: Peanuts June 23, 1981\nEdit: Got some insider info from the source himself that this is from Charlie Brown and Snoopy Show!\n\n\n\n \n\nSource MoviesAfterMidnight"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#good-sport",
    "href": "posts/2018-04-23-r-release-names/index.html#good-sport",
    "title": "R release names (Updated)",
    "section": "3.0.1 (2013-05-16) Good Sport",
    "text": "3.0.1 (2013-05-16) Good Sport\nReference: You‚Äôre a Good Sport, Charlie Brown\nThis is likely from the film You‚Äôre a Good Sport Charlie Brown, however there Peanuts November 22, 1953 does refer to being a ‚ÄúGood Sport‚Äù as well!"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#frisbee-sailing",
    "href": "posts/2018-04-23-r-release-names/index.html#frisbee-sailing",
    "title": "R release names (Updated)",
    "section": "3.0.2 (2013-09-25) Frisbee Sailing",
    "text": "3.0.2 (2013-09-25) Frisbee Sailing\nReference: Peanuts September 3, 1971"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#warm-puppy",
    "href": "posts/2018-04-23-r-release-names/index.html#warm-puppy",
    "title": "R release names (Updated)",
    "section": "3.0.3 (2014-03-06) Warm Puppy",
    "text": "3.0.3 (2014-03-06) Warm Puppy\n\n\nThere is also a book titled Happiness is a Warm Puppy.‚Äù)\nReference: Peanuts January 11, 1965"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#spring-dance",
    "href": "posts/2018-04-23-r-release-names/index.html#spring-dance",
    "title": "R release names (Updated)",
    "section": "3.1.0 (2014-04-10) Spring Dance",
    "text": "3.1.0 (2014-04-10) Spring Dance\nReference: Peanuts March 22, 1971\n\n\n   Source ebay"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#sock-it-to-me",
    "href": "posts/2018-04-23-r-release-names/index.html#sock-it-to-me",
    "title": "R release names (Updated)",
    "section": "3.1.1 (2014-07-10) Sock it to Me",
    "text": "3.1.1 (2014-07-10) Sock it to Me\nReference: This seems to be referring to a mini jigsaw puzzle, available on ebay!\n\n\n\n\n Source MoviesAfterMidnight"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#pumpkin-helmet",
    "href": "posts/2018-04-23-r-release-names/index.html#pumpkin-helmet",
    "title": "R release names (Updated)",
    "section": "3.1.2 (2014-10-31) Pumpkin Helmet",
    "text": "3.1.2 (2014-10-31) Pumpkin Helmet\nReference: You‚Äôre a Good Sport, Charlie Brown\nIt‚Äôs a bit later in the clip, around 16:45.\n\n\n  Source Something to be Found‚Äô)"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#smooth-sidewalk",
    "href": "posts/2018-04-23-r-release-names/index.html#smooth-sidewalk",
    "title": "R release names (Updated)",
    "section": "3.1.3 (2015-03-09) Smooth Sidewalk",
    "text": "3.1.3 (2015-03-09) Smooth Sidewalk\nReference: This is a page from the Happiness is a Warm Puppy book."
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#full-of-ingredients",
    "href": "posts/2018-04-23-r-release-names/index.html#full-of-ingredients",
    "title": "R release names (Updated)",
    "section": "3.2.0 (2015-04-16) Full of Ingredients",
    "text": "3.2.0 (2015-04-16) Full of Ingredients\nReference: Peanuts April 7, 1966"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#world-famous-astronaut",
    "href": "posts/2018-04-23-r-release-names/index.html#world-famous-astronaut",
    "title": "R release names (Updated)",
    "section": "3.2.1 (2015-06-18) World-Famous Astronaut",
    "text": "3.2.1 (2015-06-18) World-Famous Astronaut\nReference: Peanuts March 10, 1969\n\n\n Source Business Wire‚Äô)"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#fire-safety",
    "href": "posts/2018-04-23-r-release-names/index.html#fire-safety",
    "title": "R release names (Updated)",
    "section": "3.2.2 (2015-08-14) Fire Safety",
    "text": "3.2.2 (2015-08-14) Fire Safety\nReference: It seems that MetLife created a Peanuts themed Fire Saftey Brochure coloring and activity book."
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#wooden-christmas-tree",
    "href": "posts/2018-04-23-r-release-names/index.html#wooden-christmas-tree",
    "title": "R release names (Updated)",
    "section": "3.2.3 (2015-12-10) Wooden Christmas-Tree",
    "text": "3.2.3 (2015-12-10) Wooden Christmas-Tree\nReference: This is a line from A Charlie Brown Christmas ‚Äì Linus says ‚ÄúGee, I didn‚Äôt know they still made wooden Christmas trees‚Äù."
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#very-secure-dishes",
    "href": "posts/2018-04-23-r-release-names/index.html#very-secure-dishes",
    "title": "R release names (Updated)",
    "section": "3.2.4 (2016-03-11) Very Secure Dishes",
    "text": "3.2.4 (2016-03-11) Very Secure Dishes\nReference: Peanuts February 20, 1964"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#very-very-secure-dishes-a-rebadged-3.2.4-revised",
    "href": "posts/2018-04-23-r-release-names/index.html#very-very-secure-dishes-a-rebadged-3.2.4-revised",
    "title": "R release names (Updated)",
    "section": "3.2.5 (2016-04-11) Very, Very Secure Dishes (a rebadged 3.2.4-revised)",
    "text": "3.2.5 (2016-04-11) Very, Very Secure Dishes (a rebadged 3.2.4-revised)\nI assume this is still a reference to Peanuts February 20, 1964"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#supposedly-educational",
    "href": "posts/2018-04-23-r-release-names/index.html#supposedly-educational",
    "title": "R release names (Updated)",
    "section": "3.3.0 (2016-05-03) Supposedly Educational",
    "text": "3.3.0 (2016-05-03) Supposedly Educational\nReference: Peanuts May 7, 1971"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#bug-in-your-hair",
    "href": "posts/2018-04-23-r-release-names/index.html#bug-in-your-hair",
    "title": "R release names (Updated)",
    "section": "3.3.1 (2016-06-21) Bug in Your Hair",
    "text": "3.3.1 (2016-06-21) Bug in Your Hair\nReference: Peanuts June 15, 1967"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#sincere-pumpkin-patch",
    "href": "posts/2018-04-23-r-release-names/index.html#sincere-pumpkin-patch",
    "title": "R release names (Updated)",
    "section": "3.3.2 (2016-10-31) Sincere Pumpkin Patch",
    "text": "3.3.2 (2016-10-31) Sincere Pumpkin Patch\nReference: Peanuts Oct 30, 1968"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#another-canoe",
    "href": "posts/2018-04-23-r-release-names/index.html#another-canoe",
    "title": "R release names (Updated)",
    "section": "3.3.3 (2017-03-06) Another Canoe",
    "text": "3.3.3 (2017-03-06) Another Canoe\nReference: Peanuts June 29, 1966"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#you-stupid-darkness",
    "href": "posts/2018-04-23-r-release-names/index.html#you-stupid-darkness",
    "title": "R release names (Updated)",
    "section": "3.4.0 (2017-04-21) You Stupid Darkness",
    "text": "3.4.0 (2017-04-21) You Stupid Darkness\nReference: Peanuts September 9, 1965"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#single-candle",
    "href": "posts/2018-04-23-r-release-names/index.html#single-candle",
    "title": "R release names (Updated)",
    "section": "3.4.1 (2017-06-30) Single Candle",
    "text": "3.4.1 (2017-06-30) Single Candle\nReference: Peanuts September 9, 1965"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#short-summer",
    "href": "posts/2018-04-23-r-release-names/index.html#short-summer",
    "title": "R release names (Updated)",
    "section": "3.4.2 (2017-09-28) Short Summer",
    "text": "3.4.2 (2017-09-28) Short Summer\nReference: It was a Short Summer, Charlie Brown"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#kite-eating-tree",
    "href": "posts/2018-04-23-r-release-names/index.html#kite-eating-tree",
    "title": "R release names (Updated)",
    "section": "3.4.3 (2017-11-30) Kite-eating Tree",
    "text": "3.4.3 (2017-11-30) Kite-eating Tree\nReference: Peanuts February 19, 1967\nPeter Dalgaard made a gif to commemorate the momentous occasion!\n\n\n\n Source catawiki auctions‚Äô)"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#someone-to-lean-on",
    "href": "posts/2018-04-23-r-release-names/index.html#someone-to-lean-on",
    "title": "R release names (Updated)",
    "section": "3.4.4 (2018-03-15) Someone to Lean On",
    "text": "3.4.4 (2018-03-15) Someone to Lean On\nReference: Peanuts Figurine (1971). There are a couple of different versions of this, some with Charlie Brown and Snoopy, one with Linus and Snoopy, and one with Woodstock and Snoopy. Many of them were Hallmark cards, but there was also a badge and this figurine. The oldest (dated) one I could find was this one from 1971, so we went with that!\nAn update from Peter!\n\n\nYup. Incidentally, I believe this one was the inspiration for 3.4.4 (also, a discreet epitaph to my stepdad.) As you note, there are variants, but this is graphically the nicest in my opinion. pic.twitter.com/6syXNAJRrC\n\n‚Äî Peter Dalgaard (@pdalgd) July 3, 2018"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#joy-in-playing",
    "href": "posts/2018-04-23-r-release-names/index.html#joy-in-playing",
    "title": "R release names (Updated)",
    "section": "3.5.0 (2018-04-23) Joy in Playing",
    "text": "3.5.0 (2018-04-23) Joy in Playing\nReference: Peanuts January 27, 1973"
  },
  {
    "objectID": "posts/2018-04-23-r-release-names/index.html#feather-spray",
    "href": "posts/2018-04-23-r-release-names/index.html#feather-spray",
    "title": "R release names (Updated)",
    "section": "3.5.1 (2018-07-02) Feather Spray",
    "text": "3.5.1 (2018-07-02) Feather Spray\nReference: Peanuts March 9, 1972\n\nLooks like we have some validation (thanks Peter!) ‚Äì I‚Äôve updated the Masked Marvel accordingly r emo::ji(\"tada\").\n\n\nMostly on target. A few are from films/badges rather than strips. Notably Masked Marvel from the unforgettable arm-wrestling w/Lucy.\n\n‚Äî Peter Dalgaard (@pdalgd) September 28, 2017"
  },
  {
    "objectID": "posts/2022-09-07-tipr-an-r-package-for-sensitivity-to-unmeasured-confounding/index.html",
    "href": "posts/2022-09-07-tipr-an-r-package-for-sensitivity-to-unmeasured-confounding/index.html",
    "title": "tipr: An R package for sensitivity analyses for unmeasured confounding",
    "section": "",
    "text": "The tipr R package has some new features! And a new and improved API!"
  },
  {
    "objectID": "posts/2022-09-07-tipr-an-r-package-for-sensitivity-to-unmeasured-confounding/index.html#what-is-tipr",
    "href": "posts/2022-09-07-tipr-an-r-package-for-sensitivity-to-unmeasured-confounding/index.html#what-is-tipr",
    "title": "tipr: An R package for sensitivity analyses for unmeasured confounding",
    "section": "What is tipr",
    "text": "What is tipr\ntipr is an R package that allows you to conduct sensitivity analyses for unmeasured confounders. Why might you want to do that? Well, as it turns out, the assumption of ‚Äúno unmeasured confounders‚Äù is integral to any estimation of a causal effect. This assumption is untestable, so often the best we can do is examine how far off our estimates would be should an unmeasured confounder exists, hence sensitivity analyses!"
  },
  {
    "objectID": "posts/2022-09-07-tipr-an-r-package-for-sensitivity-to-unmeasured-confounding/index.html#how-do-i-use-tipr",
    "href": "posts/2022-09-07-tipr-an-r-package-for-sensitivity-to-unmeasured-confounding/index.html#how-do-i-use-tipr",
    "title": "tipr: An R package for sensitivity analyses for unmeasured confounding",
    "section": "How do I use tipr",
    "text": "How do I use tipr\nYou can install the CRAN version by running the following:\n\ninstall.packages(\"tipr\")\n\n\nlibrary(tipr)\n\nThe package comes with a few example data sets. For example, the dataframe exdata_rr is simulated data that can be used to estimate the effect of a binary exposure on a binary outcome, estimated via a risk ratio. This data set has 4 columns:\n\nexdata_rr\n\n# A tibble: 2,000 √ó 4\n   .unmeasured_confounder measured_confounder exposure\n                    &lt;dbl&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n 1                0.716                -0.890        0\n 2                0.134                -0.249        0\n 3                0.238                -0.104        0\n 4                0.0286                0.165        0\n 5               -0.598                 0.812        0\n 6                0.00419              -0.563        0\n 7               -0.960                -0.559        0\n 8               -1.15                  1.14         0\n 9               -0.374                -1.23         0\n10                1.80                  0.495        0\n# ‚Ñπ 1,990 more rows\n# ‚Ñπ 1 more variable: outcome &lt;int&gt;\n\n\nUsing this data, we could estimate the exposure-outcome relationship using the measured confounder as follows:\n\nmod &lt;- glm(\n  outcome ~ exposure +  measured_confounder,\n  data = exdata_rr,\n  family = poisson)\n\n## calculate the risk ratio by exponentiating \n## the coefficient\ncoef(mod) %&gt;%\n  exp()\n\n        (Intercept)            exposure measured_confounder \n         0.03656318          1.49477100          2.42566275 \n\n\nWe observe a risk ratio of 1.5 for the exposure after adjusting for the measured confounder. We can then get a confidence interval for this output. Note that here we are using a generalized linear model with a log link (via the Poisson family) to estimate this risk ratio. When estimating the risk ratio using this method, it is important to estimate the variability using robust standard errors (a sandwich estimator). In R, you can use the sandwich and lmtest packages to do this.\n\nlmtest::coefci(mod, vcov = sandwich::vcovHC) %&gt;% \n  exp()\n\n                         2.5 %     97.5 %\n(Intercept)         0.02778992 0.04810614\nexposure            1.10497223 2.02207828\nmeasured_confounder 2.13761161 2.75252986\n\n\nOur observed effect, after adjusting for our measured confounder is a risk ratio of 1.5 (95% CI: 1.1, 2.0).\nLet‚Äôs assume our unmeasured confounder is normally distributed with a mean of 0.5 in the exposed group and 0 in the unexposed (and unit variance in both) resulting in a mean difference of 0.5. We can use this to solve for the relationship between the unmeasured confounder and outcome needed to ‚Äútip‚Äù the analysis (that is needed to make the observed effect, 1.5, cross 1).\nTo do this, we are going to use the tip_with_continuous function. We will set the effect_observed to 1.5 and the exposure_confounder_effect to 0.5.\n\ntip_with_continuous(\n  effect_observed = 1.5,\n  exposure_confounder_effect = 0.5\n)\n\n‚Ñπ The observed effect (1.5) WOULD be tipped by 1 unmeasured\n  confounder with the following specifications:\n‚Ä¢ estimated difference in scaled means between the\n  unmeasured confounder in the exposed population and\n  unexposed population: 0.5\n‚Ä¢ estimated relationship between the unmeasured confounder\n  and the outcome: 2.25\n\n\n# A tibble: 1 √ó 5\n  effect_adjusted effect_observed exposure_confounder_effect\n            &lt;dbl&gt;           &lt;dbl&gt;                      &lt;dbl&gt;\n1               1             1.5                        0.5\n# ‚Ñπ 2 more variables: confounder_outcome_effect &lt;dbl&gt;,\n#   n_unmeasured_confounders &lt;dbl&gt;\n\n\nThe output is a data frame with 5 variables ‚Äì in this case, we are interested in the confounder_outcome_effect column, as this tells us the magnitude of the relationship between an unmeasured confounder and outcome needed to tip this analysis. This results in a confounder-outcome effect of 2.25, meaning that a hypothetical unobserved continuous confounder with a mean difference of 0.5 would need a relationship of at least 2.25 with the outcome to tip the analysis at the point estimate.\nAlternatively, you could look at a range of potential values for the exposure_confounder_effect and plot the relationship.\n\ntip_df &lt;- tip_with_continuous(\n  effect_observed = 1.5,\n  exposure_confounder_effect = seq(0.1, 1, by = 0.1),\n  verbose = FALSE\n)\n\nWe could then plot these results:\n\nlibrary(ggplot2)\n\nggplot(tip_df,\n       aes(x = exposure_confounder_effect, \n           y = confounder_outcome_effect)) + \n  geom_point() + \n  geom_line() + \n  labs(x = \"Exposure - unmeasured confounder effect\",\n       y = \"Unmeasured confounder - outcome effect\")\n\n\n\n\nThe line represents the values needed for the unobserved confounder to tip this relationship.\nSince this data was simulated, we can calculated what the actual effect is.\n\nmod_actual &lt;- glm(\n  outcome ~ exposure + measured_confounder + .unmeasured_confounder,\n  data = exdata_rr,\n  family = poisson)\n\ncoef(mod_actual) %&gt;%\n  exp()\n\n           (Intercept)               exposure \n            0.02450901             0.92108511 \n   measured_confounder .unmeasured_confounder \n            2.43654796             2.41680059 \n\nlmtest::coefci(mod_actual, vcov = sandwich::vcovHC) %&gt;% \n  exp()\n\n                            2.5 %     97.5 %\n(Intercept)            0.01801732 0.03333966\nexposure               0.68914975 1.23107900\nmeasured_confounder    2.12823267 2.78952863\n.unmeasured_confounder 2.10602770 2.77343223\n\n\nThe actual risk ratio is 0.9 (95% CI: 0.7, 1.2) (so null!). The actual relationship between the unmeasured confounder and outcome is 2.4. We can also calculate the actual exposure - unmeasured confounder effect:\n\nexdata_rr %&gt;%\n  dplyr::group_by(exposure) %&gt;%\n  dplyr::summarise(m = mean(.unmeasured_confounder))\n\n# A tibble: 2 √ó 2\n  exposure      m\n     &lt;dbl&gt;  &lt;dbl&gt;\n1        0 0.0438\n2        1 0.547 \n\n\nThe actual difference is 0.5. Returning to our plot, we can see that this point is to the right of the ‚Äútipping‚Äù bound, indicating that this unmeasured confounder is ‚Äúlarge‚Äù enough to tip our result (which is exactly what we saw! Before adjusting for this, we had a risk ratio of 1.5, after adjusting we observe a ‚Äútip‚Äù (crossing the null, 1) to 0.9).\n\nggplot(tip_df,\n       aes(x = exposure_confounder_effect, \n           y = confounder_outcome_effect)) + \n  geom_point() + \n  geom_line() + \n  annotate(\n    \"point\",\n    x = 0.5,\n    y = 2.4,\n    size = 2,\n    shape = \"square\",\n    color = \"red\"\n  ) + \n  labs(x = \"Exposure - unmeasured confounder effect\",\n       y = \"Unmeasured confounder - outcome effect\")"
  },
  {
    "objectID": "posts/2022-09-07-tipr-an-r-package-for-sensitivity-to-unmeasured-confounding/index.html#the-details",
    "href": "posts/2022-09-07-tipr-an-r-package-for-sensitivity-to-unmeasured-confounding/index.html#the-details",
    "title": "tipr: An R package for sensitivity analyses for unmeasured confounding",
    "section": "The details",
    "text": "The details\nThe functions in the tipr package follow a unified grammar. The function names follow this form: {action}_{effect}_with_{what}.\nFor example, to adjust (action) a coefficient (effect) with a binary unmeasured confounder (what), we use the function adjust_coef_with_binary().\nBelow is a copy of the table included in a recent JOSS article about this package.\nTable 1. Grammar of tipr functions.\n\n\n\n\n\n\n\n\ncategory\nFunction term\nUse\n\n\n\n\naction\nadjust\nThese functions adjust observed effects, requiring both the unmeasured | confounder-exposure relationship and unmeasured confounder-outcome relationship to be specified.\n\n\n\ntip\nThese functions tip observed effects. Only one relationship, either the unmeasured confounder-exposure relationship or unmeasured confounder-outcome relationship needs to be specified.\n\n\neffect\ncoef\nThese functions specify an observed coefficient from a linear, log-linear, logistic, or Cox proportional hazards model\n\n\n\nrr\nThese functions specify an observed relative risk\n\n\n\nor\nThese functions specify an observed odds ratio\n\n\n\nhr\nThese functions specify an observed hazard ratio\n\n\nwhat\ncontinuous\nThese functions specify an unmeasured standardized Normally distributed confounder. These functions will include the parameters exposure_confounder_effect and confounder_outcome_effect\n\n\n\nbinary\nThese functions specify an unmeasured binary confounder. These functions will include the parameters exposed_confounder_prev, unexposed_confounder_prev, and confounder_outcome_effect\n\n\n\nr2\nThese functions specify an unmeasured confounder parameterized by specifying the percent of variation in the exposure / outcom explained by the unmeasured confounder. These functions will include the parameters confounder_exposure_r2 and outcome_exposure_r2\n\n\n\nYou can find full documentation here: lucymcgowan.github.io/tipr/"
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html",
    "title": "Using AWK and R to parse 25tb",
    "section": "",
    "text": "How to read this post: I sincerely apologize for how long and rambling the following text is. To speed up skimming of it for those who have better things to do with their time, I have started most sections with a ‚ÄúLesson learned‚Äù blurb that boils down the takeaway from the following text into a sentence or two.\nJust show me the solution! If you just want to see how I ended up solving the task jump to the section Getting More Creative, but I honestly think the failures are more interesting/valuable."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#intro",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#intro",
    "title": "Using AWK and R to parse 25tb",
    "section": "Intro",
    "text": "Intro\nRecently I was put in charge of setting up a workflow for dealing with a large amount of raw DNA sequencing (well technically a SNP chip) data for my lab. The goal was to be able to quickly get data for a given genetic location (called a SNP) for use for modeling etc. Using vanilla R and AWK I was able to cleanup and organize the data in a natural way, massively speeding up the querying. It certainly wasn‚Äôt easy and it took lots of iterations. This post is meant to help others avoid some of the same mistakes and show what did eventually work.\nFirst some background:"
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#the-data",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#the-data",
    "title": "Using AWK and R to parse 25tb",
    "section": "The Data",
    "text": "The Data\nThe data was delivered to us by our university‚Äôs genetics processing center as 25 TB of tsvs. Before handing it off to me, my advisor split and gzipped these files into five batches each composed of roughly 240 four gigabyte files. Each row contained a data for a single SNP for a single person. ‚äïThere were ~2.5 million SNPS and ~60 thousand people Along with the SNP value there were multiple numeric columns on things like intensity of the reading, frequency of different alleles etc. All told there were around 30 columns with frustratingly unique values.\n‚äï How a SNP Chip works. Via Research Gate"
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#the-goal",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#the-goal",
    "title": "Using AWK and R to parse 25tb",
    "section": "The Goal",
    "text": "The Goal\nAs with any data management project the most important thing is to consider how the data will be used. In this case what we will mostly be doing is fitting models and workflows on a SNP by SNP basis. I.e. we only will need a single SNP‚Äôs data at a time. I needed to make it as easy, fast, and cheap as possible to extract all the records pertaining to one of the 2.5 million SNPs."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#the-first-attempt",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#the-first-attempt",
    "title": "Using AWK and R to parse 25tb",
    "section": "The first attempt",
    "text": "The first attempt\nLesson Learned: There‚Äôs no cheap way to parse 25tb of data at once.\nHaving taken a class at Vanderbilt titled ‚ÄòAdvanced methods in Big Data‚Äô I was sure I had this in the bag. ‚äïCapital B capital D Big Data, so you know it‚Äôs serious.It would be maybe an hour or two of me setting up a Hive server to run over all our data and then calling it good. Since our data is stored on AWS S3 I used a service called Athena which allows you to run Hive SQL queries on your S3 data. Not only do you get to avoid setting/ spinning up a Hive cluster, you only pay for the data searched.\nAfter pointing Athena to my data and its format I ran a few tests with queries like\n\nselect * from intensityData limit 10;\n\nand got back results fast and well formed. I was set.\nUntil we tried to use the data in real life‚Ä¶.\nI was asked to grab all the data for a SNP so we could test a model on it. I ran the query:\nselect * from intensityData \nwhere snp = 'rs123456';\n‚Ä¶ and I waited. Eight minutes and 4+ terabytes of data queried later I had my results. Athena charges you by data searched at the reasonable rate of $5 per TB. So this single query cost $20 and eight minutes. ‚äïIf we ever wanted to run a model over all the data we better be ready to wait roughly 38 years and pay $50 million. Clearly this wasn‚Äôt going to work."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#this-should-be-a-walk-in-the-parquet",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#this-should-be-a-walk-in-the-parquet",
    "title": "Using AWK and R to parse 25tb",
    "section": "This should be a walk in the Parquet‚Ä¶",
    "text": "This should be a walk in the Parquet‚Ä¶\nLesson Learned: Be careful with your Parquet file sizes and organization.\nMy first attempt to remedy the situation was to convert all of the TSV‚Äôs to Parquet files. Parquet files are good for working with larger datasets because they store data in a ‚Äòcolumnar‚Äô fashion. Meaning each column is stored in its own section of memory/disk, unlike a text file with lines containing every column. This means to look for something you only have to read the necessary column. Also, they keep a record of the range of values by column for each file so if the value you‚Äôre looking for isn‚Äôt in the column range Spark doesn‚Äôt waste it‚Äôs time scanning through the file.\nI ran a simple AWS Glue job to convert our TSVs to Parquet and hooked up the new Parquet files to Athena. This took only around five hours. However, when I ran a query it took just about the same amount of time and a tiny bit less money. This is because Spark in its attempt to optimize the job just unzipped a single TSV chunk and placed it in its own Parquet chunk. Because each chunk was big enough to contain multiple people‚Äôs full records, this meant that every file had every SNP in them and thus Spark had to open all of them to extract what we wanted.\n‚äïInterestingly the default (and recomended) Parquet compression type: ‚Äòsnappy‚Äô is not splitable. So each executor was still stuck with the task of uncompressing and loading an entire 3.5gig dataset."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#sorting-out-the-issue",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#sorting-out-the-issue",
    "title": "Using AWK and R to parse 25tb",
    "section": "Sorting out the issue",
    "text": "Sorting out the issue\nLesson Learned: Sorting is hard, especially when data is distributed.\nI thought that I had the problem figured out now. All I needed to do was to sort the data on the SNP column instead of the individual. This would allow a given chunk of data to only have a few SNPs in it and Parquet‚Äôs smart only-open-if-values-in-range feature could shine. Unfortunately, sorting billions of rows of data distributed across a cluster is not a trivial task.\n\n\nMe taking algorithms class in college: \"Ugh, no one cares about computational complexity of all these sorting algorithms\" Me trying to sort on a column in a 20TB #Spark table: \"Why is this taking so long?\" #DataScience struggles.\n\n‚Äî Nick Strayer (@NicholasStrayer) March 11, 2019\n\n\n‚äïAWS doesn‚Äôt exactly want to give refunds for the cause ‚ÄòI am an absent minded graduate student.‚Äô\nAfter attempting to run this on Amazon‚Äôs glue it ran for 2 days and then crashed."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#what-about-partitioning",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#what-about-partitioning",
    "title": "Using AWK and R to parse 25tb",
    "section": "What about partitioning?",
    "text": "What about partitioning?\nLesson Learned: Partitions in Spark need to be balanced.\nAnother idea I had was to partition the data into chromosomes. There are 23 of these (plus a few extra to account for mitochondrial DNA or unmapped regions). This would provide a way of cutting down the data into much more manageable chunks. By adding just a single line to the Spark export function in the glue script: partition_by = \"chr\", the data should be put into those buckets.\n‚äï DNA is made up of multiple chunks called Chromosomes. Img via kintalk.org.\nUnfortunately things didn‚Äôt work out well. This is because the chromosomes are different sizes and thus have different amounts of data within them. This meant that the tasks Spark sent out to its workers were unbalanced and ran slowly due to some of the nodes finishing early and sitting idle. The jobs did finish, however. But when querying for a single SNP the unbalance caused problems again. With SNPS in larger chromosomes (aka where we will actually want to get data) the cost was only improved ~10x. A lot but not enough."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#what-about-even-finer-partitioning",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#what-about-even-finer-partitioning",
    "title": "Using AWK and R to parse 25tb",
    "section": "What about even finer partitioning?",
    "text": "What about even finer partitioning?\nLesson Learned: Never, ever, try and make 2.5 million partitions.\nI decided to get crazy with my partitioning and partitioned on each SNP. This guaranteed that each partition would be equal in size. THIS WAS A BAD IDEA. I used Glue and added the innocent line of partition_by = 'snp'. The job started and ran. A day later I checked and noticed nothing had been written to S3 yet so I killed the job. Turns out Glue was writing intermediate files to hidden S3 locations, and a lot of them, like 2 billion. This mistake ended up costing more than a thousand dollars and didn‚Äôt make my advisor happy."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#partitioning-sorting",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#partitioning-sorting",
    "title": "Using AWK and R to parse 25tb",
    "section": "Partitioning + Sorting",
    "text": "Partitioning + Sorting\nLesson Learned: Sorting is still hard and so is tuning Spark.\nThe last attempt in the partitioning era was to partition on chromosome and then sort each partition. In theory this would have made each query quicker because the desired SNP data would only reside in the ranges of a few of the Parquet chunks within a given region. Alas, it turns out sorting even the partitioned data was a lot of work. I ended up switching to EMR for a custom cluster, using 8 powerful instances (C5.4xl) and using Sparklyr to build a more flexible workflow‚Ä¶\n\n# Sparklyr snippet to partition by chr and sort w/in partition\n# Join the raw data with the snp bins\nraw_data\n  group_by(chr) %&gt;%\n  arrange(Position) %&gt;% \n  Spark_write_Parquet(\n    path = DUMP_LOC,\n    mode = 'overwrite',\n    partition_by = c('chr')\n  )\n\n‚Ä¶but no mater what the job never finished. I tried all the tuning tricks: bumped up the memory allocated to each executor of the queries, used high ram node types, broadcasting variables, but it would always get around half way done then executors would slowly start failing till everything eventually ground to a halt.\n\n\nUpdate: so it begins. pic.twitter.com/agY4GU2ru5\n\n‚Äî Nick Strayer (@NicholasStrayer) May 15, 2019"
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#first-attempt-with-spark",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#first-attempt-with-spark",
    "title": "Using AWK and R to parse 25tb",
    "section": "First attempt with Spark",
    "text": "First attempt with Spark\nLesson Learned: Spark joining is fast, but partitioning is still expensive\nThe goal was to read this small (2.5 million row) dataframe into Spark, join it with the raw data, and then partition on the newly added bin column.\n\n# Join the raw data with the snp bins\ndata_w_bin &lt;- raw_data %&gt;%\n  left_join(sdf_broadcast(snp_to_bin), by ='snp_name') %&gt;%\n  group_by(chr_bin) %&gt;%\n  arrange(Position) %&gt;% \n  Spark_write_Parquet(\n    path = DUMP_LOC,\n    mode = 'overwrite',\n    partition_by = c('chr_bin')\n  )\n\n‚äïNotice the use of sdf_broadcast(), this lets Spark know it should send this dataframe to all nodes. It‚Äôs helpful when the data is small and needed for all tasks. Otherwise Spark tries to be clever and waits to distribute it till it needs it which can cause bottlenecks.\nAgain, things didn‚Äôt work out. Like the sorting attempt, the jobs would run for a while, finish the joining task, and then as the partitioning started executors would start crashing."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#bringing-in-awk",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#bringing-in-awk",
    "title": "Using AWK and R to parse 25tb",
    "section": "Bringing in AWK",
    "text": "Bringing in AWK\nLesson Learned: Don‚Äôt sleep on the basics. Someone probably solved your problem in the 80s.\nUp to this point all my Spark failures were due to the data being shuffled around the cluster because it was starting all mixed up. Perhaps I could help it out with some preprocessing. I decided to try and split the raw text data on the chromosome column, that way I would be able to provide Spark with somewhat ‚Äòpre-partitioned‚Äô data.\nI stack overflow searched how to split by column value and found this wonderful answer. Using AWK you can split a text file up by a column‚Äôs values by performing the writing in the script rather than sending results to stdout.\nI wrote up a bash script to test this. I downloaded one of the gzipped tsv, then unzipped it using gzip, piped that to awk.\n\ngzip -dc path/to/chunk/file.gz |\nawk -F '\\t' \\\n'{print $1\",...\"$30\"&gt;\"chunked/\"$chr\"_chr\"$15\".csv\"}'\n\nIt worked!"
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#saturating-the-cores",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#saturating-the-cores",
    "title": "Using AWK and R to parse 25tb",
    "section": "Saturating the cores",
    "text": "Saturating the cores\nLesson Learned: gnu parallel is magic and everyone should use it.\nThe splitting was a tad bit slow and when I ran htop to see the usage of the powerful (expensive) ec2 instance I was using a single core and ~200 MB of ram. If I wanted to get things done and not waste a lot of money I was going to need to figure out how to parallelize. Luckily I found the chapter on parallelizing workflows in Data Science at the Command Line, the utterly fantastic book by Jeroen Janssens. It introduced me to gnu parallel which is very flexible method for spinning up multiple threads in a unix pipeline.\n‚äï\nOnce I ran the splitting using the new GNU parallel workflow it was great, but I was still getting some bottle-necking caused by downloading the S3 objects to disk being a little bit slow and not fully parallelized. I did a few things to fix this.\n‚äïIt was pointed out on twitter by Hyperosonic that I forgot to cite gnu parallel properly as requested by the package. You would think that the number of times I saw the message reminding me to cite that wouldn‚Äôt be possible! Tange, Ole. ‚ÄòGnu parallel-the command-line power tool.‚Äô The USENIX Magazine 36.1 (2011): 42-47.\n\nFound out that you can implement the S3 download step right into the pipeline, completely skipping intermediate disk storage. This meant I could avoid writing the raw data to disk and also use smaller and thus cheaper storage on AWS.\nIncreased the number of threads that the AWS CLI uses to some large number (the default is 10) with aws configure set default.s3.max_concurrent_requests 50.\nSwitched to a network speed optimized ec2 instance. These are the ones with the n in the name. ‚äïI found that the loss in compute power caused from using the ‚Äòn‚Äô instances was more than made up for by the increased download speeds. I used c5n.4xl‚Äôs for most of my stuff.\nSwapped gzip to pigz, which is a parallel gzip tool that does some clever things to parallelize an inherently unparallelizable task of decompressing gziped files. (This helped the least.)\n\n\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n       # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n        \n         # Clean up intermediate data\n       rm chunked/*\ndone\n\nThese steps combined to make things very fast. By virtue of increasing the speed of download and avoiding writing to disk I was now able to process a whole 5 terabyte batch in just a few hours.\n\n\nThere's nothing sweeter than seeing all the cores you're paying for on AWS being used. Thanks to gnu-parallel I can unzip and split a 19gig csv just as fast as I can download it. I couldn't even get Spark to run this. #DataScience #Linux pic.twitter.com/Nqyba2zqEk\n\n‚Äî Nick Strayer (@NicholasStrayer) May 17, 2019\n\n\n‚äïThis tweet should have said ‚Äòtsv‚Äô. Alas."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#using-newly-parsed-data",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#using-newly-parsed-data",
    "title": "Using AWK and R to parse 25tb",
    "section": "Using newly parsed data",
    "text": "Using newly parsed data\nLesson Learned: Spark likes uncompressed data and does not like combining partitions.\nNow that I had the data sitting in an unzipped (see splittable) and semi-organized format on S3 I could go back to Spark. Surprise ‚Äì things didn‚Äôt workout again! It was very hard to accurately tell Spark how the data was partitioned and even when I did it seemed to like to split things into way too many partitions (like 95k), which then when I used coalesce to reduce down to a reasonable number of partitions, ended up ruining the partitioning I had used. ‚äïI am sure there is a way to fix this but I couldn‚Äôt find it over a couple days of looking. I did end up getting things to finish on Spark, it took a while however and my split Parquet files were not super tiny (~200KB) But the data was where it needed to be.\n Too small and uneven, wonderful!"
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#testing-out-local-spark-queries",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#testing-out-local-spark-queries",
    "title": "Using AWK and R to parse 25tb",
    "section": "Testing out local Spark queries",
    "text": "Testing out local Spark queries\nLesson Learned: Spark is a lot of overhead for simple jobs.\nWith the data loaded up into a reasonable format I could test out the speed. I setup an R script to spin up a local Spark server and then load a Spark dataframe from a given Parquet bins location. ‚äïI tried loading all the data but couldn‚Äôt get Sparklyr to recognize the partitioning for some reason.\n\nsc &lt;- Spark_connect(master = \"local\")\n\ndesired_snp &lt;- 'rs34771739'\n\n# Start a timer\nstart_time &lt;- Sys.time()\n\n# Load the desired bin into Spark\nintensity_data &lt;- sc %&gt;% \n  Spark_read_Parquet(\n    name = 'intensity_data', \n    path = get_snp_location(desired_snp),\n    memory = FALSE )\n\n# Subset bin to snp and then collect to local\ntest_subset &lt;- intensity_data %&gt;% \n  filter(SNP_Name == desired_snp) %&gt;% \n  collect()\n\nprint(Sys.time() - start_time)\n\nThis took 29.415 seconds. Much better than before, but still not a great sign for mass testing of anything. In addition, I couldn‚Äôt try and speed it up by enabling caching because when I tried to cache the bin‚Äôs Spark dataframe in memory Spark always crashed, even when I gave it 50+ gigs of memory for a dataset that was at this point smaller than 15."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#back-to-awk",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#back-to-awk",
    "title": "Using AWK and R to parse 25tb",
    "section": "Back to AWK",
    "text": "Back to AWK\nLesson Learned: Associative arrays in AWK are super powerful.\nI knew I could do better. I remembered that I had read in this charming AWK guide by Bruce Barnett about a cool feature in AWK called ‚Äúassociative arrays‚Äô. These are essentially a key-value stores in AWK that for some reason had been given a different name and thus I never thought too much about. ‚äïIt was brought to my attention by Roman Cheplyaka that the term ‚ÄòAssociative Array‚Äô is much older than ‚Äòkey-value store‚Äô. In fact, key-value store doesn‚Äôt even show up on google ngrams when you look for it, but associative array does! In addition, key-value stores are more often associated with database systems and thus a hashmap is really a more appropriate comparison here. I realized that I could use these associative arrays to perform the union between my SNP -&gt; bin table and my raw data without using Spark.\nTo do this I used the BEGIN block in my AWK script. This is a block of code that gets run before any lines of data are fed into the main body of the script.\njoin_data.awk\nBEGIN {\n  FS=\",\";\n  batch_num=substr(chunk,7,1);\n  chunk_id=substr(chunk,15,2);\n  while(getline &lt; \"snp_to_bin.csv\") {bin[$1] = $2}\n}\n{\n  print $0 &gt; \"chunked/chr_\"chr\"_bin_\"bin[$1]\"_\"batch_num\"_\"chunk_id\".csv\"\n}\nThe while(getline...) command loaded all the rows in from my bin csv and set the first column (the SNP name) as the key to the bin associative array and the second value (the bin) to the value. Then, in the { block } that gets run on every line of the main file, each line is sent to an output file that was had a unique name based upon its bin: ..._bin_\"bin[$1]\"_....\n‚äïThe variables of batch_num and chunk_id corresponded to data given by the pipeline that allowed me to avoid race conditions in my writing by making sure that every thread run by parallel wrote to its own unique file. \nBecause I had all the raw data split into chromosome folders from my previous AWK experiment I could now write another bash script to work through a chromosome at a time and send back the further partitioned data to S3.\n\nDESIRED_CHR='13'\n\n# Download chromosome data from s3 and split into bins\naws s3 ls $DATA_LOC |\nawk '{print $4}' |\ngrep 'chr'$DESIRED_CHR'.csv' |\nparallel \"echo 'reading {}'; aws s3 cp \"$DATA_LOC\"{} - | awk -v chr=\\\"\"$DESIRED_CHR\"\\\" -v chunk=\\\"{}\\\" -f split_on_chr_bin.awk\"\n\n# Combine all the parallel process chunks to single files and upload to rds using R\nls chunked/ |\ncut -d '_' -f 4 |\nsort -u |\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\nrm chunked/*\n\nThis script has two parallel sections:\nThe first one reads in every file containing data for the desired chromosome and divvies them up to multiple threads that spit their files into its representative bins. In order to prevent race conditions from the multiple threads writing to the same bin file, AWK is passed the name of the file which it uses to write to unique locations, e.g.¬†chr_10_bin_52_batch_2_aa.csv This results in a ton of tiny files located on the disk (I used 1TB EBS volumes for this).\nThe second parallel pipeline goes through and merges every bin‚Äôs separate files into single csv‚Äôs with cat, and sends them for export‚Ä¶"
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#piping-to-r",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#piping-to-r",
    "title": "Using AWK and R to parse 25tb",
    "section": "Piping to R?",
    "text": "Piping to R?\nLesson Learned: You can access stdin and stdout from inside an R script and thus use it in a pipeline.\nYou may have noticed this part of the bash script above: ...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R.... This line pipes all the concatenated files for a bin into the following R script‚Ä¶ ‚äïThe {} in there is a special parallel technique that pastes whatever data it is sending to the given thread right into the command it‚Äôs sending. Other option are {#} which gives the unique thread ID and {%} which is the job slot number (repeates but never at the same time). For all of the option checkout the docs.\n\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the variable file(\"stdin\") it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using aws.s3.\n‚äïRds is kind-of like a junior version of Parquet without the niceties of columnar storage.\nAfter this bash script had finished I have a bunch of .rds files sitting in S3 benefiting with the benefits of efficient compression and built-in types.\nEven with notoriously slow R in the workflow, this was super fast. ‚äïUnsurprisingly the parts of R for reading and writing data are rather optimized. After testing on a single average size chromosome the job finished in about two hours using a C5n.4xl instance."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#limits-of-s3",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#limits-of-s3",
    "title": "Using AWK and R to parse 25tb",
    "section": "Limits of S3",
    "text": "Limits of S3\nLesson Learned: S3 can handle a lot of files due to smart path implementation.\nI was worried about how S3 would handle having a ton of files dumped onto it. I could make the file names make sense, but how would S3 handle searching for one?\n‚äï Folders in S3 are just a cosmetic thing and S3 doesn‚Äôt actually care about the / character.From the S3 FAQs page\nIt turns out S3 treats the path to a given file as a simple key in what can be thought of as a hash table, or a document-based database. Think of a ‚Äúbucket‚Äù as a table and every file is an entry.\nBecause speed and efficiency are important to S3 making money for Amazon, it‚Äôs no surprise that this key-is-a-file-path system is super optimized. Still, I tried to strike a balance. I wanted to not need to do a ton of get requests and I wanted the queries to be fast. I found that making around 20k bin files worked best. ‚äïI am sure further optimizations could speed things up (such as making a special bucket just for the data and thus reducing the size of the lookup table.) But I ran out of time and money to do more experiments."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#what-about-cross-compatibility",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#what-about-cross-compatibility",
    "title": "Using AWK and R to parse 25tb",
    "section": "What about cross-compatibility?",
    "text": "What about cross-compatibility?\nLesson Learned: Premature optimization of your storage method is the root of all time wasted.\nA very reasonable thing to ask at this point is ‚Äúwhy would you use a proprietary file format for this?‚Äù The reason came down to speed of loading (using gzipped csvs took about 7 times longer to load) and compatibility with our workflows.‚äïOnce R can easily load Parquet (or Arrow) files without the overhead of Spark I may reconsider. Everyone else in my lab exclusively uses R and if I end up needing to convert the data to another format I still have the original raw text data and can just run the pipeline again."
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#divvying-out-the-work",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#divvying-out-the-work",
    "title": "Using AWK and R to parse 25tb",
    "section": "Divvying out the work",
    "text": "Divvying out the work\nLesson Learned: Don‚Äôt try to hand optimize jobs, let the computer do it.\nNow that I had the workflow for a single chromosome working, I needed to process every chromosome‚Äôs data. I wanted to spin up multiple ec2 instances to convert all my data but I also didn‚Äôt want to have super unbalanced job loads (just like how Spark suffered from the unbalanced partitions). ‚äïI also didn‚Äôt want to spin up a single instance for each chromosome, since there is a limit by default of 10 instances at a time for AWS accounts.\nMy solution was to write a brute force job optimization script using R‚Ä¶.\nFirst I queried S3 to figure out how large each chromosome was in terms of storage.\n\nlibrary(aws.s3)\nlibrary(tidyverse)\n\nchr_sizes &lt;- get_bucket_df(\n  bucket = '...', prefix = '...', max = Inf\n) %&gt;% \n  mutate(Size = as.numeric(Size)) %&gt;% \n  filter(Size != 0) %&gt;% \n  mutate(\n    # Extract chromosome from the file name \n    chr = str_extract(Key, 'chr.{1,4}\\\\.csv') %&gt;%\n             str_remove_all('chr|\\\\.csv')\n  ) %&gt;% \n  group_by(chr) %&gt;% \n  summarise(total_size = sum(Size)/1e+9) # Divide to get value in GB\n\n# A tibble: 27 x 2\n   chr   total_size\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 0           163.\n 2 1           967.\n 3 10          541.\n 4 11          611.\n 5 12          542.\n 6 13          364.\n 7 14          375.\n 8 15          372.\n 9 16          434.\n10 17          443.\n# ‚Ä¶ with 17 more rows\nThen I wrote a function that would take this total size info, shuffle the order, and split into num_jobs groups and report how variable the sizes of each job‚Äôs data was.\n\nnum_jobs &lt;- 7\n# How big would each job be if perfectly split?\njob_size &lt;- sum(chr_sizes$total_size)/7\n\nshuffle_job &lt;- function(i){\n  chr_sizes %&gt;%\n    sample_frac() %&gt;% \n    mutate(\n      cum_size = cumsum(total_size),\n      job_num = ceiling(cum_size/job_size)\n    ) %&gt;% \n    group_by(job_num) %&gt;% \n    summarise(\n      job_chrs = paste(chr, collapse = ','),\n      total_job_size = sum(total_size)\n    ) %&gt;% \n    mutate(sd = sd(total_job_size)) %&gt;% \n    nest(-sd)\n}\n\nshuffle_job(1)\n\n# A tibble: 1 x 2\n     sd data            \n  &lt;dbl&gt; &lt;list&gt;          \n1  153. &lt;tibble [7 √ó 3]&gt;\nOnce this was setup I ran a thousand shuffles using purrr and picked the best one.\n\n1:1000 %&gt;% \n  map_df(shuffle_job) %&gt;% \n  filter(sd == min(sd)) %&gt;% \n  pull(data) %&gt;% \n  pluck(1)\n\nThis gave me a set of jobs that were all very close in size. All I had do do then was wrap my previous bash script in a big for loop‚Ä¶ ‚äïIt took me ~10 mins to write this job optimization which was way less time than the inballance caused by my manual job creation would have added to the processing so I think I didn‚Äôt fall for premature optimization here.\n\nfor DESIRED_CHR in \"16\" \"9\" \"7\" \"21\" \"MT\"\ndo\n# Code for processing a single chromosome\nfi\n\nadd a shutdown command at the end‚Ä¶.\n\nsudo shutdown -h now\n\n‚Ä¶ and I was off the the races. I used the AWS CLI to spin up a bunch of instances, passing them their job‚Äôs bash script via the user_data option. They ran and then shutdown automatically so I didn‚Äôt pay for extra compute.\n\naws ec2 run-instances ...\\\n--tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=&lt;&lt;job_name&gt;&gt;}]\" \\\n--user-data file://&lt;&lt;job_script_loc&gt;&gt;"
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#intelligent-caching.",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#intelligent-caching.",
    "title": "Using AWK and R to parse 25tb",
    "section": "Intelligent caching.",
    "text": "Intelligent caching.\nLesson Learned: If your data is setup well, caching will be easy!\nSince one of the main workflows for these data was running the same model/ analysis across a bunch of SNPs at a time, I decided that I should use the binning to my advantage. When pulling the data down for a SNP, the entire bin‚Äôs data is kept and attached to the returned object. This means if a new query is run the old queries result‚Äôs can (potentially) be used to speed it up.\n\n# Part of get_snp()\n...\n  # Test if our current snp data has the desired snp.\n  already_have_snp &lt;- desired_snp %in% prev_snp_results$snps_in_bin\n\n  if(!already_have_snp){\n    # Grab info on the bin of the desired snp\n    snp_results &lt;- get_snp_bin(desired_snp)\n\n    # Download the snp's bin data\n    snp_results$bin_data &lt;- aws.s3::s3readRDS(object = snp_results$data_loc)\n  } else {\n    # The previous snp data contained the right bin so just use it\n    snp_results &lt;- prev_snp_results\n  }\n...\n\n‚äïWhile building the package I ran a lot of benchmarks to compare the speed between different methods. I recomend it because sometimes the results went against my intuition. For instance, dplyr::filter was much faster than using indexing based filtering for grabbing rows, but getting a single column from a filtered dataframe was much faster using indexing syntax.\nNotice that the prev_snp_results object contains the key snps_in_bin. This is an array of all unique SNPs in the bin, allowing fast checking for if we already have the data from a previous query. It also makes it easy for the user to loop through all the SNPs in a bin using code like:\n\n# Get bin-mates\nsnps_in_bin &lt;- my_snp_results$snps_in_bin\n\nfor(current_snp in snps_in_bin){\n  my_snp_results &lt;- get_snp(current_snp, my_snp_results)\n  # Do something with results \n}"
  },
  {
    "objectID": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#take-away",
    "href": "posts/2019-06-04-using-awk-and-r-to-parse-25tb/index.html#take-away",
    "title": "Using AWK and R to parse 25tb",
    "section": "Take Away",
    "text": "Take Away\nThis post post isn‚Äôt meant to be a how-to guide. The final solution is bespoke and almost assuredly not the optimal one. For risk of sounding unbearably cheesy this was about the journey. I want others to realize that these solutions don‚Äôt pop fully formed into peoples head‚Äôs but they are a product of trial and error.\nIn addition, if you are in the position of hiring someone as a data scientist please consider the fact that getting good at these tools requires experience, and experience requires money. I am lucky that I have grant funding to pay for this but many who assuredly could do a better job than me will never get the chance because they don‚Äôt have the funds to even try.\n‚ÄúBig Data‚Äù tools are generalists. If you have the time you will almost assuredly be able to write up a faster solution to your problem using smart data cleaning, storage, and retrieval techniques. Ultimately it comes down to a cost-benefit analysis.\n\nAll lessons learned:\nIn case you wanted everything in a neat list format:\n\nThere‚Äôs no cheap way to parse 25tb of data at once.\nBe careful with your Parquet file sizes and organization.\nPartitions in Spark need to be balanced.\nNever, ever, try and make 2.5 million partitions.\nSorting is still hard and so is tuning Spark.\nSometimes bespoke data needs bespoke solutions.\nSpark joining is fast, but partitioning is still expensive\nDon‚Äôt sleep on the basics. Someone probably solved your problem in the 80s.\ngnu parallel is magic and everyone should use it.\nSpark likes uncompressed data and does not like combining partitions.\nSpark is a lot of overhead for simple jobs.\nAssociative arrays in AWK are super powerful.\nYou can access stdin and stdout from inside an R script and thus use it in a pipeline.\nS3 can handle a lot of files due to smart path implementation.\nPremature optimization of your storage method is the root of all time wasted.\nDon‚Äôt try to hand optimize jobs, let the computer do it.\nKeep API simple for your end users and flexible for you.\nIf your data is setup well, caching will be easy!"
  },
  {
    "objectID": "posts/2020-05-17-graph-detective/index.html",
    "href": "posts/2020-05-17-graph-detective/index.html",
    "title": "Graph detective",
    "section": "",
    "text": "A plot has been floating around on Twitter from Georgia where the x-axis is all scrambled. Let‚Äôs look into it and see if we can fix it!\nlibrary(tidyverse)\nlibrary(ggridges)\n\nd &lt;- read_csv(\"https://github.com/nytimes/covid-19-data/raw/master/us-counties.csv\")\nd &lt;- d %&gt;%\n  filter(state == \"Georgia\",\n         county %in% c(\"Cobb\", \"DeKalb\", \"Fulton\", \"Gwinnett\", \"Hall\")) %&gt;%\n  group_by(county) %&gt;%\n  mutate(case = c(cases[1], diff(cases)))\nI pulled in the NY Times data to look at this. It looks like their estimates are different from the ones in the original graph (this is not unusual, I‚Äôve noticed for my county the counts are quite different depending on which sources you pull from), so I am going to recreate the original atrocity using the NY Times data for comparison."
  },
  {
    "objectID": "posts/2020-05-17-graph-detective/index.html#remake-their-silly-plot-with-ny-times-data",
    "href": "posts/2020-05-17-graph-detective/index.html#remake-their-silly-plot-with-ny-times-data",
    "title": "Graph detective",
    "section": "Remake their silly plot with NY Times data",
    "text": "Remake their silly plot with NY Times data\n\nd %&gt;%\n  filter(date &gt;= \"2020-04-26\", date &lt;= \"2020-05-09\") %&gt;%\n  mutate(\n    date = format(date, \"%d%b%Y\"),\n    date = factor(date,\n                  levels = c(\"28Apr2020\", \"27Apr2020\", \"29Apr2020\",\n                             \"01May2020\", \"30Apr2020\", \"04May2020\",\n                             \"06May2020\", \"05May2020\", \"02May2020\",\n                             \"07May2020\", \"26Apr2020\", \"03May2020\",\n                             \"08May2020\", \"09May2020\"))) %&gt;%\n  group_by(date) %&gt;%\n  mutate(rank = rank(-case, ties = \"first\")) %&gt;%\n  ggplot(aes(x = date, y = case, group = rank, fill = county)) +\n  geom_col(position = position_dodge()) +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n  scale_fill_manual(\"County\",\n                    values = c(\"#5854A8\", \"#46868E\", \"#958648\",\n                               \"#8F5D37\", \"#5D98DB\"),\n                    guide =  guide_legend(title.position = \"top\",\n                                          title.hjust = 0.5)) + \n  ggtitle(\n    \"Top 5 Counties with the Greatest Number of Confirmed COVID-19 Cases\",\n    subtitle = \"The chart below represents the most impacted counties over the past 15 days and the number of cases over time.\") +\n  theme_classic() +\n  theme(legend.position = \"top\", \n        panel.background = element_rect(fill = \"#182F4E\"),\n        plot.background = element_rect(fill = \"#182F4E\"),\n        axis.line = element_line(color = \"white\"),\n        axis.text = element_text(color = \"white\"),\n        axis.title = element_text(color = \"white\"),\n        legend.background = element_rect(fill = \"#182F4E\"),\n        legend.text = element_text(color = \"white\"),\n        legend.title = element_text(color = \"white\"),\n        title = element_text(color = \"white\"))\n\n\n\n\nHmm, this is a remake of their plot, but with NY Times data. The dates are in the same order as theirs, but it doesn‚Äôt give the same misleading message because they seemed to have sorted their x-axis to make it look like the cases were descending. We can remake that misleading plot using NY Times data too, though!"
  },
  {
    "objectID": "posts/2020-05-17-graph-detective/index.html#shuffled-still-bad-plot",
    "href": "posts/2020-05-17-graph-detective/index.html#shuffled-still-bad-plot",
    "title": "Graph detective",
    "section": "Shuffled, still bad, plot",
    "text": "Shuffled, still bad, plot\n\nd %&gt;%\n  filter(date &gt;= \"2020-04-26\", date &lt;= \"2020-05-09\") %&gt;%\n  mutate(\n    date = format(date, \"%d%b%Y\"),\n    date = factor(date,\n                  levels = c(\"02May2020\", \"06May2020\", \"04May2020\",\n                             \"01May2020\", \"27Apr2020\", \"29Apr2020\",\n                             \"28Apr2020\", \"05May2020\", \"08May2020\",\n                             \"30Apr2020\", \"09May2020\", \"03May2020\",\n                             \"07May2020\", \"26Apr2020\"))) %&gt;%\n  group_by(date) %&gt;%\n  mutate(rank = rank(-case, ties = \"first\")) %&gt;%\n  ggplot(aes(x = date, y = case, group = rank, fill = county)) +\n  geom_col(position = position_dodge()) +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n  scale_fill_manual(\"County\",\n                    values = c(\"#5854A8\", \"#46868E\", \"#958648\",\n                               \"#8F5D37\", \"#5D98DB\"),\n                    guide =  guide_legend(title.position = \"top\",\n                                          title.hjust = 0.5)) + \n  ggtitle(\n    \"Top 5 Counties with the Greatest Number of Confirmed COVID-19 Cases\",\n    subtitle = \"The chart below represents the most impacted counties over the past 15 days and the number of cases over time.\") +\n  theme_classic() +\n  theme(legend.position = \"top\", \n        panel.background = element_rect(fill = \"#182F4E\"),\n        plot.background = element_rect(fill = \"#182F4E\"),\n        axis.line = element_line(color = \"white\"),\n        axis.text = element_text(color = \"white\"),\n        axis.title = element_text(color = \"white\"),\n        legend.background = element_rect(fill = \"#182F4E\"),\n        legend.text = element_text(color = \"white\"),\n        legend.title = element_text(color = \"white\"),\n        title = element_text(color = \"white\"))\n\n\n\n\nNice, that‚Äôs good and misleading."
  },
  {
    "objectID": "posts/2020-05-17-graph-detective/index.html#fix-plot",
    "href": "posts/2020-05-17-graph-detective/index.html#fix-plot",
    "title": "Graph detective",
    "section": "Fix plot",
    "text": "Fix plot\nNow this is what the plot would look like if we plot the x-axis sensibly.\n\n\nWhy am I not using something sensible like scale_x_date(date_breaks = \"1 day\", guide = guide_axis(n.dodge = 2)), well I was, but there was a weird issue that it either cut off half of the bars on the first & last date, or added an extra date to either side. I had a weird hack that fixed it, but then it didn‚Äôt nicely match up with the other plot, so here we are.\n\nd %&gt;%\n  filter(date &gt;= \"2020-04-26\", date &lt;= \"2020-05-09\") %&gt;%\n  mutate(\n    date = format(date, \"%d%b%Y\"), \n    date = factor(date,\n                  levels = c(\"26Apr2020\", \"27Apr2020\", \"28Apr2020\", \n                             \"29Apr2020\", \"30Apr2020\", \"01May2020\", \n                             \"02May2020\", \"03May2020\", \"04May2020\",\n                             \"05May2020\", \"06May2020\", \"07May2020\", \n                             \"08May2020\", \"09May2020\"))) %&gt;%\n  ggplot(aes(x = date, y = case, group = county, fill = county)) +\n  geom_col(position = position_dodge()) + \n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n  scale_fill_manual(\"County\",\n                    values = c(\"#5854A8\", \"#46868E\", \"#958648\", \n                               \"#8F5D37\", \"#5D98DB\"),\n                    guide = guide_legend(title.position = \"top\",\n                                         title.hjust = 0.5)) + \n  ggtitle(\n    \"Top 5 Counties with the Greatest Number of Confirmed COVID-19 Cases\",\n    subtitle = \"The chart below represents the most impacted counties over the past 15 days and the number of cases over time.\") +\n  theme_classic() +\n  theme(legend.position = \"top\", \n        panel.background = element_rect(fill = \"#182F4E\"),\n        plot.background = element_rect(fill = \"#182F4E\"),\n        axis.line = element_line(color = \"white\"),\n        axis.text = element_text(color = \"white\"),\n        axis.title = element_text(color = \"white\"),\n        legend.background = element_rect(fill = \"#182F4E\"),\n        legend.text = element_text(color = \"white\"),\n        legend.title = element_text(color = \"white\"),\n        title = element_text(color = \"white\"))\n\n\n\n\nHmm, ok that is better, in that at least the x-axis is sensible. It‚Äôs still pretty hard to glean anything from this graph. Let‚Äôs try a few different visualizations."
  },
  {
    "objectID": "posts/2020-05-17-graph-detective/index.html#overlaid-histograms",
    "href": "posts/2020-05-17-graph-detective/index.html#overlaid-histograms",
    "title": "Graph detective",
    "section": "Overlaid Histograms",
    "text": "Overlaid Histograms\n\nd %&gt;%\n  filter(date &gt;= \"2020-04-26\", date &lt;= \"2020-05-09\") %&gt;%\n  ggplot(aes(x = date, y = case, group = county, fill = county)) +\n  geom_col(position = \"identity\", alpha = 0.75) + \n  scale_x_date(date_labels = \"%d%b%Y\",\n               date_breaks = \"5 days\") +\n  scale_fill_manual(\"County\",\n                    values = c(\"#5854A8\", \"#46868E\", \"#958648\",\n                               \"#8F5D37\", \"#5D98DB\"),\n                    guide = guide_legend(title.position = \"top\",\n                                         title.hjust = 0.5)) + \n  ggtitle(\n    \"Top 5 Counties with the Greatest Number of Confirmed COVID-19 Cases\",\n    subtitle = \"The chart below represents the most impacted counties over the past 15 days and the number of cases over time.\") +\n  theme_classic() +\n  theme(legend.position = \"top\", \n        panel.background = element_rect(fill = \"#182F4E\"),\n        plot.background = element_rect(fill = \"#182F4E\"),\n        axis.line = element_line(color = \"white\"),\n        axis.text = element_text(color = \"white\"),\n        axis.title = element_text(color = \"white\"),\n        legend.background = element_rect(fill = \"#182F4E\"),\n        legend.text = element_text(color = \"white\"),\n        legend.title = element_text(color = \"white\"),\n        title = element_text(color = \"white\")) \n\n\n\n\nBlah too busy."
  },
  {
    "objectID": "posts/2020-05-17-graph-detective/index.html#facet-histograms",
    "href": "posts/2020-05-17-graph-detective/index.html#facet-histograms",
    "title": "Graph detective",
    "section": "Facet Histograms",
    "text": "Facet Histograms\n\nd %&gt;%\n  filter(date &gt;= \"2020-04-26\", date &lt;= \"2020-05-09\") %&gt;%\n  ggplot(aes(x = date, y = case, group = county, fill = county)) +\n  geom_col() + \n  scale_x_date(date_labels = \"%d%b%Y\",\n               date_breaks = \"5 days\") +\n  scale_fill_manual(\"County\",\n                    values = c(\"#5854A8\", \"#46868E\", \"#958648\", \n                               \"#8F5D37\", \"#5D98DB\"),\n                    guide = guide_legend(title.position = \"top\",\n                                         title.hjust = 0.5)) + \n  ggtitle(\n    \"Top 5 Counties with the Greatest Number of Confirmed COVID-19 Cases\",\n    subtitle = \"The chart below represents the most impacted counties over the past 15 days and the number of cases over time.\") +\n  theme_classic() +\n  theme(legend.position = \"top\", \n        panel.background = element_rect(fill = \"#182F4E\"),\n        plot.background = element_rect(fill = \"#182F4E\"),\n        axis.line = element_line(color = \"white\"),\n        axis.text = element_text(color = \"white\"),\n        axis.title = element_text(color = \"white\"),\n        legend.background = element_rect(fill = \"#182F4E\"),\n        legend.text = element_text(color = \"white\"),\n        legend.title = element_text(color = \"white\"),\n        title = element_text(color = \"white\")) +\n  facet_grid(county~.)\n\n\n\n\nThis is okay, I still find it kind of hard to compare though."
  },
  {
    "objectID": "posts/2020-05-17-graph-detective/index.html#maybe-a-combo",
    "href": "posts/2020-05-17-graph-detective/index.html#maybe-a-combo",
    "title": "Graph detective",
    "section": "Maybe a combo?",
    "text": "Maybe a combo?\n\nd %&gt;%\n  filter(date &gt;= \"2020-04-26\", date &lt;= \"2020-05-09\") %&gt;%\n  ggplot(aes(x = date, y = county, fill = county, height = case)) +\n  geom_density_ridges(scale = 4, stat = \"identity\") + \n  scale_x_date(date_labels = \"%d%b%Y\",\n               date_breaks = \"5 days\") +\n  scale_fill_manual(\"County\",\n                    values = c(\"#5854A8\", \"#46868E\", \"#958648\",\n                               \"#8F5D37\", \"#5D98DB\"),\n                    guide = guide_legend(title.position = \"top\",\n                                         title.hjust = 0.5)) + \n  ggtitle(\n    \"Top 5 Counties with the Greatest Number of Confirmed COVID-19 Cases\",\n    subtitle = \"The chart below represents the most impacted counties over the past 15 days and the number of cases over time.\") +\n  theme_classic() +\n  theme(legend.position = \"top\", \n        panel.background = element_rect(fill = \"#182F4E\"),\n        plot.background = element_rect(fill = \"#182F4E\"),\n        axis.line = element_line(color = \"white\"),\n        axis.text = element_text(color = \"white\"),\n        axis.title = element_text(color = \"white\"),\n        legend.background = element_rect(fill = \"#182F4E\"),\n        legend.text = element_text(color = \"white\"),\n        legend.title = element_text(color = \"white\"),\n        title = element_text(color = \"white\")) \n\n\n\n\nRidgeline plots are nice, but I‚Äôm still not sure I get a lot out of this vis. Hmm."
  },
  {
    "objectID": "posts/2020-05-17-graph-detective/index.html#line-plot",
    "href": "posts/2020-05-17-graph-detective/index.html#line-plot",
    "title": "Graph detective",
    "section": "Line plot",
    "text": "Line plot\n\nd %&gt;%\n  filter(date &gt;= \"2020-04-26\", date &lt;= \"2020-05-09\") %&gt;%\n  ggplot(aes(x = date, y = case, color = county)) +\n  geom_line() + \n  geom_point() +\n  scale_x_date(date_labels = \"%d%b%Y\",\n               date_breaks = \"5 days\") +\n  scale_color_manual(\"County\",\n                     values = c(\"#5854A8\", \"#46868E\", \"#958648\",\n                                \"#8F5D37\", \"#5D98DB\"),\n                     guide = guide_legend(title.position = \"top\",\n                                          title.hjust = 0.5)) + \n  ggtitle(\n    \"Top 5 Counties with the Greatest Number of Confirmed COVID-19 Cases\",\n    subtitle = \"The chart below represents the most impacted counties over the past 15 days and the number of cases over time.\") +\n  theme_classic() +\n  theme(legend.position = \"top\", \n        panel.background = element_rect(fill = \"#182F4E\"),\n        plot.background = element_rect(fill = \"#182F4E\"),\n        axis.line = element_line(color = \"white\"),\n        axis.text = element_text(color = \"white\"),\n        axis.title = element_text(color = \"white\"),\n        legend.background = element_rect(fill = \"#182F4E\"),\n        legend.text = element_text(color = \"white\"),\n        legend.title = element_text(color = \"white\"),\n        title = element_text(color = \"white\"))\n\n\n\n\nMeh."
  },
  {
    "objectID": "posts/2020-05-17-graph-detective/index.html#bump-chart",
    "href": "posts/2020-05-17-graph-detective/index.html#bump-chart",
    "title": "Graph detective",
    "section": "Bump chart",
    "text": "Bump chart\nJoshua suggested a bump chart with the dots scaled based on the number of cases.\n\n\nWould something like a bump chart be better? You could encode severity in size maybe?\n\n‚Äî Joshua de la Bruere (@delaBJL) May 17, 2020\n\n\n\nd %&gt;%\n  filter(date &gt;= \"2020-04-26\", date &lt;= \"2020-05-09\") %&gt;%\n  group_by(date) %&gt;%\n  mutate(rank = rank(-case, ties = \"first\")) %&gt;%\n  ggplot(aes(x = date, y = rank, color = county)) +\n  geom_line() + \n  geom_point(aes(size = case)) +\n  scale_x_date(date_labels = \"%d%b%Y\",\n               date_breaks = \"5 days\") +\n  scale_size_continuous(\"Number of cases\",\n                        guide = guide_legend(title.position = \"top\",\n                                             title.hjust = 0.5)) +\n  scale_color_manual(\"County\",\n                     values = c(\"#5854A8\", \"#46868E\", \"#958648\",\n                                \"#8F5D37\", \"#5D98DB\"),\n                     guide = guide_legend(title.position = \"top\",\n                                          title.hjust = 0.5)) + \n  ggtitle(\n    \"Top 5 Counties with the Greatest Number of Confirmed COVID-19 Cases\",\n    subtitle = \"The chart below represents the most impacted counties over the past 15 days and the number of cases over time.\") +\n  theme_classic() +\n  theme(legend.position = \"top\", \n        panel.background = element_rect(fill = \"#182F4E\"),\n        plot.background = element_rect(fill = \"#182F4E\"),\n        axis.line = element_line(color = \"white\"),\n        axis.text = element_text(color = \"white\"),\n        axis.title = element_text(color = \"white\"),\n        legend.background = element_rect(fill = \"#182F4E\"),\n        legend.text = element_text(color = \"white\"),\n        legend.title = element_text(color = \"white\"),\n        title = element_text(color = \"white\"))\n\n\n\n\nI don‚Äôt hate it‚Ä¶but it is still kind of busy. I think part of the problem is it is quite hard to compare 5 things over two axis simultaneously without a clear signal. Maybe if we could reduce it to comparisons between 2 things at a time?"
  },
  {
    "objectID": "posts/2020-05-17-graph-detective/index.html#plotly",
    "href": "posts/2020-05-17-graph-detective/index.html#plotly",
    "title": "Graph detective",
    "section": "Plotly",
    "text": "Plotly\n\nlibrary(plotly)\n\n\nd %&gt;%\n  filter(date &gt;= \"2020-04-26\", date &lt;= \"2020-05-09\") %&gt;%\n  ggplot(aes(x = date, y = case, fill = county)) +\n  geom_col(position = \"identity\", alpha = 0.75) + \n  scale_x_date(date_labels = \"%d%b%Y\",\n               date_breaks = \"5 days\") +\n  scale_fill_manual(\"County\",\n                    values = c(\"#5854A8\", \"#46868E\", \"#958648\",\n                               \"#8F5D37\", \"#5D98DB\"),\n                    guide = guide_legend(title.position = \"top\",\n                                         title.hjust = 0.5)) + \n  ggtitle(\n    \"Top 5 Counties with the Greatest Number of Confirmed COVID-19 Cases\",\n    subtitle = \"The chart below represents the most impacted counties over the past 15 days and the number of cases over time.\") +\n  theme_classic() +\n  theme(legend.position = \"top\", \n        panel.background = element_rect(fill = \"#182F4E\"),\n        plot.background = element_rect(fill = \"#182F4E\"),\n        axis.line = element_line(color = \"white\"),\n        axis.text = element_text(color = \"white\"),\n        axis.title = element_text(color = \"white\"),\n        legend.background = element_rect(fill = \"#182F4E\"),\n        legend.text = element_text(color = \"white\"),\n        legend.title = element_text(color = \"white\"),\n        title = element_text(color = \"white\")) -&gt; p\n\nggplotly(p)\n\n\n\n\n\nYou can click on the legend to hide counties, allowing you to compare just 2 at a time. This seems marginally better."
  },
  {
    "objectID": "posts/2020-05-17-graph-detective/index.html#barchart-race",
    "href": "posts/2020-05-17-graph-detective/index.html#barchart-race",
    "title": "Graph detective",
    "section": "Barchart race",
    "text": "Barchart race\nJoshua also suggested (and created) a barchart race! I actually think this may have some utility here because it helps narrow the focus (looking at one day at a time), which is part of the problem with these visualizations.\n\n\nMaybe this is a job for a bar chart race! https://t.co/BnTvHxKZ6fIf the goal is to show ranking across time. But that seems like a pretty volatile metric\n\n‚Äî Joshua de la Bruere (@delaBJL) May 17, 2020\n\n\nThe barchart races I‚Äôve seen are typically cumulative, so I‚Äôm not sure this is exactly the right use case (and in this particular case, the rankings of the cumulative counts don‚Äôt change much over these 15 days), but let‚Äôs see how it looks.\n‚äïI learned how to do this from Michael Toth‚Äôs blogpost How to Create a Bar Chart Race in R - Mapping United States City Population 1790-2010\n\nlibrary(gganimate)\nd %&gt;%\n  filter(date &gt;= \"2020-04-26\", date &lt;= \"2020-05-09\") %&gt;%\n  group_by(date) %&gt;%\n  mutate(rank = rank(-case, ties = \"first\")) %&gt;%\n  ggplot(aes(x = -rank, y = case, fill = county)) +\n  geom_tile(aes(y = case / 2, height = case), width = 0.9) +\n  geom_text(aes(label = county), hjust = \"right\", \n            colour = \"white\", fontface = \"bold\",\n            nudge_y = -5) +\n  scale_fill_manual(values = c(\"#5854A8\", \"#46868E\", \"#958648\",\n                               \"#8F5D37\", \"#5D98DB\")) +\n  geom_text(aes(label = scales::comma(case)), hjust = \"left\", \n            nudge_y = 5, colour = \"white\") +\n  coord_flip(clip = \"off\") +\n  scale_x_discrete(\"\") +\n  scale_y_continuous(\"\") +\n  theme_classic() +\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"none\",\n        plot.margin = margin(1, 1, 1, 2, \"cm\"),\n        axis.text.y = element_blank(),\n        panel.background = element_rect(fill = \"#182F4E\"),\n        plot.background = element_rect(fill = \"#182F4E\"),\n        axis.line = element_line(color = \"white\"),\n        axis.text = element_text(color = \"white\"),\n        axis.title = element_text(color = \"white\"),\n        title = element_text(color = \"white\")) +\n  transition_time(date) +\n  ease_aes(\"cubic-in-out\") +\n  labs(\n    title = \"Top 5 Counties with the Greatest Number of Confirmed \\nCOVID-19 Cases\",\n    subtitle = \"Date: {as.Date(frame_time)}\",\n    caption = \"Data: NY Times\\nGraph: @LucyStats\") -&gt; p\n\n\np"
  },
  {
    "objectID": "posts/2022-02-20-exploring-impacts-of-noninferiority-thresholds/index.html",
    "href": "posts/2022-02-20-exploring-impacts-of-noninferiority-thresholds/index.html",
    "title": "Exploring the impacts of noninferiority trial thresholds",
    "section": "",
    "text": "This post explores the impact of setting particular criteria for ‚Äúsuccess‚Äù in clinical trial designs. A common study design is a ‚Äúnon-inferiority‚Äù trial. The goal here is to show that some intervention is not inferior, that is not worse, than some already approved intervention, by some specific definition of not worse. While this may sound straightforward, it can be tricky! Especially because in all clinical trials we are working with a sample of individuals, so there is necessarily uncertainty around the estimates we can calculate. Also, the definition of not worse can vary.\n\n\nThe standard error is just the {standard devation} divided by the square-root of the sample size, \\(n\\), \\(\\frac{sd}{\\sqrt{n}}\\)\nIn statistics speak, this uncertainty is often quantified as a standard error, a measure that takes into account both the standard deviation of our outcome of interest and the number of people we have measured this outcome on. What do I mean? Here is an example.\n\n\nGMT: geometric mean titer. This is a measure of the antibody titers. We use the geometric mean because this data is quite skewed (it is also why you typically see it plotted on the log scale). For those of you who ‚ù§Ô∏è math, the equation for the geometric mean is just \\(\\exp\\{\\frac{\\sum_{i=1}^n\\textrm{log}(x_i)}{n}\\}\\)\nLet‚Äôs say we want to know whether new intervention is not inferior to benchmark intervention with respect to some measure, in this case we are going to use the geometric mean titers (GMT), a common measure of antibody response. In this first example, the truth is these two interventions yield the same GMT, that is, they are equivalent, so new intervention is not inferior to benchmark intervention. Let‚Äôs say we have a sample of 253 people who received the benchmark intervention. The truth is that GMT is 1142.5 with a standard deviation of 0.8. We won‚Äôt see these exact results in our benchmark intervention group since we‚Äôre just looking at one sample of 253. The plot below shows what our 253 folks could look like if we were to collect these data 10 different times. The circles represent the GMT in each sample and the lines show the 95% confidence intervals.\n\n\nNote: If we were trying to conduct a hypothesis test to establish whether the mean was different from 1142, and we collected these same 10 samples, the 4th sample would result in a Type 1 error, that is we‚Äôd reject the null hypothesis that the mean is equal to 1142 even though that is the truth. When using 95% confidence intervals, we expect to see a Type 1 error 5% of the time.\nIn reality, we would only see one of these results. Notice how some of these estimates differ from what we know is the truth (a GMT of 1142), for example the 4th sample had a geometric mean of 980 ‚Äì this is expected. In fact, based on what we know about confidence intervals, if we repeat this process 100 times, we‚Äôd expect about 5 of these intervals to not contain 1142 at all. Notice the width of the confidence intervals is generally the same. This is determined by the standard deviation (which here is 0.8) and the sample size (253).\n\n\n\n\n\nOk, so let‚Äôs say we observed the 10th sample in the plot above (GMT: 1146.5 (95% CI: 1257.2, 1045.5)). We want to show that some new intervention yields a GMT that is not inferior (not worse) than this benchmark intervention. One way we could do that is collect data on the new intervention, compare it to the old, and set some criteria for ‚Äúsuccess‚Äù. What would make this new intervention not inferior? Let‚Äôs see what happens if we set our criteria for success to be that the geometric mean titers among our sample of those in the new intervention group is not less than the geometric mean we observed in our benchmark intervention sample. Another way to look at this is to look at the ratio between our two geometric means, the GMT in the new intervention divided by the GMT in the old intervention group. If this ratio is equal to one, they are the same, if the ratio is less than 1, the new intervention is deemed ‚Äúworse‚Äù (inferior) and if the ratio is greater than 1, the new intervention is better. Another way to state this criteria is that the GMT ratio needs to be \\(\\ge 1\\).\n\nScenario 1: By ‚Äúnon-inferiority‚Äù we mean the observed point estimates will be equal or the point estimate for the new group will be higher\n\n\nThreshold: GMT ratio \\(\\ge 1\\)\nI am going to generate some new intervention samples, remember we only observe one of these in reality. I am generating them from the same distribution as the old intervention, so the truth here is that the two are equal (and thus also non-inferior). Remember that to generate these samples we also need to know the sample size. Let‚Äôs assume the sample size is the same (253).\n\n\n\n\n\nThe plot above shows 10 random samples from our new intervention. Remember that these were generated exactly like our old intervention, so the truth is that they are equivalent (and thus it is also true that the new intervention is not inferior, not worse, than the old). Notice here, 50% of the samples ‚Äúfailed‚Äù (in orange in the plot above) by our criteria that the geometric mean (the point estimate) of our new intervention had to be greater than or equal to our observed geometric mean in the old intervention. This is exactly what we‚Äôd expect to happen! Because there is some random chance involved (we are only looking at a sample), the probability of falling on either side of a point estimate is going to be a coin toss when the ‚Äútruth‚Äù is centered there.\nOften, the criteria for non-inferiority trials is set based on a lower bound threshold instead. For example, we could say we want to have some certainty that the new intervention is not some percent worse than the old. Maybe we set our lower bound to 0.67. This would mean that in order to be deemed a ‚Äúsuccess‚Äù the lower bound of our geometric mean ratio between the old and new interventions would need to exceed 0.67. Looking at the plot above, this seems easy! However, if we assume the same standard deviation (0.8) and same sample size (253), we can achieve this threshold even if the new intervention is statistically significantly worse. Let‚Äôs see that.\n\n\nScenario 2: By ‚Äúnon-inferiority‚Äù we mean we can have some level of confidence that the new intervention is not more than 67% worse than the old\n\n\nThreshold: GMT ratio lower bound \\(\\ge 0.67\\)\nNow I am going to assume that the truth is that the new intervention is only 75% as good as the old, meaning the true ratio between the two is 0.75. This would equate to being around 6 standard errors worse than equal, so pretty bad. Let‚Äôs generate some samples under this assumption and see how we do.\n\n\n\n\n\nOur lower bound threshold of 0.67 ‚Äúcaught‚Äù our inferior samples ~70% of the time here (in orange), but notice there were three samples that would be considered ‚Äúsuccessful‚Äù by this metric (blue), even though the full confidence interval falls below 1. How likely something like this is to occur depends on two things:\n\nthe standard deviation of the outcome in new intervention group\nthe sample size of the new intervention group\n\nOften clinical trials need to prespecify their sample size, so we can guard against one of these, but the standard deviation is often just a guess. For example, what if the standard deviation in the new intervention group was much smaller, say half of the standard deviation in the old intervention group. Let‚Äôs see what that looks like.\n\n\n\n\n\nNotice in the plot above with the smaller standard deviations, the confidence intervals shrunk! Now we have 60% successes, all with upper bounds less than 1 üò±. This is why often a second criteria is set to guard against this possibility. In the past, I have often seen this as a criteria for the upper bound ‚Äì for example, you could require that the upper bound not be less than 1 to exclude the possibility of a ‚Äúsuccess‚Äù when the new intervention is in fact statistically significantly worse. Why not only set an upper bound criterion? If we did that, we could have the opposite scenario where the true standard deviation is much larger than expected, so despite the upper bound being above one, the confidence interval spans a very large range (like below!).\n\n\nScenario 3: By ‚Äúnon-inferiority‚Äù we mean the new intervention is not statistically significantly worse than the old intervention\n\n\nThreshold: GMT ratio upper bound \\(\\ge 1\\)\n\n\n\n\n\nIncluding both criteria would guard against this. Let‚Äôs look at that across all of the scenarios we‚Äôve explored so far.\n\n\nScenario 4: By ‚Äúnon-inferiority‚Äù we mean we can have some level of confidence that the new intervention is not more than 67% worse than the old and the new intervention is not statistically significantly worse than the old intervention\n\n\nThresholds: GMT ratio lower bound \\(\\ge 0.67\\) AND GMT ratio upper bound \\(\\ge 1\\)\n\n\nRemember, I‚Äôm showing you 10 different possible sampling scenarios generated under the same ‚Äútruth‚Äù. In reality, we‚Äôd only observe one of these. The reason I‚Äôm showing 10 is so you can see the probability of success or failure based on the criteria and the truth.\n\n\n\n\n\nBeautiful! Now we are failing when we should be, regardless of our standard deviation, and succeeding when we should be (when the truth is that the two groups are the same). What we‚Äôve seen so far is similar to how the immunobridging Pfizer trial was designed to evaluate vaccines in kids. Because Pfizer didn‚Äôt think they would have enough time or participants to accurately estimate the efficacy of the vaccine in young children, they instead designed a trial to compare their antibody response to the antibody response seen in 16-25 year olds, for whom they were able to demonstrate efficacy. The thinking being: if the antibody levels in kids is comprable to that in 16-25 year olds, then hopefully the efficacy will be comparable too. The issue, of course, is what do they mean by comparable (or not inferior). Originally they designed this with two thresholds: the lower bound needed to be \\(\\ge 0.67\\) and the point estimate needed to be \\(\\ge 0.8\\). This is similar to scenario 4, but there are important differences, which brings me to scenario 5!\n\n\nScenario 5: By ‚Äúnon-inferiority‚Äù we mean we can have some level of confidence that the new intervention is not more than 67% worse than the old and the new intervention and even more confidence that it is not more than 80% worse than the old intervention\n\n\nThresholds: GMT ratio lower bound \\(\\ge 0.67\\) AND GMT ratio point estimate \\(\\ge 0.8\\)\nFixing a non-inferiority bound at a point estimate introduces some complications. By not taking the upper bound into account, we don‚Äôt fully guard against seeing a statistically significantly worse result that ‚Äúsucceeds‚Äù by our criteria. For example, consider the scenario where the ‚Äútruth‚Äù is that the new intervention is 85% as good as the old. This passes our definition of ‚Äúnon-inferiority‚Äù, but we could end up with many cases where the new intervention is statistically significantly worse but still ‚Äúsuccessful‚Äù. Much of these decisions need to be grounded both in the statistics and the science. If it is deemed scientifically acceptable that generating 85% of an antibody response is ok, then this is a fine threshold.\n\n\n\n\n\nThis will succeed 90% of the time, but notice that but notice that 8 of the 9 ‚Äúsuccesses‚Äù the confidence interval indicates that the intervention is statistically significantly worse.\nIt turns out, after Pfizer finished their 5-11 trial, the FDA requested that they move this second threshold to be more stringent.. Instead of needing a point estimate \\(\\ge 0.8\\), they now needed a point estimate \\(\\ge 1\\).\nThis is where things get tricky! Recall from the very first scenario that we looked at that setting a threshold based on the point estimate like this will fail 50% of the time even if the truth is that the two groups are exactly equal. By imposing this new threshold, the FDA is essentially requiring that the antibody response in children by superior to that in 16-25 year olds, not just not inferior. In addition, setting this threshold like this essentially renders the lower bound moot unless the variability in antibody response is much larger in young children compared to 16-25 year olds. In fact, in order for that ‚Äúlower bound \\(\\ge 0.67\\)‚Äù threshold to be invoked while the ‚Äúpoint estimate \\(\\ge 1\\)‚Äù succeeds, the actual standard deviation in the young children‚Äôs antibody titers would need to be about 4 times that observed in the older cohort. This threshold (the need to have the point estimate \\(\\ge 1\\)) basically sets a large ‚Äúweight‚Äù on the younger antibody response needing to be much stronger than that observed in the older cohort. Let‚Äôs take the 2-4 year olds as an example.\n\n\nScenario 6: By ‚Äúnon-inferiority‚Äù we mean we can have some level of confidence that the new intervention is not more than 67% worse than the old and the new intervention and even more confidence that it is not any worse than the old intervention\n\n\nThresholds: GMT ratio lower bound \\(\\ge 0.67\\) AND GMT ratio point estimate \\(\\ge 1\\)\nIf we think that an antibody response exactly equal to that seen by 16-25 year olds is sufficient to deem this vaccine non inferior, then this is not the right threshold, as a trial testing this way will fail 50% of the time. That is, the chance of failing is a coin toss, even if the two groups are exactly equal. Below shows this again. Remember for a given trial, we are only lucky enough to see one of these outcomes in the plot below; I am showing 10 so you can see the variation based on the sampling variability.\n\n\n\n\n\nStatistically, in order to have a 85% chance of ‚Äúsuccess‚Äù, the truth would need to be that the antibody response in kids is ~1 standard error higher compared to 16-25 year olds. This is based on basic normal theory. It turns out if a variable is log normally distributed, the geometric mean is equivalent to the median of the distribution, which ties to the mean of the normal distribution. Therefore, we can use basic normal theory to understand the impact of requiring the point estimate of a distribution to be greater than some value. Assuming that the truth is that this point estimate (the GMT ratio) is 1 \\(\\times\\) x [standard errors], and you take a sample from a log normally distributed population, the probability that the GMT ratio that you observe is 1 is equal to qnorm(x). For example, if the truth is that the GMT ratio is 1 \\(\\times 1.96\\) [standard errors], and you ran this trial a bunch of times, you would expect a ‚Äúsuccess‚Äù 95% of the time. (The 5% of the time that you saw a ‚Äúfailure‚Äù would be considered a Type II error).\nSO what does this mean? Well it depends on what the standard deviation ends up being for 2-4 year old antibody response. If it is the same as 16-25 year olds, in order to avoiding making a Type II error more than 15% of the time, the geometric mean of the antibody response in 2-4 year olds would need to be 5-6% higher than what was observed in 16-25 year olds. If their standard deviation is actually larger this will increase, for example if it was 2, the true antibody response in 2-4 year olds would need to be ~ 14% higher.\nAs many have pointed out, it is not uncommon for pediatric trials to be held to a higher standard, often requiring efficacy beyond what is required of adults due to an appropriate caution against intervention in an often vulnerable group. I fully believe that the regulators that requested this had every best intention in doing so. In this particular case, however, this type of threshold can potentially lead to the opposite effect. By requiring the younger children to mount a higher antibody response than the older cohort in order to pass regulatory hurdles, we may be inadvertently pushing towards higher dosing, for example.\nIs this why the 2-4 year old vaccine failed previously? It‚Äôs not totally clear since the data hasn‚Äôt been released, however based on the tid bits we‚Äôve gotten from media reports, I don‚Äôt think so. The New York Times reported, for example, that the 2-4 vaccine only elicited 60% of the response compared to the 16-25 year olds, suggesting that it would have failed by the lower bound criteria alone. So why does this matter? Presumably, these thresholds will be used to compare the post-3rd dose response to the 16-25 year olds as well ‚Äì does it make sense to require the under 5s to have a stronger antibody response than the 16-25 year olds? Especially with no other option for protection via a vaccine for this age group, I would say no."
  },
  {
    "objectID": "posts/2016-12-15-hill-for-the-data-scientist/index.html",
    "href": "posts/2016-12-15-hill-for-the-data-scientist/index.html",
    "title": "Hill for the data scientist: an xkcd story",
    "section": "",
    "text": "This was inspired by Hilary Parker & Roger Peng‚Äôs Not So Standard Deviations Episode 28, which can be found here. It was suggested that it would be useful to lay out Hill‚Äôs criterion for data scientists, I agree!\n\n[causation: An event or outcome B is influenced by a change in A]\n Sir Austin Bradford Hill, a statistician and epidemiologist, created a list of guidelines for evaluating whether there is evidence of a causal relationship.[1] He determined the following aspects of associations ought to be considered when assessing causality. When thinking about this problem, an xkcd comic I have seen in every lecture on this topic came to mind:\n  \n\nThis inspired me to attempt to explain Hill‚Äôs criteria using xkcd comics, both because it seemed fun, and also to motivate causal inference instructures to have some variety in which xkcd comic they include in lectures (bear with me, some of these are a stretch üôàüíÅüèª).\n\n\nStrength\n\nHow big is the effect you are seeing?\nNote: Hill suggests that huge effects can suggest causality, however this does not mean small effects cannot\n\n\n\nNote: I am using this idea for a talk and I found a strip (to the left) that I think better represents this concept. The original post had this one.\n  \n\n\n\nConsistency\n\nThis is essentially reproducibility & replicability\nCan your analysis be reproduced?\nHas anyone been able to replicate your findings?\n\n  \n\n\n\nSpecificity\n\nCan the association be pinpointed to a specific cause with no other plausible explanation?\nI appreciate Hill‚Äôs caveat here, ‚Äúif specificity exists we may be able to draw conclusions without hesitation; if it is not apparent, we are not thereby necessarily left sitting irresolutely on the fence.‚Äù\n\n  \n\n\n\nTemporality\n\nDoes the timeline make sense?\nIn general, the exposure ought to come before the outcome it is said to cause.\n\n  \n\n\n\nBiological gradient\n\nThe wording of this point makes it a bit difficult to untangle from the medical application, but generally this refers to a dose effect\nDoes increasing an exposure yield a change in the outcome.\n\n  \n\n\n\nPlausibility\n\nDoes the causal relationship make sense?\nThis is also a tricky one since plausibility depends on knowledge at the time. If we found it perfectly plausible, we may not need statistics to show the relationship.\n\n  \n\n\n\nCoherence\n\nSimilar to plausibility, is there a logical argument that can be made by/to experts in the field regarding causality.\nDoes it fit into the understanding of the field (authors note: this should have caveats too‚Ä¶the field could be wrong).\n\n  \n\n\n\nExperiment\n\nIf a controlled experiment can take place, this can strengthen the argument for causality\nI view this as a general attempt to implement a counterfactual analysis.\n\n  \n\n\n\nAnalogy\n\nHave we seen a similar effect from a similar exposure?\n\n  \n[1] Hill, A. B. (1965). The Environment and Disease: Association or Causation? Proceedings of the Royal Society of Medicine, 58(5), 295‚Äì300.\nThink I‚Äôve missed something? Submit a PR."
  },
  {
    "objectID": "posts/2020-10-02-how-to-build-a-diy-lightboard/index.html",
    "href": "posts/2020-10-02-how-to-build-a-diy-lightboard/index.html",
    "title": "How to build a DIY Lightboard",
    "section": "",
    "text": "Here are some examples of my DIY lightboard in action!  * How logging symptoms helps slow the virus spread * Lucy D‚ÄôAgostino McGowan discusses peer learning communities * Law of Iterated Expectation\nThis summer in an effort to spruce up my home office and online statistics courses, I was in search of a simple solution that would let me walk through equations, as I normally would with a whiteboard in my office, remotely. Enter the lightboard! I love the aesthetic of these! They let you work through fun equations while still letting the viewer see your facial expressions, getting as close to an ‚Äúin person‚Äù interaction as I‚Äôve seen! The downside? They are super pricey! Some sites were quoting over $2,500 for the simpliest models. Even DIY ‚Äúkits‚Äù were coming in at over $1,000 - yikes! Then I came across this amazing YouTube video that lays out a simple DIY method that can be put together for much less - in fact, I was able to get my supplies together for less than $100. I put together a quick tweetorial outlining the steps and the supplies I used; I thought I‚Äôd copy the contents here, since a blogpost is sometimes a bit easier to read!"
  },
  {
    "objectID": "posts/2020-10-02-how-to-build-a-diy-lightboard/index.html#supplies",
    "href": "posts/2020-10-02-how-to-build-a-diy-lightboard/index.html#supplies",
    "title": "How to build a DIY Lightboard",
    "section": "Supplies",
    "text": "Supplies\nr emo::ji(\"white_large_square\") 1/4 x 24 x 36 plexiglass\nr emo::ji(\"bulb\") 16ft of LED strip lights\nr emo::ji(\"clamp\")2 one inch C-clamps\nr emo::ji(\"book\") 4 bookshelf L brackets\nr emo::ji(\"pen\") neon markers\n\nStep 1\nr emo::ji(\"bulb\") stick 3ft of the LED strip lights on a table\nr emo::ji(\"point_up\") your plexiglass will sit on top of this\n\n\n\nStep 2\nr emo::ji(\"sandwich\") put your plexiglass between two of the shelf brackets on one side\nr emo::ji(\"clamp\") fasten with the C-clamp\nr emo::ji(\"repeat\") repeat on the other side\n\n\n\nStep 3\nr emo::ji(\"bulb\") wrap the LED strip lights around the remainder of the plexiglass\nr emo::ji(\"point_up\") I used a little tape to hold it on\n\n\n\nStep 4\nr emo::ji(\"selfie\") set up camera\nr emo::ji(\"partying_face\") use fun markers\nr emo::ji(\"left_right_arrow\") flip video to mirror image"
  },
  {
    "objectID": "posts/2020-05-16-this-one-cool-hack/index.html",
    "href": "posts/2020-05-16-this-one-cool-hack/index.html",
    "title": "This one cool hack will‚Ä¶help you categorize Harry Potter characters!",
    "section": "",
    "text": "Hilary Parker & Roger Peng mention a cool tactic for categorizing data on their podcast, Not So Standard Deviations. (If I recall correctly, I think Hilary mentioned it first, then Roger used it in a subsequent episode, unfortunately I don‚Äôt remember which episodes these were and a quick look-back proved futile. If I figure it out, I‚Äôll link it here!)\nThe basic concept is to make a small table & join it into the data frame you are trying to categorize instead of writing a bunch of if/else statements. This is especially useful if you are:\n‚òùÔ∏è Using the same categories on a bunch of different data frames (you can just create the small table of categories once!)\n‚úåÔ∏è Creating multiple new variables from a single variable\nI tweeted about this kernel of wisdom and a few people asked me to write up an example, so here it is!"
  },
  {
    "objectID": "posts/2020-05-16-this-one-cool-hack/index.html#example",
    "href": "posts/2020-05-16-this-one-cool-hack/index.html#example",
    "title": "This one cool hack will‚Ä¶help you categorize Harry Potter characters!",
    "section": "Example",
    "text": "Example\nFor this example, I am going to rank Harry Potter characters based on the house they were sorted into on a variety of characteristics.\n‚äïThis data originated from a Kaggle Dataset by Gulsah Demiryurek\n\nlibrary(tidyverse)\nharry_potter &lt;- read_csv(\"https://raw.githubusercontent.com/LFOD/real-blog/master/static/data/harry-potter.csv\")"
  },
  {
    "objectID": "posts/2020-05-16-this-one-cool-hack/index.html#tired",
    "href": "posts/2020-05-16-this-one-cool-hack/index.html#tired",
    "title": "This one cool hack will‚Ä¶help you categorize Harry Potter characters!",
    "section": "Tired",
    "text": "Tired\nHere is how I would do this with case_when().\n\nharry_potter %&gt;%\n  mutate(\n    smart_rank = case_when(\n      House == \"Ravenclaw\" ~ 1,\n      House == \"Gryffindor\" ~ 2,\n      House == \"Slytherin\" ~ 3,\n      House == \"Hufflepuff\" ~ 4\n    ),\n    brave_rank = case_when(\n      House == \"Gryffindor\" ~ 1,\n      House == \"Slytherin\" ~ 2,\n      House == \"Ravenclaw\" ~ 3,\n      House == \"Hufflepuff\" ~ 4\n    ),\n    cunning_rank = case_when(\n      House == \"Slytherin\" ~ 1,\n      House == \"Ravenclaw\" ~ 2,\n      House == \"Gryffindor\" ~ 3,\n      House == \"Hufflepuff\" ~ 4\n    ),\n    kind_rank = case_when(\n      House == \"Hufflepuff\" ~ 1,\n      House == \"Gryffindor\" ~ 2,\n      House == \"Ravenclaw\" ~ 3,\n      House == \"Slytherin\" ~ 4\n    )\n  )"
  },
  {
    "objectID": "posts/2020-05-16-this-one-cool-hack/index.html#wired",
    "href": "posts/2020-05-16-this-one-cool-hack/index.html#wired",
    "title": "This one cool hack will‚Ä¶help you categorize Harry Potter characters!",
    "section": "Wired",
    "text": "Wired\nHere‚Äôs how I would do this with a data frame\n\nranks &lt;- tibble(\n  House = c(\"Gryffindor\", \"Ravenclaw\", \"Hufflepuff\", \"Slytherin\"),\n  smart_rank = c(2, 1, 4, 3),\n  brave_rank = c(1, 3, 4, 2),\n  cunning_rank = c(3, 2, 4, 1),\n  kind_rank = c(2, 3, 1, 4)\n)\n\nharry_potter %&gt;%\n  left_join(ranks, by = \"House\")"
  },
  {
    "objectID": "posts/2020-05-16-this-one-cool-hack/index.html#inspired",
    "href": "posts/2020-05-16-this-one-cool-hack/index.html#inspired",
    "title": "This one cool hack will‚Ä¶help you categorize Harry Potter characters!",
    "section": "Inspired",
    "text": "Inspired\nAfter tweeting this out, several people pointed out that this is a nice use case for tribble()!\n\n\nSuch lookup tables are things where tribble really shines, IMHO.\n\n‚Äî Konrad Rudolph (@klmr) May 16, 2020\n\n\n\n\ntribble() would be just as clear as the tired one and just as nifty as the wired one.\n\n‚Äî Dave Harris, but masked (@davidjayharris) May 16, 2020\n\n\n\n\nThat‚Äôs how set nice labels for plots and tables. Make a tribble and join at the last minute.\n\n‚Äî tj mahr üçïüçç (@tjmahr) May 16, 2020\n\n\nLet‚Äôs see how that looks!\n\nranks &lt;- tribble(\n  ~House,      ~smart_rank, ~brave_rank, ~cunning_rank, ~kind_rank,\n  \"Gryffindor\", 2,           1,          3,             2,\n  \"Ravenclaw\",  1,           3,          3,             3,\n  \"Hufflepuff\", 4,           4,          4,             1,\n  \"Slytherin\",  3,           2,          1,             4,\n)\n\nharry_potter %&gt;%\n  left_join(ranks, by = \"House\")"
  },
  {
    "objectID": "posts/2017-10-14-mcmc-explainer/index.html",
    "href": "posts/2017-10-14-mcmc-explainer/index.html",
    "title": "MCMC and the case of the spilled seeds",
    "section": "",
    "text": "Earlier this month I did a post on simulated annealing, an algorithm that I learned in the class I‚Äôm currently taking: Advanced Statistical Computing taught by Chris Fonnesbeck here at Vanderbilt. Recently we have moved on to more traditionally ‚Äústatistics‚Äù algorithms, and one that Dr.¬†Fonnesbeck is particularly well versed in, Markov-Chain Monte-Carlo (or MCMC). ‚äïDr.¬†Fonnesbeck is the creator of the very popular python library pymc. In this post I hope to elucidate what MCMC is, why we need it, and show how one particular way of doing it (the metropolis hastings sampler) works."
  },
  {
    "objectID": "posts/2017-10-14-mcmc-explainer/index.html#what-is-mcmc",
    "href": "posts/2017-10-14-mcmc-explainer/index.html#what-is-mcmc",
    "title": "MCMC and the case of the spilled seeds",
    "section": "What is MCMC?",
    "text": "What is MCMC?\nIf you want to sound smart, next time someone asks you what you like thinking about, say ‚ÄúMarkov-Chain Monte-Carlo‚Äù, ‚äï I always think of Marco Polo when I think of Monte-Carlo. it‚Äôs a nice long mouthful of sciency soundings words that cause most people to zone out after about two syllables. What does it mean in normal language? Markov-Chain ‚äï Markov Chains are used in lots of cool places. For instance, generating fake text.refers to the idea of a sequence of draws such that the only thing that all you need to know about what the next value will be is the one just prior to it. Think of it like the mathematical equivalent to Dory from Finding Nemo, once she‚Äôs in a place she has no memory of where she was before and her next action is based only upon her current situation. Monte-Carlo refers to the act of estimating something by simulation. Basically the concept of, if you want to understand a process, repeat it a bunch of times and look at the aggregate behavior. ‚äïAnother example of monte carlo methods in statistics can be seen in my previous post Wait, what are P-values?"
  },
  {
    "objectID": "posts/2017-10-14-mcmc-explainer/index.html#why-do-we-need-mcmc",
    "href": "posts/2017-10-14-mcmc-explainer/index.html#why-do-we-need-mcmc",
    "title": "MCMC and the case of the spilled seeds",
    "section": "Why do we need MCMC?",
    "text": "Why do we need MCMC?\nWhen I say ‚Äòwe‚Äô in this context I mean Bayesian statisticians (which, while reading this article, you are), or statisticians that like to think in terms of probabilities instead of frequencies. I won‚Äôt go into the Bayesian vs Frequentest debate here as it would be way too long and boring, but there‚Äôs plenty of literature on it to be found. ‚äïLike all of life‚Äôs problems, stack overflow has a good answer.\nA huge reason why Bayesian statistics was so unpopular up until very recently is because when calculating the ‚Äúposterior‚Äù distribution a large portion of the time the math simply wasn‚Äôt tractable. To see what I mean it helps to think about the form of a posterior distribution.\n\\[\\begin{aligned}\n\\text{Posterior of parameter} &= \\frac{P(\\text{data given parameter}) \\cdot P(\\text{parameter})}{\\int P(\\text{data given parameter}) \\cdot P(\\text{parameter})}\n\\end{aligned}\\]\nThe top of this equation is rather straightforward, you simply multiply the distribution you believe your data is derived from by the distribution you believe the true parameter of interest follows. E.g. the density function of the normal distribution multiplied by the density function of another normal representing the distribution of the mean of the data‚Äôs distribution. The problem comes with the integral below. Since it‚Äôs integrating over the entire possible space of the parameter of interest it is simply a constant, but finding what constant it is is tricky. Even if you did some optimization method to find it, you still have the problem that most of the math you would want to do with the posterior distribution involves taking an integral over the same values again so just having the normalizing constant is not of much help. ‚äïOne of my mottos in life is, ‚Äòif the math is at all hard, just simulate it‚Äô, which turns out to be rather helpful in a lot of statistics.\nWhat MCMC does to solve this is, using the numerator of that posterior (also known as the un-normalized posterior), it generates data that fits the distribution, once you have a bunch of samples of data from a given distribution you can do almost any inference you would desire. Want to know how often you would see parameter values greater than \\(p\\)? Just take the samples MCMC generated for you and count how many were above \\(p\\) and divide by the total number of samples."
  },
  {
    "objectID": "posts/2017-10-14-mcmc-explainer/index.html#how-does-mcmc-work",
    "href": "posts/2017-10-14-mcmc-explainer/index.html#how-does-mcmc-work",
    "title": "MCMC and the case of the spilled seeds",
    "section": "How does MCMC work?",
    "text": "How does MCMC work?\nThere are a few ways of performing MCMC and the way I am going to demonstrate here (Metropolis-Hastings) is not the state of the art,‚äïFor a fantastic overview of the more cutting edge Hamiltonian Monte-Carlo methods, I recomend taking a look at the lecture notes for my class on github.  however, it is the most straightforward and intuitive method of MCMC and I think is valuable to understand. Essentially the algorithm works like this (I am going to demonstrate for a single parameter but the concept extends to multiple dimensions with the exact same logic):\n\nChoose a random initial value of the parameter of interest \\(\\theta_0\\)\nCalculate the value of your un-normalized posterior multiplied by this \\(\\theta_0\\) and remember it (\\(P_0\\)).\nRandomly jump from \\(\\theta_0\\) to some new value nearby, \\(\\theta_1\\).\nCalculate the value of your un-normalized posterior at this new location, \\(P_1\\).\nChoose to move to \\(\\theta_1\\) with probability proportional to how much better or worse \\(P_1\\) is than \\(P_0\\), if you don‚Äôt move make your next \\(\\theta\\) a duplicate of \\(\\theta_0\\).\nRepeat steps 3-5 for a while, recording each step‚Äôs \\(\\theta_i\\).\nTreat all of your recorded \\(\\theta\\)s as observations from the posterior distribution and relish in the wonderful bayesianness of this whole endeavor.\n\nLike all good algorithms there are knobs to turn. For instance, how big of a jump your algorithm takes at any given step is very important. Too large and it will almost never be accepted and the algorithm will take forever to converge to the posterior, too small and the steps nearby each other will be too correlated and the algorithm will focus too much on a given area of the posterior. For metropolis hastings the sweet spot seems to be making your jumps distributed widely enough to have roughly 23% of the proposals accepted. ‚äïOptimal Acceptance Rates for Metropolis Algorithms:Moving Beyond 0.234 is a good paper to read if you‚Äôre interested in how these things are calculated and why that single number is still not perfect.\nHowever, I‚Äôve just spewed a bunch of words and complex math terms at you, and I had the same thing done to me many times before I truly understood the algorithm. What got me to understand it? Playing with the knobs and seeing how it works. I‚Äôve tried to distill the algorithm to its fundamental parts below for you to tinker with yourself."
  },
  {
    "objectID": "posts/2017-10-14-mcmc-explainer/index.html#problem-setup",
    "href": "posts/2017-10-14-mcmc-explainer/index.html#problem-setup",
    "title": "MCMC and the case of the spilled seeds",
    "section": "Problem Setup",
    "text": "Problem Setup\nLet‚Äôs say you have a bird feeder on your porch and you have two bird species that visit it: a Cardinal ‚äï Cardinal and a Carolina Wren‚äï Carolina Wren. You have noticed that there tend to be a lot of seeds spilled on the ground around the feeder. This annoys you because you spent a lot of money on those seeds and those ungrateful birds are wasting a lot of them. You decide to Google the problem and find a handy website that lays out the seed spilling habits of both the Cardinal and the Carolina Wren. This site says that the Cardinal‚Äôs seed spillage amount per feeding follows a normal distribution with a mean of 65 and a standard deviation of 10, and the Carolina Wren‚Äôs happen to follow a normal distribution with a mean of 85 seeds and a standard deviation of 10 as well. ‚äïSometimes the birds carry seeds with them and thus can possibly spill negative seeds, so this truly is normal‚Ä¶\nThe thing is, you want to know what the makeup of your bird spillers are so you can know which bird species to be more mad at. You put a scale beneath the feeder that can automatically measure the spillage from a single feeding. ‚äïIt‚Äôs a single bird feeder so only one bird feeds at a time. So you have data on how much seed was spilled at each feeding, and you know the distribution of spillage for each bird species, but you have a job so you can‚Äôt actually sit there and watch the birds feed, plus you understand that the real distribution of birds feeding probably is just that, a distribution. What can you do? You can do Bayesian statistics!\nThe data likelihood is a normal mixture: \\(L(\\delta|\\text{seeds}) = \\delta N(\\text{Cardinal}) + (1 - \\delta)N(\\text{Carolina Wren})\\)\nYou don‚Äôt have any real idea about which bird will be more likely so your prior is a uniform distribution over the range 0 and 1 (\\(U(0,1)\\)).\n‚äïTurns out they actually make products to catch spilled seeds so this convoluted example isn‚Äôt too crazy. \nYour proposal distribution is another random uniform with a width of 2, aka \\(U(-1,1)\\).\nYou can fiddle with the sliders below to change the true mean of seeds dropped by two birds, the true percentage of times the Cardinal is at the feeder, the size of your observation data and how wide your proposals are. Play around with the dials to see how the algorithm reacts. The plot below will show the real-time results of MCMC running including the current 95% credible interval (aka what people think a confidence interval is) and the acceptance rate. The algorithms proposed values for the proportion that are rejected are displayed as red circles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere are a few experiments you may want to run to explore the algorithm‚Äôs behavior:\n\nCan you fool the credible interval? Try setting the true means of the two birds very close and your number of observations really low, intuitively this will be much harder to discern between each bird in the data.\nSee how increasing the number of observations results in a skinnier posterior.\nIf you‚Äôre really adventurous, try forking the code for this simulation from github and switching the prior to a beta distribution in this file. The beta will allow you to give opinionated priors about the distribution of the parameter of interest, but sometimes at a cost of accuracy.\n\nStill confused? MCMC is non-trivial to say the least, so don‚Äôt feel bad if you don‚Äôt get it. Please let me know in the comments or on twitter what is confusing so I can try and make this explanation better!"
  },
  {
    "objectID": "posts/2017-08-14-pvalues_can_be_inappropriate/index.html",
    "href": "posts/2017-08-14-pvalues_can_be_inappropriate/index.html",
    "title": "Why you maybe shouldn‚Äôt care about that p-value",
    "section": "",
    "text": "Recently, there seems to have been an uptick in citations of studies or statistics about this or that in the news and on the internet. As a statistician I think this is great. Anytime we can start to bring objectivity into our assessments of situations it‚Äôs fantastic. ‚äïThis is not to say all data/ stats are objective. Very frequently they can be very dangerously not so. However, I have also noticed some incorrect conclusions being drawn on the basis of these statistics. The following is a small example of a situation where a p-value might not technically be wrong, but simply does not do justice to the question at hand."
  },
  {
    "objectID": "posts/2017-08-14-pvalues_can_be_inappropriate/index.html#setup",
    "href": "posts/2017-08-14-pvalues_can_be_inappropriate/index.html#setup",
    "title": "Why you maybe shouldn‚Äôt care about that p-value",
    "section": "",
    "text": "Recently, there seems to have been an uptick in citations of studies or statistics about this or that in the news and on the internet. As a statistician I think this is great. Anytime we can start to bring objectivity into our assessments of situations it‚Äôs fantastic. ‚äïThis is not to say all data/ stats are objective. Very frequently they can be very dangerously not so. However, I have also noticed some incorrect conclusions being drawn on the basis of these statistics. The following is a small example of a situation where a p-value might not technically be wrong, but simply does not do justice to the question at hand."
  },
  {
    "objectID": "posts/2017-08-14-pvalues_can_be_inappropriate/index.html#scenario",
    "href": "posts/2017-08-14-pvalues_can_be_inappropriate/index.html#scenario",
    "title": "Why you maybe shouldn‚Äôt care about that p-value",
    "section": "Scenario",
    "text": "Scenario\nSay you‚Äôre hungry for lunch one day and there are two restaurants just opened up nearby, Tony‚Äôs Tacos and Sheela‚Äôs Sashimi. You want to go to one of them but can‚Äôt decide which one. You happen to like both foods exactly the same and are bad at making decisions so you decide to go onto the (fictional) restaurant rating site FishersRestaurantReviews.com.\n‚äïTaco Tuesday  or Sushi Cat? \nThis rating site lets users rate restaurants they went to on a scale from 0 (this restaurant makes me question humanity‚Äôs choice to eat food) to 1 (this makes me question my choice to ever not eat food). You plug in both Tony‚Äôs Tacos and Sheela‚Äôs Sashimi into the search system and out pops the result: ‚Äúgo to Sheela‚Äôs Sashimi, it‚Äôs better than Tony‚Äôs Tacos p &lt; 0.05‚Äù.\nBased on this concrete evidence, you decide to go to Sheela‚Äôs Sashimi and come back mildly satisfied. Soon, your coworker who didn‚Äôt look at the rating website comes back from Tony‚Äôs Tacos and raves about the food. Disappointed and confused as to how this could be, you decide to go back to the food rating service and pull all reviews for Tony‚Äôs and Sheela‚Äôs from their handy dandy reviews api‚Ä¶\n\n# Pull data from api\nsheelas_reviews &lt;- ratingsApiQuery(restaurant = \"Sheela's Sashimi\")\ntonys_reviews &lt;- ratingsApiQuery(restaurant = \"Tony's Tacos\")\nhead(sheelas_reviews)\n## [1] 0.1731754 0.4026381 0.3552855 0.8238345 0.9141786\n## [6] 0.1322581\n\nGreat, now that you have the data let‚Äôs run a t-test to check if Sheela‚Äôs really is better.\n\n# Do a T-test on the data to see if the ratings website is correct\n(t_results &lt;- t.test(sheelas_reviews,tonys_reviews,alternative=\"greater\"))\n## \n##  Welch Two Sample t-test\n## \n## data:  sheelas_reviews and tonys_reviews\n## t = 1.9131, df = 3991, p-value = 0.0279\n## alternative hypothesis: true difference in means is greater than 0\n## 95 percent confidence interval:\n##  0.001878156         Inf\n## sample estimates:\n## mean of x mean of y \n## 0.4942271 0.4808139\n\nWell look at that, the ratings website at least seems to be getting their two-sample t-tests correct. But still, how could it be that you had a worse experience at Sheela‚Äôs than your coworker did at Tony‚Äôs? Maybe the API is broken, let‚Äôs just visualize the data to see if something seems astray‚Ä¶\n\n# Combine data into a dataframe for investigation\nreview_data &lt;- rbind(\n  data_frame(rating = sheelas_reviews, restaurant = \"Sheela's Sashimi\"),\n  data_frame(rating = tonys_reviews, restaurant = \"Tony's Tacos\")\n)\n## Warning: `data_frame()` was deprecated in tibble 1.1.0.\n## ‚Ñπ Please use `tibble()` instead.\n\n# plot the ratings.\nrating_dists &lt;- ggplot(review_data, aes(rating)) + \n  geom_histogram(bins = 100) +\n  facet_grid(restaurant ~ .) +\n  labs(title = \"Distributions of Restaurant Reviews\")\n\nrating_dists\n\n\n\n\nWait, what? Those look like they‚Äôre identical, how in the world could our p-value be so significant. Sheela‚Äôs should be clearly better than Tony‚Äôs? Right?\nTo check your logic you flip open your dusty intro-stats book and look at the *deep breath* Student‚Äôs two-sample-equal-sample-size-equal-variance-t-test formula.\n\\[T = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p \\sqrt{2/n}}\\] ‚äïWe can ignore the \\(s_p\\), it‚Äôs just that annoyingly complicated pooled standard deviation that literally noone has ever memorized.\nSuddenly you remember, ‚Äòthat‚Äôs right, we‚Äôre testing if the average review differs!‚Äô Even so, how does this make sense? Those distributions up there look practically identical. Let‚Äôs put a mean line on them to see if we need glasses or something.\n\n# Find each restaurants average rating\nreview_means &lt;- review_data %&gt;% \n  group_by(restaurant) %&gt;% \n  summarise(average = mean(rating))\n\n# Plot distributions again with the average imposed over this time. \n# plot the ratings.\nrating_dists + \n  geom_vline(data = review_means, aes(xintercept = average), color = \"orangered\", size = 2) + \n  geom_text(data = review_means, \n            aes(x = average + 0.01, y = 5, label = sprintf(\"average = %3.2f\",average) ), \n            hjust = 0, color = \"orangered\")\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2\n## 3.4.0.\n## ‚Ñπ Please use `linewidth` instead.\n\n\n\n\nSo they are different but by 0.01‚Ä¶ this is hardly what one would call ‚Äúsignificant‚Äù. What‚Äôs going on?"
  },
  {
    "objectID": "posts/2017-08-14-pvalues_can_be_inappropriate/index.html#why-the-p-value-is-misleading",
    "href": "posts/2017-08-14-pvalues_can_be_inappropriate/index.html#why-the-p-value-is-misleading",
    "title": "Why you maybe shouldn‚Äôt care about that p-value",
    "section": "Why the p-value is misleading",
    "text": "Why the p-value is misleading\nHere in lies the problem with p-values. In this case our p-value for ‚ÄúSheela‚Äôs is better than Tony‚Äôs‚Äù is formally saying, if we were to re-poll an identical group of reviewers an infinite amount of times to rate these two restaurants, and they truly were the same quality, then the chances of us getting reviews who‚Äôs means favor Sheela‚Äôs restaurant as much or more as they do in our data, is 0.028. Aka it‚Äôs unlikely. ‚äïIt took me a solid minute of thinking through this in my head to make sure I wrote it down properly. How any statistician complains about people misinterpreting p-values is a mystery to me.\nThe thing is, when you choose to eat at a restaurant you don‚Äôt care about the asymptotic behaviors of the average review, you care about if you are going to enjoy your particular meal. In other words, you care about an individual realization of the distribution you are modeling. In the above scenario I don‚Äôt think anyone would look at these data and say going to Sheela‚Äôs will for sure result in a better experience. ‚äïP-values make statements about populations, very frequently we‚Äôre not interested in the population.\nP-values and indeed much of common statistical methodology relies on summary statistics. This fact is very easy to forget, I do all the time and I‚Äôm literally paid to understand statistics.\n ‚äïPerhaps we should stop looking at the forest and look at the trees. Image source"
  },
  {
    "objectID": "posts/2017-08-14-pvalues_can_be_inappropriate/index.html#what-can-we-do-about-this",
    "href": "posts/2017-08-14-pvalues_can_be_inappropriate/index.html#what-can-we-do-about-this",
    "title": "Why you maybe shouldn‚Äôt care about that p-value",
    "section": "What can we do about this?",
    "text": "What can we do about this?\nWhile this example may seem contrived (and it really is) I think that taking a step back every time we see a p-value or cite some scientific result that compares two groups and think about what exactly is being compared. Are we making judgement about individuals based upon summary statistics of populations? Could tiny perturbations caused by un(observed/ accounted for) factors be enough to tip that average into ‚Äústatistical significance‚Äù. For instance perhaps those who like sushi are a tiny bit more likely to leave a positive review, thus biasing the sample a tiny bit to Sheela‚Äôs, which in turn results in a significant p-value even though there‚Äôs no functional difference between the two.\nI absolutely don‚Äôt have a fix to this problem but I will throw in my biased piece of advice for not falling for the summary statistics = individual trap: plot.\nThe above is an example of this. If we just relied on the p-values to decide for us we would have never realized that the two restaurants were essentially identical in terms of their ratings. Even worse we may have taken the bias we gained from our knowledge of the p-value and had it effect our experience and subsequent ratings. Perhaps the tiny difference in mean is entirely due to situations like that, in which case, even the p-value when interpreted in it‚Äôs strict and horribly esoteric way is wrong."
  },
  {
    "objectID": "posts/2017-08-14-pvalues_can_be_inappropriate/index.html#adendum",
    "href": "posts/2017-08-14-pvalues_can_be_inappropriate/index.html#adendum",
    "title": "Why you maybe shouldn‚Äôt care about that p-value",
    "section": "Adendum",
    "text": "Adendum\nTurns out our review data for the two restaurants happen to exactly follow a beta distribution! How weird.\n\nn &lt;- 2001\nalpha &lt;- 8\nbeta_s &lt;- 2\nbeta_t &lt;- 2.15\n\nratingsApiQuery &lt;- function(restaurant){\n  if (restaurant == \"Sheela's Sashimi\"){\n    return(rbeta(n, beta_s, beta_s))\n  } else {\n    return(rbeta(n, beta_s, beta_t))\n  }\n}"
  },
  {
    "objectID": "posts/2020-05-04-may-the-fourth/index.html",
    "href": "posts/2020-05-04-may-the-fourth/index.html",
    "title": "May the Fourth Be With You (#rstats style)",
    "section": "",
    "text": "Happy Star Wars Day!\nRafael Irizarry has made a fabulous TIE fighter plot.\n\n\nHappy #MayFourth #rstatspar(bg=1,fg=\"white\")x&lt;-0.5-&gt;yz&lt;-\"|-o-|\"s&lt;-cbind(runif(50),runif(50))m&lt;-c(-1,1)/20while(TRUE){ rafalib::nullplot(xaxt=\"n\",yaxt=\"n\",bty=\"n\") points(s,pch=\".\") text(x,y,z, cex=4) x&lt;-pmin(pmax(x+sample(m,1),0),1) y&lt;-pmin(pmax(y+sample(m,1),0),1)} pic.twitter.com/2kBwklMjTy\n\n‚Äî Rafael Irizarry (@rafalab) May 4, 2018\n\n\nJake Thompson recreated this plot using the {gganimate} package. I posted this amazing recreation in celebration of Star Wars Day (May the Fourth be with YOU!), and Rafa pointed out that the stars are missing!\n\n\nWhere are the stars ‚ú® though? üòÅ\n\n‚Äî Rafael Irizarry (@rafalab) May 4, 2020\n\n\nSo I‚Äôve taken it upon myself to add them. Here you go!\n\nlibrary(tidyverse)\nlibrary(gganimate)\nset.seed(20200504)\n\nlocations &lt;- 20\nn_stars &lt;- 50\ntie_data &lt;- tibble(\n  id = rep(seq_len(locations), each = n_stars),\n  x = rep(runif(locations), each = n_stars),\n  y = rep(runif(locations), each = n_stars),\n  star_x = rep(runif(n_stars), locations),\n  star_y = rep(runif(n_stars), locations),\n  label = \"|-o-|\"\n)\n\n\nggplot(tie_data, aes(x = x, y = y, label = label)) +\n  geom_point(color = \"white\", aes(star_x, star_y)) +\n  geom_text(color = \"white\", fontface = \"bold\", size = 12) +\n  expand_limits(x = c(-0.1, 1.1), y = c(-0.1, 1.1)) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"black\")) +\n  transition_states(id, transition_length = 4, state_length = 1)\n\n\n\n\nCheck out the other amazing Star Wars - R content from the fabulous #rstats community!\n\n\nHappy #MayThe4th! In honor of this special day, I've curated a list of my favorite #StarWars and #rstats crossovers üíï üßµ 1/n pic.twitter.com/A4HTZesxl8\n\n‚Äî Lucy D‚ÄôAgostino McGowan (@LucyStats) May 4, 2020"
  },
  {
    "objectID": "posts/2017-01-17-regression-modeling-strategies-a-students-perspective/index.html",
    "href": "posts/2017-01-17-regression-modeling-strategies-a-students-perspective/index.html",
    "title": "Regression modeling strategies: a student‚Äôs perspective",
    "section": "",
    "text": "Frank Harrell teaches an amazing course ‚ÄúRegression Modeling Strategies‚Äù based on his book each spring at Vanderbilt.\nThis was one of my all time favorite courses. It has just the right amount of practical strategies, brilliant statistical insight, and zealous disdain for all things stepwise regression. In fact, Frank‚Äôs valiant fight against dichotomization inspired the name of this very blog.\nThis semester, Nick is enrolled in the course, and I will be TAing it, so we thought it would be a perfect time to do what all good 21st century students do, blog about it. We will be using the #rms tag to thread the series to make it easy to follow along.\nTo kick things off, I encourage everyone to get a flavor of our beloved professor‚Äôs style by trying out the following R code:\nlibrary(\"fortunes\")\nfortune(\"Harrell\")\nHere is one of my personal favorites:\n\n\n\nThere are companies whose yearly license fees to SAS total\nmillions of dollars. Then those companies hire armies of\nSAS programmers to program an archaic macro language using\nold statistical methods to produce ugly tables and the\nworst graphics in the statistical software world.\n   -- Frank Harrell\n      R-help (November 2004)\n\n\nWe have been planning on kicking this series off for a while, but it happens to serendipitously coincide with Frank jumping online via twitter and a blog! We are so excited to see how this goes."
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "",
    "text": "We‚Äôve updated our R Markdown website tutorial to depend on RStudio for simplicity. You can find our previous version if you would rather not depend on RStudio."
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#setup",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#setup",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "Setup",
    "text": "Setup\nToday, more than ever, a website is like a business card. As a graduate student or academic, by having a nice website you are not only providing a one-stop-shop for all of your necessary information, ‚äïAND you are illustrating that you have the technical prowess to accomplish this! üí™ üë©‚Äçüíª you are showing that you are savvy enough to know the importance of a high-quality web-presence."
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#examples",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#examples",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "Examples",
    "text": "Examples\n\nCheck out Lucy‚Äôs entirely made in R Markdown personal website!\nThe sample website made in this tutorial is available here."
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#prerequisites",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#prerequisites",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "Prerequisites",
    "text": "Prerequisites\n‚äïGet a GitHub account! 1. GitHub account\n‚äïWhat is R Markdown you ask? 2. Working knowledge of R Markdown.\n‚äïDownload RStudio! 3. RStudio\n4. Collection of all the reasons your awesome! ‚äïSometimes it feels a bit üò¨ to list your accomplishments, but be loud and proud on your personal website!"
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#goals",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#goals",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "Goals",
    "text": "Goals\nBy the end of this (hopefully) you will have,\n\nconstructed a simple website with basic information about yourself,\nhosted it to GitHub for the world to access,\nhave enough knowledge to know what to google to make it better.\n\nOkay, let‚Äôs get started."
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#step-1-git-on-with-it",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#step-1-git-on-with-it",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "Step 1: Git on with it",
    "text": "Step 1: Git on with it\nFirst things first, let‚Äôs set up our GitHub repository for hosting this site.\n‚ÄúHosting this site‚Äù?\nWhenever you go to a website, e.g.¬†www.vanderbilt.edu, your computer is sending out a request across the series of tubes known as the internet to a server sitting on top of some cloud somewhere (aka Indiana) that it would like to look at Vanderbilt‚Äôs website. That server, which is simply another computer, receives the request, then goes into its hard drive and pulls up the file it has stored for vanderbilt.com and sends that file back to your computer. So when we say ‚Äúhost your site‚Äù we simply mean we need to find a server to put your website‚Äôs files on that will then deliver those sites to people who want to see them via their web browser of choice.\nThis all sounds very complicated and expensive, and it used to be, but now computation is so cheap that companies literally give away server space to people all the time. One example of this is GitHub. Every time you host a repository on GitHub it is stored on a server for access.\nCreate Repo\nClick the plus icon in the upper right corner of your GitHub page and select ‚ÄúNew repository‚Äù. If you‚Äôd like this to be your main website, name this repo yourgithubname.github.io for example, Nick‚Äôs would be nstrayer.github.io, Lucy‚Äôs would be lucymcgowan.github.io, etc.\n\nOn the top, you should see a box with your Repository‚Äôs URL. Hover over the clipboard icon to copy this URL.\n\nKeep this on your clipboard, we are going to use it in just a minute!"
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#step-2-rstudio-magic",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#step-2-rstudio-magic",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "Step 2: RStudio magic ‚ú®",
    "text": "Step 2: RStudio magic ‚ú®\nOpen RStudio and select ‚ÄúNew Project‚Äù under the ‚ÄúFile‚Äù menu item. ‚äïIf you are having some trouble here, check out Jenny‚Äôs delightful Happy git with R tutorial. A New Project box should pop up - click ‚ÄúVersion Control‚Äù.\n\nNow click ‚ÄúGit‚Äù.\n\nPaste your git repo link in the ‚ÄúRepository URL‚Äù dialogue box. (If it is no longer on your clipboard, navigate back to GitHub and select ‚ÄúClone or download‚Äù and copy the link again).\n\nClick ‚ÄúCreate Project‚Äù üéâ. Okay good! Now we have a repo and an RStudio project setup. Now let‚Äôs actually get a website on it!"
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#step-3-start-your-markdowns",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#step-3-start-your-markdowns",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "Step 3: Start your Markdowns!",
    "text": "Step 3: Start your Markdowns!\nFirst we do some administrative work to make sure we don‚Äôt run into roadblocks on the way. ‚äïThese steps all take place within the RStudio project we just created! Let‚Äôs update our rmarkdown package to make sure we actually have the version that supports R Markdown websites.\n\ninstall.packages(\"rmarkdown\", type = \"source\")\n\nNext we need to create a couple empty files inside your repository.\n\nUnder ‚ÄúFile‚Äù select ‚ÄúNew File‚Äù then ‚ÄúR Markdown‚Äù - save this file as ‚Äúindex.Rmd‚Äù.\nUnder ‚ÄúFile‚Äù select ‚ÄúNew File‚Äù then ‚ÄúR Markdown‚Äù - save this file as ‚Äúabout.Rmd‚Äù.\nUnder ‚ÄúFile‚Äù select ‚ÄúNew File‚Äù then ‚ÄúText File‚Äù - save this file as ‚Äú_site.yml‚Äù.\n\nWe will start by filling out the yml file. yml files, while confusing looking at first, are basically a road map for R to know how to assemble your website.\n_site.yml\n\nname: \"personal-website\"\noutput_dir: \".\"\nnavbar:\n  title: \"Personal Website\"\n  left:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About Me\"\n      href: about.html\n\nNext we will fill out the bare minimum for the .Rmd files.\nindex.Rmd\n\n---\ntitle: \"Personal Website\"\n---\n\nHello, World!\n\nabout.Rmd\n\n---\ntitle: \"About Me\"\n---\n\nWhy I am awesome. \n\nIf you got lost at any point during this tutorial, you can download a template of these files from Lucy‚Äôs GitHub."
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#step-4-lets-build-it",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#step-4-lets-build-it",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "Step 4: Let‚Äôs build it!",
    "text": "Step 4: Let‚Äôs build it!\nOkay, one last step to actually have a functioning website. We need to actually turn these separate files into a single cohesive website. Simply run the following within your RStudio project.\n\nrmarkdown::render_site()\n\nNow if everything has gone according to plan, you should get a bunch of unintelligible output followed by the message : Output created: index.html. ‚äï If so, yay üôå, if not, double check all the stuff above to make sure you followed it exactly. Or more likely I messed up and you should inform me.\nNow we can open it up. Open the repository with finder or whatever tool your computer uses to look at files, then click on index.html and hopefully you should get something that looks like this.\n\nSweet. You have now created your own personal website. First let‚Äôs push it to GitHub and then we can get down to making it good for you."
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#step-5-git-it-hosted",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#step-5-git-it-hosted",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "Step 5: Git it hosted",
    "text": "Step 5: Git it hosted\n‚äïEmoji tip: You can include emoji in your git commit messages by keyword - checkout this list üéâ üçª üêìNow we just have to add commit and push everything to GitHub. In the upper right corner of RStudio, you should see a ‚ÄúGit‚Äù tab. Click the ‚ÄúCommit‚Äù button. The following box should pop up.\n\n‚äïIf you are having trouble with this step, take a üëÄ at Jenny‚Äôs lovely Happy git with R. Click all of the files to stage them, type a commit message, and then click ‚ÄúCommit‚Äù. The click the green up arrow to push the commit to GitHub.\n\nChoose your own adventure\nI am updating a bit based on Yihui‚Äôs comment - it sounds like the cool kids are using Netlify now üòé!\nNow that your site is on GitHub, you have some hosting options - you can either host it on GitHub, or another hosting site.\nHost on Netlify\nHosting on Netlify is actually quite simple! Just bop over to https://www.netlify.com, sign in with GitHub, and choose the Repository you‚Äôve just pushed your site to. You should see a dialogue box - just click ‚ÄúDeploy Site‚Äù and all will be well!\n\n‚äïFor a more detailed explanation, Yihui has a great section in his book. You can update your site‚Äôs name by clicking ‚ÄúChange site name‚Äù - if you are using the free plan, it will append .netlify.com to the end. For example, I have put this example site on http://lucys-personal-site.netlify.com/.\nA clear added benefit of using Netlify is the HTTPS support - so be sure to enable that!\nHost on GitHub\nReturn to your GitHub repository in the browser. Click on the ‚ÄúSettings‚Äù tab.\n\nScroll down to the ‚ÄúGitHub Pages‚Äù header. Under ‚Äúsource‚Äù select ‚Äúmaster branch‚Äù and then click ‚ÄúSave‚Äù.\n\nNow we can navigate to our hosted site! Open your browser of choice and go to www.&lt;your github name&gt;.github.io/&lt;your sites repo name&gt;. (E.g. www.lucymcgowan.github.io/personal_site).\nNote: GitHub has to build stuff on its end so it may take a minute or so for stuff to show up. Just keep impatiently refreshing the page and it will go faster.\nYay, it works. Now we can make it better."
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#show-the-world-who-you-are",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#show-the-world-who-you-are",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "Show the world who you are",
    "text": "Show the world who you are\nYou know how to use R Markdown. So basically everything that you know how to do you can do here.\nLike let‚Äôs say you want to make your about page more descriptive.\nabout.Rmd\n---\ntitle: \"About Me\"\n---\n\n- __Name:__ Nick\n- __Ocupation:__ \"Student\"\n- __Hobbies:__ Learning software development instead of studying for exams. \n\nHere is a super cool photo of me doing one of my favorite things, yawning. \n\n![](me_yawning.jpg)\nNow just rebuild your site by running rmarkdown::render_site() again and open index.html again to see if it worked. Ideally now you should be able to click on your about page and see the new results!\n\nOh my, that photo looks mighty large. Perhaps we want to make it smaller. We can do that, by adding a special styling file called a css file. Create a new ‚ÄúText file‚Äù in RStudio named ‚Äústyle.css‚Äù and add the following lines:\nstyle.css\n\nimg {\n  width: 400px;\n  display: block;\n  margin: 0 auto;\n}\n\nThis takes every image that appears on our site and makes them 400 pixels wide and centers them. You can change these parameters as you want. There are infinitely many ways to customize the style of a website using css. For more information try googling how to &lt;do something&gt; with css and you will most likely find 10,000 ways to do it.\nNow just add the following lines to your _site.yml file to apply this css to your site.\n_site.yml\n\nname: \"nicks-website\"\noutput_dir: \".\"\nnavbar:\n  title: \"Nicks Website\"\n  left:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About Me\"\n      href: about.html\noutput:\n  html_document:\n    theme: flatly\n    css: style.css\n\nWe have done a few things here. One we have created the new output field. We have given it a theme (you can choose from any you desire here) and we have added our custom css file to the whole thing as well.\nOnce again, run rmarkdown::render_site() to checkout how things have changed.\n\nLooking a lot better.\n‚äïMaybe you‚Äôre not ü§∑‚Äç‚ôÄ ‚Ä¶we made this originally for our department. You are a biostatistician however, so how about we try and show that off.\n\nAdd Projects/ other links\nLet‚Äôs make a page with links to your cool (open) projects.\nAgain we edit the _site.yml file‚Ä¶\n_site.yml\n\nname: \"nicks-website\"\noutput_dir: \".\"\nnavbar:\n  title: \"Nicks Website\"\n  left:\n    - text: \"Home\"\n      href: index.html\n    - text: \"Projects\"     ##### the new\n      href: projects.html  ##### stuff\n    - text: \"About Me\"\n      href: about.html\noutput:\n  html_document:\n    theme: flatly\n    css: style.css\n\nAdd another file called projects.Rmd (you know how to do this at this point).\nprojects.Rmd\n\n---\ntitle: \"Projects\"\n---\n\nSometimes I like to do projects and then post them on the internet for the whole world to benefit!\nhere's some examples. \n\n## [Data Visualization in R](http://nickstrayer.me/visualization_in_r/)\n\n- An RMarkdown presentation on the common mistakes made in visualization and how to fix them.\n- Includes a GitHub repo for access to all the code.\n- Look at how high quality my work is, hire and or collaborate with me. \n\n## [Statistical Plots](http://bl.ocks.org/nstrayer/37a503dd1db369a8f7e3ce21757e19ee)\n\n- Interactive plots of things\n- I can code!\n\nAgain, build the site with your build script and then take a look at what you have!"
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#so-what-now",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#so-what-now",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "So what now?",
    "text": "So what now?\nWell first off you add, commit, and push all your new fancy changes to GitHub.\nNow you have a website that is better than 95% of people in your situation. What do you do now?\nYou never stop making it better! Every new project you get you do you post it to your projects page, get sick new head shots in? Add that to your about page. Customize it. For instance you may want to add a splash of personalization to your main page. Perhaps a nice chart? Ever made a plot in an R Markdown before? You know how to do it then.\nindex.Rmd\n---\ntitle: \"Nick's Website\"\n---\n\n__Look at how cool this plot is!__\n\n$$Y = \\alpha \\cdot \\sin(X), \\alpha = 0,0.1,0.2,...,3$$\n\n\n## Remove # before ticks to get this to work. \n#```{r, echo = FALSE, fig.align='center'}\nlibrary(tidyverse)\ncool_function &lt;- function(x, alpha) return(sin(alpha*x))\nxs &lt;- seq(0, pi*1.5, 0.005)\nys &lt;- cool_function(xs, 1)\nresults &lt;- data_frame(xs, ys, alpha = \"1\")\nfor(alpha in seq(0,3, 0.1)){\n  results &lt;- results %&gt;% \n    bind_rows(data_frame(\n      xs, \n      ys = cool_function(xs, alpha),\n      alpha = as.character(alpha)\n    ))\n}\n\nggplot(results, aes(x = xs, y = ys, color = alpha)) + \n  geom_line() + \n  theme_bw() + \n  theme(legend.position=\"none\")\n#```\n\nEveryone likes a good chart."
  },
  {
    "objectID": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#afterward",
    "href": "posts/2017-08-08-how-to-make-an-rmarkdown-website/index.html#afterward",
    "title": "How to make an R Markdown website (with RStudio!)",
    "section": "Afterward",
    "text": "Afterward\nLearn More\nIf you want to learn more about what‚Äôs possible using R Markdown sites (and there is a lot), a good starting place is this document hosted by RStudio. This is where I got almost everything for this.\nStyle\nThis is rather personal, but as for the style: keep it as simple as possible. The simpler your website the less overwhelming it is to viewers. Try and convey everything that you need to and nothing more. That being said, personal flourishes do sometimes help set you apart from others. It‚Äôs a balancing act.\nQuestions ‚äï\nIf this tutorial was confusing or you don‚Äôt know what words to use to search for new stuff feel free to contact us."
  },
  {
    "objectID": "posts/2018-09-14-one-year-to-dissertate/index.html",
    "href": "posts/2018-09-14-one-year-to-dissertate/index.html",
    "title": "One year to dissertate",
    "section": "",
    "text": "I‚Äôve compiled some resources that I used when completing my dissertation and I wanted to share them with YOU! Throughout this post, I link to a bunch of different templates that I used throughout my process. You can find them all in this GitHub repo.\nI‚Äôve split this post into three sections:\nThis how-to has gotten a biiiiiit long. This post contains the whole kit-and-caboodle, but I will also be releasing these in a series of smaller posts over the next couple of weeks, so if this seems overwhelming, stay tuned for that!"
  },
  {
    "objectID": "posts/2018-09-14-one-year-to-dissertate/index.html#why-am-i-writing-this",
    "href": "posts/2018-09-14-one-year-to-dissertate/index.html#why-am-i-writing-this",
    "title": "One year to dissertate",
    "section": "Why am I writing this?",
    "text": "Why am I writing this?\n\n\nThis is by no means the only way to complete a dissertation (or even the best way!) but maybe you can find something useful from my process ü§∑.\nSpeaking with current and former PhD students, many of us find / found one of the most difficult parts of the PhD process was the element of unstructured time. I thought it may be useful to lay out one way to structure this time. For that reason, I am specifically focusing on the dissertating phase of a PhD rather than the coursework portion."
  },
  {
    "objectID": "posts/2018-09-14-one-year-to-dissertate/index.html#background",
    "href": "posts/2018-09-14-one-year-to-dissertate/index.html#background",
    "title": "One year to dissertate",
    "section": "Background",
    "text": "Background\nPrograms come in all shapes and sizes, so this may not fit your experiences.\nFor a bit of context:\n\nI graduated from Vanderbilt‚Äôs Biostatistics program\nAt Vanderbilt, we have 2.5 years of mandatory course work\nWe have two written comprehensive exams taken at the end of Year 1 and the end of Year 2\nAround the 2.5 year mark (January of Year 3), we have an oral exam - this (roughly) marks the start of your dissertating phase\nAfter I completed my oral exam, I had the goal of finishing up my dissertation in 1 year\n\nI think the tools I am going to lay out here could be easily applied to other timelines if you just shrink or stretch each phase."
  },
  {
    "objectID": "posts/2018-09-14-one-year-to-dissertate/index.html#the-practicals",
    "href": "posts/2018-09-14-one-year-to-dissertate/index.html#the-practicals",
    "title": "One year to dissertate",
    "section": "The practicals",
    "text": "The practicals\n\nDissertation requirements\nTo start off, you‚Äôll need your department‚Äôs dissertation requirements. This is sometimes vague, for example our department‚Äôs website just states:\n\nThe dissertation committee is responsible for administering the final PhD examination (i.e., dissertation defense) and will determine whether the candidate has presented an acceptable dissertation and has demonstrated strong proficiency in the five key skill areas during their educational tenure at Vanderbilt‚Ä¶Candidates for the PhD degree in Biostatistics must present a dissertation that yields clear evidence of original research and thinking, and that advances methodological knowledge in the discipline of Biostatistics.\n\nI found it helpful to set up a meeting with my advisor to specifically outline his expectations. I found it really helpful to record all expectations in writing and have them reaffirmed via email after the meeting. This may seem obvious (or may seem like overkill!), but I think clear communication is one of the common roadblocks in the dissertation process. Here is a worksheet I‚Äôve put together that can help with this.\n\nClick here to download the file from GitHub.\n‚äïNote that this can be revised lots of times! My third dissertation paper ended up being on a completely different topic than what was originally discussed ‚Äì I think this is totally normal!\nHere is (roughly) what mine would have looked like at the beginning.\n\n\n\nCommittee Meetings\nI had committee meetings every three months. This was double the number required, but I found it really helpful for keeping my timeline. In order to get away with hosting so many meetings with many Very Busy people, these meetings had to be informative and efficient. I did a few things to try to achieve this, here are my recommendations:\n\nSend a draft of the dissertation chapter (or whatever content) you will be focusing on in the meeting one week prior.\nIn addition to the dissertation chapter, include a short paragraph with a summary of changes directing the reader to specific paragraphs. This makes it a bit more likely that they may open the draft (or at least skim the summary paragraph), making it possible to dive right into the material at the beginning of the meeting.\nCreate an agenda for the meeting. I am a HUGE fan of agendas. Here is the format mine took:\n\n\n‚äïNot familiar with .Rmd files? Check out these awesome resources from RStudio.\nClick here to download the .Rmd file from GitHub.\n\nSend a follow up email after the meeting with the meeting minutes and a poll to schedule the subsequent meeting. For this, I liked to just fill in my agenda with the next steps we discussed. It would often look something like this.\n\n\nClick here to download the .Rmd file from GitHub.\n\nCompile a final document with the meeting minutes, the date for the next committee meeting, and a line indicating that all committee members signed off on the current plan. Here is what mine often looked like.\n\n\nClick here to download the .Rmd file from GitHub.\n\n\nDrafting your thesis\nThere are lots of great resources on how to write academic papers. My favorite is Jeff Leek‚Äôs first paper GitHub repo. My advice here is going to focus more on the practicals of drafting your dissertation paper(s): file structure, using bookdown, and file sharing.\nThese sections assume you are interested in writing your thesis using R. If not, you may want to skip down to the next section.\nFile structure\nThe most important part of having a nice structure for your files is that it is consistent. I‚Äôm going to briefly discuss how I do this, but it isn‚Äôt really important how you structure your files, you just want to be sure to do something logical (this will save lots of time later when doing revisions / getting feedback!).\nThe first part of constructing a lovely file structure is proper naming. Jenny Bryan has a beautiful slide deck on ‚ÄúNaming Things‚Äù. The main gist is file names should follow the following principles:\nü§ñ Machine readable\nüíÅ Human readable\nü§∏‚Äç‚ôÄ plays well with default ordering\nNow that we have beautiful names, here is generally how I structure my files:\nthesis \n  |_ 01-chapter1.Rmd\n  |_ 02-chapter2.Rmd\n  |_ 03-chapter3.Rmd\n  |_ 04-conclusion.Rmd\n  |_ 05-references.Rmd\n  |_ /_book\n    |_ _main.pdf\n  |_ citations.bib\n  |_ /data  \n  |_ index.Rmd \n  |_ /notes  \n  |_ /scripts  \n    |_ compile.R\n    |_ send-to-drive.R  \n  |_ /submissions  \n  |_ /tex\n    |_ acknowledgements.tex\n    |_ dedication.tex\n    |_ doc_preface.tex\n    |_ params.tex\n    |_ preamble.tex\n  |_ thesis.Rproj  \nSome of these files may not mean much to you, but the general gist is I have each chapter in a different file (like 01-chapter1.Rmd) and I have folders for the following:\n\n/_book: This is where bookdown compiles my final thesis, called _main.pdf.\n\n/data: This is where I have all of my data.\n/notes: This is where I hold notes for various iterations of the thesis (including meeting minutes).\n/scripts: This is where I hold my compilation scripts, for example compile.R to compile my thesis.\n/submissions: This is where I hold versions of chapters that were submitted to journals for publication.\n/tex: This is where I hold all of the .tex files needed for compilation. This will be explained further in the Using bookdown section.\n\nAngela Li has a great tweet about how she structured her thesis files as another example of how to do this!\n\n\nWhen I was trying to procrastinate on actually writing, I would just organize my work, including my entire thesis into separate R Markdown documents linked by a master THESIS.Rmd file 2/n pic.twitter.com/8iGnOyrY2h\n\n‚Äî Angela Li (@CivicAngela) August 1, 2018\n\n\nIn the thread above, she highlights a bunch of resources for thinking about file structure ‚Äì I highly recommend a click through!\nUsing bookdown\n‚äïYihui has a great book (written using bookdown, obviously), bookdown: Authoring Books and Technical Documents with R Markdown, with lots of bookdown details that aren‚Äôt covered here! I wrote my dissertation using bookdown ‚Äì it was a total delight! You can download a sample version of how I set everything up from GitHub.\nI am going to walk through how you can update the files in the sample version for your own dissertation.\n\nThe main file is index.Rmd. Here is what my sample index looks like.\n\n--- \nbibliography: citations.bib\noutput:\n  bookdown::pdf_book:\n    toc: no\n    template: null\n    includes:\n      in_header: tex/preamble.tex\n      before_body: tex/doc_preface.tex\ndocumentclass: book\nclassoption: oneside\nfontsize: 12pt\nsubparagraph: yes\nlink-citations: yes\nbiblio-style: apalike\n---\n\n# Introduction\n\nHere is the introduction to my thesis.\nTwo things I‚Äôd like to draw your attention to are the in_header and before_body parameters. These allowed me to incorporate the Vanderbilt LaTeX template. We‚Äôll be updating these in the next steps. You‚Äôll add your Introduction chapter to this file.\n\nThe tex folder contains the files you‚Äôll need to update to personalize this.\n\n\npreamble.tex and doc_preface.tex are both based on Vanderbilt‚Äôs LaTeX template. If you aren‚Äôt updating the template, don‚Äôt touch these files. If your school has certain specifications, then you‚Äôll need to edit these. preamble.tex will include anything you want to be located before the \\begin{document} command. doc_preface.tex includes anything you want after \\begin{document}, but before the main part of your text (the title page, etc.).\n\nUpdate the acknowledgements.tex and dedication.tex files with your Acknowledgements and Dedication.\nUpdate the params.tex file with your information (name, title, university, etc.).\nMy set up assumes you are using the nomencl LaTeX package to handle your list of abbreviations. To do this, the first time an abbreviation is used in your dissertation, include it like this \\nomenclature{&lt;abbreviation&gt;}{&lt;description&gt;} (I have an example of this in the 01-chapter1.Rmd file). After you compile your bookdown book, a file called _main.nlo will be created. In your terminal, navigate to your thesis folder and run the following:\n\nmakeindex _main.nlo -s nomencl.ist -o _main.nls\n\nRecompile your bookdown book, and a list of abbreviations will automagically be added.\n\n\nAs I mentioned in the File naming section, I have a separate .Rmd file for each chapter. These will be automatically compiled when you compile the index.Rmd file with bookdown. The book is compiled using the render_book() function:\n\n\nbookdown::render_book(\"index.Rmd\")\n\nFile sharing\nI have gone through many iterations of file sharing, and this is where I have landed.\n\nWrite each chapter in RStudio as a .Rmd file.\n\nPush to GitHub. ‚äïThis script is saved in the scripts folder in the bookdown example in my dissertation toolkit GitHub repo.\nknit the .Rmd file to a word_document and send this to Google Drive for comments / revisions from co-authors. I do this using the googledrive R package üì¶.\n\n\n# uncomment run the line below if googledrive package is not installed\n# install.packages(\"googledrive\")\n\n#---- render your chapter as a Word document\nrmarkdown::render(\n  \"01-chapter1.Rmd\",\n  output_format = \"word_document\")\n\n#---- run this next part only once\nfile &lt;- googledrive::drive_upload(\n    \"01-chapter1.docx\", \n    type = \"document\")\n\n# get the file id (copy this to your clipboard)\nfile$id\n\n#----\n\n# paste file id, run this next part every time you want to push a new version to Google Drive\nid &lt;- \"pasted File Id\"\ngoogledrive::drive_update(\n    googledrive::as_id(id), \n    \"01-chapter1.docx\")\n\n\nIncorporate Google Drive suggestions by hand into the .Rmd file. This seems like a major pain, but the problem of round tripping from R to Google Drive is still an open question in the #rstats field. On the bright side, I find stepping through the Google Drive revisions is a good opportunity to thoroughly read through them.\nSend the update to GitHub.\nRevise the chapter.\nRepeat steps 2 - 7 as needed!\n\n\n\nPlanning and accountability\nThe section below is going to give great detail on my planning process. In addition to setting up a detailed plan to make an unstructured timeline quite structured, having someone (or many someones!) to hold you to this plan can be very helpful! For me, my husband was this someone. I shared my goals at the beginning of each month with him and asked him to ask me about it to be sure I stuck to the plan. Additionally, I set up ‚Äúofficial‚Äù deadlines via committee meetings, conference presentations, and journal club presentations in an attempt to build in some concrete structure."
  },
  {
    "objectID": "posts/2018-09-14-one-year-to-dissertate/index.html#the-planning",
    "href": "posts/2018-09-14-one-year-to-dissertate/index.html#the-planning",
    "title": "One year to dissertate",
    "section": "The planning",
    "text": "The planning\n\n\nA lot of my goal-setting was adapted from principals I picked up using a Passion Planner.\nThis section is going to explain the specifics behind how I planned my dissertation timeline. I found it really helpful to have specific goals on detailed timelines in order to create structure around an inherently unstructured time.\nOnce you have made up your mind that you would like to finish your dissertation in a certain amount of time, pull out your department‚Äôs expectations and move on to Step 1.\n\nStep 1: Setting the goal\nFor this first step, we are going to create a mind map with completing your dissertation at the center. Grab a blank piece of paper, and draw a box in the middle that says I HAVE COMPLETED MY DISSERTATION üéâ or download and print this one üëá. Or if you don‚Äôt like to print, download this and stick it on a tablet you can write on (that‚Äôs how I did these!).\n\nClick here to download it from GitHub.\n\n\nSetting the timer became a really important part of the process for me. If I hadn‚Äôt set a short timer, I‚Äôm not sure that I would have followed through with my goal-setting routine.\n\nüìÜ Add your defense goal in the Date box.\n\nüï¶ Set a timer for five minutes and write down all of the tasks that would need to happen in order to reach this goal.\n\n‚òùÔ∏è Add in due dates for each of these tasks.\n\n1Ô∏è‚É£ Stick a number next to each task in the order it must be completed.\n\nHere is an example of what a portion of my original mind map looked like.\n\nA few tips:\n\nDon‚Äôt panic and don‚Äôt be a perfectionist! You‚Äôll get a chance to update these at the beginning of each month!\n\nUse things like conferences and journal clubs to set ‚Äúreal‚Äù deadlines for yourself. As much as my artificial deadlines helped add some structure to my process, having a ‚Äúreal‚Äù deadline, such as needing to prepare a talk, was super helpful.\nSpeaking of ‚Äúreal‚Äù deadlines, as I mentioned above, I found it really helpful to have committee meetings every 3 months. This was much more frequent than the requirements, but these deadlines kept things moving along and kept everyone on the same page throughout. An important piece to this is making sure that the committee meetings are informative and efficient, using techniques like the ones highlighted in the practicals section above.\nTry to make the tasks as specific as possible. You‚Äôll be updating this each month, so if specifics are hard to come by for a task that seems way in the future, you can update it later!\nIf you aren‚Äôt sure of the exact date for something, just put the week.\n\n\n\nStep 2: Set up your weekly calendar\nNow that you have a bit of a global map üó∫ for the overall course of your next year (or whatever timeline you‚Äôve set), now we need to scale this down to the micro-level. I found that a weekly calendar worked best for this!\nHere is a weekly calendar I created for this purpose.\n\nClick here to dowload it from GitHub.\nAt the beginning of each month, map the dates you‚Äôve set on your mind map to these weekly views.\n\nFill in the Week of and the dates for the days of the week in your calendar.\n\nPut the task from your mind map in the This Week‚Äôs Aim box.\n\nIf you set a specific date, add the task to the aim under the date you‚Äôve aimed for\nNow that you‚Äôve mapped the global tasks, using the same template as above, create a mini map for the month. Specifically:\n\n\nPut the tasks for the month from your large mind map in the center of your new monthly map\nSet a timer for 5 minutes and write down specific (low-level) tasks that need to be completed to achieve your goals\nAdd in due dates for these tasks\nStick a number next to each task\nAdd these micro-tasks to your weekly calendar\n\nHere is an example of a week I mapped from the ‚ÄúSubmit tipr to CRAN‚Äù task I set for myself on my mind map.\n\n\n\nStep 3: Re-evaluate your mind map\nThe last week of the month, set aside 10 minutes to re-evaluate your mind map.\n\nPull out the mind map you‚Äôve set up and stick a nice check mark ‚úîÔ∏è next to the things you‚Äôve completed (Woohoo, go you!!).\n\nTake a look at the next tasks you have coming up. If they seem realistic, complete Step 2 for your next month, if not, cross out the dates and set new ones! Add new tasks as needed to make your goals a reality!\n\nHere is an example of an updated mind map."
  },
  {
    "objectID": "posts/2018-09-14-one-year-to-dissertate/index.html#other-tid-bits",
    "href": "posts/2018-09-14-one-year-to-dissertate/index.html#other-tid-bits",
    "title": "One year to dissertate",
    "section": "Other tid bits",
    "text": "Other tid bits\nThis is a random list of pointers that I heard throughout my process that I found useful.\n\nSurround yourself with people who encourage you! This makes such a difference for maintaining a positive attitude throughout! In our student-run journal clubs, we began each meeting with ‚ÄúGood News‚Äù to give us a chance to celebrate each other‚Äôs successes ‚Äì I loved hearing about the exciting things everyone was doing and found it so uplifting to have a cohort so full of encouragement!\n\nFamiliarize yourself with on campus resources. Most universities have lots of programming for PhD students (but it is not always easy to find!). Your mental health is extremely important during this time, so be sure to take care of yourself!\nFind ways to make sure you aren‚Äôt isolating yourself. For me, this included scheduling weekly fun activities (like Trivia!) and getting involved with groups both on campus (like Graduate Student Council) and off (like R-Ladies!).\nChoose your advisor carefully. Having an advisor that is a good personality match makes everything abundantly easier!\nWrite a little bit every day. Think of writing as your job - what you write may never end up in a final paper, but constantly practicing will make you better!\n\nDo you have anything I missed? Please let me know, I‚Äôm excited to have this be a live document!\nüåª Happy Dissertating!  Source"
  },
  {
    "objectID": "posts/2019-10-16-phewas_me/index.html",
    "href": "posts/2019-10-16-phewas_me/index.html",
    "title": "PheWAS-ME, an app for exploration of multimorbidity patterns in PheWAS",
    "section": "",
    "text": "This post is a longer-form and less-formal accompaniment to the manuscript just up on MedRxiv: ‚ÄúPheWAS-ME: A web-app for interactive exploration of multimorbidity patterns in PheWAS‚Äù and accompanying application.\nAs the first of three papers that make up my PhD dissertation, the project represents a significant collaborative effort bringing together Electronic Health Records (EHR) and Biobank data using R and Shiny.\n‚äïThe main dashboard view of Phewas-ME.\nI have organized the following into three primary sections: what the app does, why it does it, and how it works."
  },
  {
    "objectID": "posts/2019-10-16-phewas_me/index.html#what-is-the-app",
    "href": "posts/2019-10-16-phewas_me/index.html#what-is-the-app",
    "title": "PheWAS-ME, an app for exploration of multimorbidity patterns in PheWAS",
    "section": "What is the app?",
    "text": "What is the app?\nThe paper describes Phewas-Multimorbidity Explorer (Phewas-ME): a Shiny app that we have been producing over the past two years in collaboration with the Vanderbilt Drug Repurposing team. It is an interactive data exploration tool for digging into PheWAS results and the subject-level data that generated those results."
  },
  {
    "objectID": "posts/2019-10-16-phewas_me/index.html#what-is-phewas",
    "href": "posts/2019-10-16-phewas_me/index.html#what-is-phewas",
    "title": "PheWAS-ME, an app for exploration of multimorbidity patterns in PheWAS",
    "section": "What is Phewas?",
    "text": "What is Phewas?\nPheWAS is a statistical method for finding associations between a given genetic mutation (often a Single Nucleotide Polymorphism, or SNP) and phenotypes. It is a sibling to the GWAS (genome-wide association study). However, in GWAS, you fix your desired phenotype and scan the genome for associations, whereas in PheWAS, you fix a genetic mutation, and scan the ‚Äòphenome.‚Äô\n‚äïSlide from my talk Taking a network view of EHR and Biobank data to find explainable multivariate patterns."
  },
  {
    "objectID": "posts/2019-10-16-phewas_me/index.html#what-is-the-data",
    "href": "posts/2019-10-16-phewas_me/index.html#what-is-the-data",
    "title": "PheWAS-ME, an app for exploration of multimorbidity patterns in PheWAS",
    "section": "What is the data?",
    "text": "What is the data?\nBoth PheWAS analyses and Phewas-ME take two types of subject-level data. First is information on a given genetic mutation. In this case, 0, 1, or 2 copies of the minor allele of an SNP. The second is the subject‚Äôs ‚Äòphenome.‚Äô In this case, a list of every phenotype that they had. In the paper and this blog post, we have used the Vanderbilt developed ‚ÄòPhecode,‚Äô but any binary phenotype information works. ‚äïIn this post, when I refer to ‚Äòcodes‚Äô I mean ‚Äôphecodes."
  },
  {
    "objectID": "posts/2019-10-16-phewas_me/index.html#why-is-the-app-needed",
    "href": "posts/2019-10-16-phewas_me/index.html#why-is-the-app-needed",
    "title": "PheWAS-ME, an app for exploration of multimorbidity patterns in PheWAS",
    "section": "Why is the app needed?",
    "text": "Why is the app needed?\nPhewas-ME came out of the need for subject-experts to dig through the very high-dimensional results of PheWAS analyses. Unlike a typical statistical analysis, which may return just a handful of p-values or effect-sizes of interest, a PheWAS analysis returns a single p-value and effect-size for every phenotype in the scanned phenome. In the case of Phecodes, this is around 1,500 (and goes into the many thousands for more traditional ICD-based encodings).\n\nPrevious methods\nIn the past, communicating these results was accomplished using two different tools.\nFirst is a manhattan plot: or simply a plot of every phenotype investigated and their significance (as encoded by the negative log of the p-value). This plot allows the reader to pick out which codes are significant in the results.\n‚äïManhattan plot for PheWAS results. Taken from Denny et al, 2016 \nThe second is the ever-present table. A PheWAS table has columns on all sorts of values of interest such as p-value, effect-size, phenotype description, etc.. Due to a large number of phenotypes typically present, these tables often get filtered to only includes codes that fit some criteria, such as significance level.\n‚äïTable of PheWAS results from simulated data provided with app. \n\n\nProblems with previous methods\nWhile looking at the most significantly associated phenotypes is straightforward, it can mask important aspects of the data. Different phenotypes across the phenome correlate with each other in complex ways. These correlations manifest themselves in common patterns of phenotypes called multimorbidities (also referred to as comorbidities). Since PheWAS looks at single SNP and Phenotype associations, these correlations can be extremely hard to discover and reason about with a traditional manhattan plot and table report."
  },
  {
    "objectID": "posts/2019-10-16-phewas_me/index.html#why-does-our-app-fix-these-problems",
    "href": "posts/2019-10-16-phewas_me/index.html#why-does-our-app-fix-these-problems",
    "title": "PheWAS-ME, an app for exploration of multimorbidity patterns in PheWAS",
    "section": "Why does our app fix these problems?",
    "text": "Why does our app fix these problems?\nPheWAS-Multimorbidity Explorer helps subject-matter experts parse through the results of a PheWAS analysis by letting them isolate and explore specific regions of phenotypes by looking not only at their p-values and effects-sizes but also by the subject-level data that generated those results.\nThe analyst selects a set of phenotypes to explore by dragging a selection box around a region in a manhattan plot, choosing directly from a table, or using text-search. Once a desired set of phenotypes is selected, the application displays the subject-level data using an interactive force-directed network plot and an upset plot.\n\nThese subject-level visualizations allow the analyst to see patterns of comorbid phenotypes directly and to interrogate their potential causes. For instance, often, a group of codes are highly correlated because they are more or less specific definitions of the same phenotype, e.g., cancer -&gt; lung cancer -&gt; stage 2 lung cancer. ‚äïIn the above plot, you can see a cluster of phenotypes in light green. This cluster is caused by the afformentioned phenotype hierarchy, with each refering to the same broad-level phenotype with differing levels of specificity. Another possibility is multimorbidity caused by drug side-effects; e.g., patients taking a drug to treat the cancer phenotype likely also have the nausea phenotype."
  },
  {
    "objectID": "posts/2019-10-16-phewas_me/index.html#how-is-interaction-used-in-the-app",
    "href": "posts/2019-10-16-phewas_me/index.html#how-is-interaction-used-in-the-app",
    "title": "PheWAS-ME, an app for exploration of multimorbidity patterns in PheWAS",
    "section": "How is interaction used in the app?",
    "text": "How is interaction used in the app?\nInteraction with Phewas-ME centers around one primary thing: a list of currently selected phenotypes. The typical way to choose these phenotypes is by using the Phewas results panel.\n\nThis panel includes the traditional manhattan plot and results table; however, they are now interactive. Selecting codes using this panel can be done in a few different ways.\n\n\n\n\nCode Selection\n\nRegion Dragging\nThe user can drag a box around a region in the manhattan plot, selecting the codes within. Adding another region to the selection is accomplished by holding down the ‚Äòa‚Äô key (for add) and dragging another box. Conversely, removing a region of codes from the selection can be done by holding down the ‚Äúd‚Äù key and dragging a box.\n\n\n\nvia GIPHY\n\n\n\nCode Clicking\nClicking individual phenotypes in either the plot or the table toggles their selected-ness. This slower but more precise method allows for fine-grained tuning of the selected codes.\n\n\n\nvia GIPHY\n\n\n\nSearch\nAbove the results table is a search bar. The user can search for phenotypes by name or description. Codes that match a supplied search query are raised to the top of the results table and highlighted. The user can then select codes as needed.\n\n\n\nvia GIPHY\n\n\n\nNetwork Filtering\nIn the subject-level network plot, nodes corresponding to phenotypes can be used to filter the selection further. After selecting codes in the network plot by clicking, users can delete from the current selection, isolate a subset, or invert (this flips the definition of a connection to a phenotype in the network to the lack of that phenotype in a subject‚Äôs phenome).\n\n\n\nvia GIPHY\n\n\n\n\nOther interaction\nOutside of phenotype selection, there are a few other forms of interaction.\n\nPattern Highlighting\nWhen a user clicks on a given pattern in the Upset Plot panel, the network plot updates to highlight the subjects who have that pattern. Similarly, if a phenotype node in the network plot itself is moused over, the subjects connected to that phenotype are highlighted.\n\n\n\nvia GIPHY\n\n\n\nInfo Tooltips\nIn both the manhattan plot and the network plot, when a given phenotype‚Äôs point is moused-over, a tooltip with all the supplied information for that phenotype appears.\n\n\nSNP Filtering\nA check-box by the network plot allows the user to show only carriers of the minor allele of interest in the network plot. This helps differentiate between genetics driven network patterns and simply population trends.\n\n\n\nvia GIPHY\n\n\n\nUpset Interaction\nThere are two helpful interaction aides not present in the original UpSet plot.\nSingleton Toggle First, a toggle allows the hiding of single-phenotype patterns (i.e., subjects who had just a single phenotype). Often these singletons can crowd out the more exciting comorbidity patterns, so by hiding them, the user can focus on the patterns.\n\n\n\nvia GIPHY\n\nMinimum Pattern Size Slider Second, a slider allows the user to set the threshold for inclusion in the plot based on the number of times a pattern appears. Often there will be patterns of phenotypes that are only had by one or two subjects. Because of their tiny sample-sizes, these are usually not of interest, so filtering them out can again improve the plot‚Äôs effectiveness."
  },
  {
    "objectID": "posts/2019-10-16-phewas_me/index.html#how-was-it-made",
    "href": "posts/2019-10-16-phewas_me/index.html#how-was-it-made",
    "title": "PheWAS-ME, an app for exploration of multimorbidity patterns in PheWAS",
    "section": "How was it made?",
    "text": "How was it made?\nThere is a lot of exciting tech going on behind the scenes of Phewas-ME.\n\nGeneral App\n\nFramework\nThe main app is built using the Shiny package/framework in R. An R package was created to wrap all the code, easing deployment and customization. There are two main functions in the package: run_me() which takes dataframes of all necessary information as input and starts at the main dashboard; and also build_me_app() which returns an app that starts at a data-loading screen where the user can either load data using spreadsheets, or pick from a list of pre-populated datasets. All individual plots and panels exported by the package as shiny modules. This modular format eases the creation of customized versions of PheWAS-ME.\n\n\nState Management\nThe reactivity protocol of shiny works great if you have a single component of your app that listens to and modifies unique parts of your apps state. In PheWAS-ME, multiple components listen to the same state variables (for instance, both the PheWAS results table and the network plot modify the currently selected codes). A state management system inspired by Javascript‚Äôs Redux was built to keep each component pure and modular.\n‚äïBroad level idea of how Redux works, via the jscrabler blog \nIn this system, a single message-passing reactive value is supplied to all of the separate components. When a component needs to modify the state, it sends a message through that reactive value to the main app.\nAs an example, if the user removes a code using the network plot. The network plot module sends a list to the message passing reactive variable containing an action type: 'delete code', and then a payload: '008.12'. Then at the main app level, an observeEvent() chunk that watches that message passing reactive reads the message and modifies the appropriate state variables, in this case, the selected codes list.\n\nshiny::observeEvent(app_interaction(),{\n  action_type &lt;- app_interaction() %&gt;% pluck('type')\n  action_payload &lt;- app_interaction() %&gt;% pluck('payload')\n  \n  action_type %&gt;%\n    switch(\n      delete = {\n        codes_to_delete &lt;- extract_codes(action_payload)\n        prev_selected_codes &lt;- state$selected_codes()\n        state$selected_codes(\n          prev_selected_codes[!(prev_selected_codes %in% codes_to_delete)]\n        )\n      },\n      selection = {\n        ...\n      },\n      ...\n    )\n  ...\n})\n\nBy isolating all state modification in a single observable chunk, the primary app state is only ever modified in a single place. This isolation reduces the amount of overhead required to reason about state changes, both for Shiny and the programmer.\n\n\n\nR to Javascript Communications\nEvery visual in the app is built using custom javascript and HTML. The fantastic package R2D3 is used to facilitate the handoff of data between R/Shiny and the javascript visualization code.\n\nManhattan Plot\nThe manhattan plot is built using d3js, the popular javascript visualization library. With such a large number of points plotted, finding points within a selected area can be very slow when naively searching through every point. ‚äï A 2d quadree storing datapoints. These subdividing boxes are stored as in a tree-structure, which is always a big-O help. From wikipedia The javascript code utilizes a special data structure known as a quad-tree to speed up finding codes within a region. By storing data in a hierarchical location-aware tree, a quad-tree only searches through points near the selection, cutting down on the computation needed and speeding up app reaction times.\n\n\nPheWAS Table\nLike the manhattan plot, the PheWAS table has tricks to help keep the app responsive while also showing large amounts of data. The browser stores all the elements on a webpage in a text-based format called the DOM (or Document Object Model). (What you get when you right-click and say ‚Äòview source.‚Äô) When a browser renders the page, it parses through the DOM and figures out how to place that element on the screen. Because the browser has to parse through so much every time it wants to update the screen, there are limits to how many elements can be stored in the DOM before things start slowing down. ‚äï  Recomendations from developers.google.com for optimal DOM performance. Oops! Displaying a table of more than 1,500 phenotypes, along with 5+ columns, quickly hits that limit.\nThe trick used to display a large table while not overloading the DOM is only rendering a small portion of the table at a time. At any moment, only 50 rows of the table are actually in the DOM. As the user scrolls down, the javascript writes new rows at the bottom and removes old rows from the top. As to maintain the illusion that the entire table is available for scrolling, secret elements placed above and below the visible rows that expand and contract to simulate a table with every row filled in. The user sees a table just like it was full of 1,000 rows, but the browser only has to render 50. Implementing this optimization resulted in interaction latency dropping by ~4 times.\n‚äïPerformance improvements in results table after implementing smart-table. Via the Chrome devtool‚Äôs lighthose audits tool. \n\n\nUpSet Plot\nLike the manhattan plot, the upset plot is drawn using d3. Unlike the results table, there are not too many elements to draw here, so no fancy tricks are needed to speed it up.\n\n\nNetwork Plot\nThe network plot was where the most coding and optimization hours were spent. The goal was to have a plot that could show every subject‚Äôs data. To draw every subject often involved drawing tens of thousands of points and even more links. Also, since the user would be on-the-fly reconfiguring the data displayed, we needed to calculate the layout in real-time.\nDrawing lots of points and lines Plotting lots of points and lines had a relatively simple solution: use the Canvas element rather than the SVG. The Canvas element allows the user to draw to the screen at the pixel-level, basically coding an image. Because the Canvas element acts as a fancy image, the DOM only has a single item added is not slowed down.\nAdding interactivity The only issue with the canvas element: there‚Äôs no easy way to detect if a component of the chart is interacted with by the user. We wanted interactivity with the phenotypes (e.g., mouseover and selection). Adding interactivity was solved by overlaying an SVG element on top of the Canvas element. The SVG element draws only the phenotype nodes. We could then use the SVG‚Äôs built-in intersection observers to detect interaction, rather than building a custom solution using things like hidden colors.\n‚äïBoth the Canvas and the SVG are absolutely positioned within the parent to make sure they overlap properly. \nExporting paper-ready plots One feature that was frequently requested was the ability to export the network plot for use in a publication. Since canvas elements are just raster-images, they didn‚Äôt scale well when exported. To allow for clean high-res exports, an ‚Äòexport mode‚Äô was added that switches all rendering to the SVG (which is vector-based) and adds a download button. Also available is a callouts mode where little draggable annotations for each phenotype name are added. When a user presses the download button, the SVG element is converted to the SVG file format and downloaded the user‚Äôs computer. Adobe Illustrator or other vector-based tools can be used to insert the plot into a figure or PDF.\n\n\n\nvia GIPHY\n\nNot visible due to compression for video: a slight increase in sharpness when switching to all SVG-Rendering.\nCalculating a force-layout in real time Calculating the layout was relatively easy to do: D3 includes a sub-library for running force-layout simulations. The problem is these force simulations are computationally intensive, especially when you start running them on large networks. When you run computationally intensive tasks in javascript, your app will slow to a crawl because the same thread responsible for updating the screen is also running those calculations.\nOur solution was to offload the layout calculation to another thread using a technology called ‚Äòweb workers.‚Äô Web workers allow javascript code to run on another thread and communicate with the main thread through a series of asynchronous callbacks.\n‚äï Web workers are there own threads with their own event loops, so they don‚Äôt slow down your main page. Source: Barbaric Basics.\n\n// Function to send new network data to webworker\nfunction send_new_job(network_data, callbacks){\n\n  const {on_progress_report, on_finish, on_layout_data} = callbacks;\n\n  // Initialize the worker\n  worker = new Worker(worker_url);\n\n  // Send worker the data we are working with\n  worker.postMessage(network_data);\n\n  // Control what is done when a message is received from the webworker\n  worker.onmessage = function(event) {\n    switch (event.data.type) {\n      case \"progress_report\": return on_progress_report(event.data.progress);\n      case \"layout_data\": return on_layout_data(event.data);\n      case \"end\": return on_finish(event.data);\n    }\n  };\n};\n\nWhen the network plot receives new data, it spins up one of these web-worker threads, sends the data to it to run the layout simulation. The simulation web worker sends back occasional updates on the node positions to the main thread to update the plot, assuring the user it is doing something and not frozen. This adaptation meant that massive networks could be laid out and rendered without causing any perceivable slowdowns on the main app.\n\n\n\nvia GIPHY"
  },
  {
    "objectID": "posts/2019-10-16-phewas_me/index.html#how-can-i-use-the-app-myself",
    "href": "posts/2019-10-16-phewas_me/index.html#how-can-i-use-the-app-myself",
    "title": "PheWAS-ME, an app for exploration of multimorbidity patterns in PheWAS",
    "section": "How can I use the app myself?",
    "text": "How can I use the app myself?\nDo you have data of the form described and want to try the app out yourself? There are two main methods for doing so.\n\nHosted Version\nWe have a version of Phewas-ME up and running on our lab‚Äôs website that anyone can use. There is a simulated dataset for exploration, but a data loading screen allows you to test out with data you upload yourself.\n\n\nAn R Package\nAs mentioned before, the application lives in an R package. Because of this, running it locally is as easy as installing the package from GitHub using‚Ä¶\ndevtools::install_github('tbilab/meToolkit')\n‚Ä¶and then starting an app up with the function run_me().\nIf you want to learn how to make an app that loads directly to a given dataset or preload data, check out the online documentation for the package at prod.tbilab.org/phewas_me_manual.\n‚äïThe package‚Äôs beautiful website, courtesy of pkgdown!"
  },
  {
    "objectID": "posts/2017-03-16-enar-in-words/index.html",
    "href": "posts/2017-03-16-enar-in-words/index.html",
    "title": "ENAR in words",
    "section": "",
    "text": "I had an absolutely delightful time at ENAR this year. Lots of talk about the intersection between data science & statistics, diversity, and exceptional advancements in statistical methods.\nI loved it, but let‚Äôs see what others were saying! Check out this word cloud of the most commonly tweeted words.\nThis certainly sums up my experience. Some of my favorites that make a big appearance:\nSince there was quite a bit of twitter action, I thought I‚Äôd do a quick tutorial in scraping twitter data in R."
  },
  {
    "objectID": "posts/2017-03-16-enar-in-words/index.html#get-twitter-credentials",
    "href": "posts/2017-03-16-enar-in-words/index.html#get-twitter-credentials",
    "title": "ENAR in words",
    "section": "Get twitter credentials",
    "text": "Get twitter credentials\nGo here and create an app - this will give you a Consumer key, Consumer secret.\n\n\nPro Tip: be sure to enter http://127.0.0.1:1410 as your Callback URL. If you get lost, there is a great tutorial on this process here"
  },
  {
    "objectID": "posts/2017-03-16-enar-in-words/index.html#scrape-tweets",
    "href": "posts/2017-03-16-enar-in-words/index.html#scrape-tweets",
    "title": "ENAR in words",
    "section": "Scrape tweets",
    "text": "Scrape tweets\nWe will use the rtweet package to scrape the tweets using the search_tweets function.\n\n\nMy original tutorial used twitteR, but Ma√´lle kindly pointed out that it is on the way out and rtweet is the better option, so it‚Äôs been updated!\n\nlibrary('rtweet')\n\ntwitter_token &lt;- create_token(\n  app = \"PASTE_YOUR_APP_NAME_HERE\",\n  consumer_key = \"PASTE_YOUR_CONSUMER_KEY_HERE\",\n  consumer_secret = \"PASTE_YOUR_CONSUMER_SECRET_HERE\")\n\ndat &lt;- search_tweets('#ENAR2017', n = 1e4, since = '2017-03-10', token = twitter_token)\n\nIf you would like to practice with the ENAR tweet data, you can load mine in with the following code & continue with the example.\n\nload(url(\"https://github.com/LFOD/real-blog/raw/master/data/enar_data.rda\"))"
  },
  {
    "objectID": "posts/2017-03-16-enar-in-words/index.html#wrangle-tweets",
    "href": "posts/2017-03-16-enar-in-words/index.html#wrangle-tweets",
    "title": "ENAR in words",
    "section": "Wrangle tweets",
    "text": "Wrangle tweets\nNow we need to corral these tweets into something we can analyze. We are going to use some data-wrangling packages (dplyr, purrrüò∫, and stringr) as well as Julia & David‚Äôs tidytext.\n\n\nFor more details on how to analyze text, check out their book Text Mining with R, the code below is modified from one of their examples.\nWe will then use the wordcloud package to display our results.\n\n#load packages\nlibrary('dplyr')\nlibrary('purrr')\nlibrary('stringr')\nlibrary('tidytext')\nlibrary('wordcloud')\n\nWe are going to get rid of unwanted symbols and links, split the tweets into individual words, and filter out some stop words.\n\n#this will drop links & symbols\ndrop_pattern &lt;- \"https://t.co/[A-Za-z\\\\d]+|http://[A-Za-z\\\\d]+|&amp;|&lt;|&gt;|RT|https|ht\"\n#this pattern is great for twitter, includes # and @ symbols\nunnest_pattern &lt;- \"([^A-Za-z_\\\\d#@']|'(?![A-Za-z_\\\\d#@]))\"\n\ntweets &lt;- dat %&gt;% \n  filter( !grepl(\"#OTORRINO\", text)) %&gt;% # we have one tweeter with our hashtag that wasn't at our conference\n  mutate(text = str_replace_all(text, drop_pattern, \"\")) %&gt;%\n  unnest_tokens(word, \n                text, \n                token = \"regex\", \n                pattern = unnest_pattern) %&gt;%\n  filter(!(word %in% stop_words$word),\n         str_detect(word, \"[a-z]\"),\n         !grepl(\"@\", word )) \n\nNow it‚Äôs plotting time!\n\ncols &lt;- c(brewer.pal(8,\"Dark2\"), rep(brewer.pal(8,\"Dark2\"), each = 5) ) #make some colors for our plot\n\ntweets %&gt;%\n  count(word) %&gt;%\n  with(wordcloud(word, \n                 n,\n                 min.freq = 5,\n                 random.order = FALSE,\n                 colors = cols))\n\nYou did it! Easy as œÄ.\n\nFor giggles, let‚Äôs try another word cloud package wordcloud2. This one is interactive (but not on CRAN, you can install using devtools::install_github(\"lchiffon/wordcloud2\")).\nFor a word cloud similar to the one above, we can use the wordcloud2 function.\n\nlibrary('wordcloud2')\n\ntweets %&gt;%\n  count(word) %&gt;%\n  filter(n &gt; 2) %&gt;%\n  wordcloud2(size = 3, minRotation = -pi/2, maxRotation = -pi/2)\n\n\n\n\n\nTry the following to make an R shaped cloud using the letterCloud function!\n\ntweets %&gt;%\n  count(word) %&gt;%\n  filter(n &gt; 1) %&gt;%\n  letterCloud(size = 3, word = \"R\") \n\nHappy scraping!"
  },
  {
    "objectID": "posts/2023-04-28-imputation-might-be-silly/index.html",
    "href": "posts/2023-04-28-imputation-might-be-silly/index.html",
    "title": "When is complete case analysis unbiased?",
    "section": "",
    "text": "Here is the scenario: You are trying to predict some outcome, \\(y\\), and some of your predictors have missing data. Will doing a complete case analysis give you unbiased results? What additional information do you need before deciding?\nFor some reason, when I tried to answer this question, my first instinct was to try to decide whether the data were missing at random, but it turns out, this might not be the right first question! Why? Complete case analysis will give us unbiased estimates even if the data are not missing at random. Excuse me? Yes. When is this the case? When:\nWhen I was thinking through this, I found it helped to work up a few short simulations, so here we go."
  },
  {
    "objectID": "posts/2023-04-28-imputation-might-be-silly/index.html#scenario-1-i-have-a-predictor-that-is-missing-not-at-random",
    "href": "posts/2023-04-28-imputation-might-be-silly/index.html#scenario-1-i-have-a-predictor-that-is-missing-not-at-random",
    "title": "When is complete case analysis unbiased?",
    "section": "Scenario 1: I have a predictor that is missing not at random",
    "text": "Scenario 1: I have a predictor that is missing not at random\nFirst let‚Äôs generate our ‚Äútrue‚Äù values. Here the true model is below, which shows that:\n\\(y = x + \\varepsilon\\)\nIn other words, \\(\\beta_0: 0\\) and \\(\\beta_1: 1\\)\n\nlibrary(tidyverse)\n\nn &lt;- 1000000\n\nset.seed(1)\ndata &lt;- tibble(\n  x = rnorm(n),\n  c = rnorm(n),\n  y = x + rnorm(n)\n)\n\nLet‚Äôs force x to be missing under three scenarios:\n\nMissing completely at random\nMissing conditionally at random (based on c)\nMissing not at random\n\n\ndata &lt;- data |&gt;\n  mutate(\n    x_obs_mcar = ifelse(\n      rbinom(n, 1, 0.5), \n      NA, \n      x\n    ),\n    x_obs_mar = ifelse(\n      rbinom(n, 1, 1 / (1 + exp(-c))),\n      NA,\n      x\n    ),\n    x_obs_mnar = ifelse(\n      rbinom(n, 1, 1 / (1 + exp(-x))),\n      NA,\n      x\n    )\n  )\n\ndata |&gt;\n  summarise(mcar_miss = mean(is.na(x_obs_mcar)),\n        mar_miss = mean(is.na(x_obs_mar)),\n        mnar_miss = mean(is.na(x_obs_mnar)))\n\n# A tibble: 1 √ó 3\n  mcar_miss mar_miss mnar_miss\n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     0.501    0.500     0.501\n\n\nIn all three cases ~50% of the data are missing.\nLet‚Äôs see how these impact the ability to predict y (and the estimation of \\(\\hat\\beta\\))\n\nlm(y ~ x_obs_mcar, data = data)\n\n\nCall:\nlm(formula = y ~ x_obs_mcar, data = data)\n\nCoefficients:\n(Intercept)   x_obs_mcar  \n   -0.00104      0.99964  \n\n\nOk, the missing completely at random x is estimated correctly when we do a complete cases analysis (that is when we list-wise delete every row that has missing data). This checks out!\nLet‚Äôs check out the missing conditionally at random x. Note that this is missing at random after conditioning on c (although we aren‚Äôt actually doing anything with c at this point, so it might as well be missing not at random‚Ä¶)\n\nlm(y ~ x_obs_mar, data = data)\n\n\nCall:\nlm(formula = y ~ x_obs_mar, data = data)\n\nCoefficients:\n(Intercept)    x_obs_mar  \n  -0.000482     0.999835  \n\n\nWhoa! Prediction is still correct and the coefficients themselves are still unbiased even if we do complete case analysis.\nOk, what about missing not at random?\n\nlm(y ~ x_obs_mnar, data = data)\n\n\nCall:\nlm(formula = y ~ x_obs_mnar, data = data)\n\nCoefficients:\n(Intercept)   x_obs_mnar  \n  -7.23e-05     9.99e-01  \n\n\nStill ok! No bias in the coefficients and the predicted values are unbiased even though the predictor was missing not at random (and we did a complete cases analysis).\nWhy is this the case? Travis Gerke pointed out this excellent paper that has a beautiful plot that demonstrates why this happens. I am going to simulate some smaller data than what I have above to demonstrate the point. Here, the missingess is deterministic (and definitely not random!) ‚Äì if x is greater than 0, it is missing, otherwise it isn‚Äôt. Again, this renders ~50% of our data missing.\n\ndata &lt;- tibble(\n  x = seq(-3, 3, by = 0.1),\n  y = x + rnorm(61),\n  x_miss = ifelse(x &gt; 0, \"yes\", \"no\")\n)\n\nggplot(data, aes(x = x, y = y, color = x_miss)) +\n  geom_point() + \n  geom_line(aes(x = x, y = predict(lm(y ~ x, data = data)))) +\n  geom_vline(xintercept = 0, lty = 2) + \n  scale_color_manual(values = c(\"cornflowerblue\", \"orange\")) +\n  geom_label(aes(x = -2, y = 3), label = \"observed data\", color = \"cornflowerblue\") + \n  geom_label(aes(x = 2, y = -3), label = \"missing data\") + \n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nMaybe this result is a somewhat obvious result because despite the fact that the missingness was not at random, it had nothing to do with the outcome, y, let‚Äôs see what would happen if c was related to y."
  },
  {
    "objectID": "posts/2023-04-28-imputation-might-be-silly/index.html#scenario-2-i-have-missing-data-that-is-missing-due-to-some-factor-and-other-things-so-still-not-mar",
    "href": "posts/2023-04-28-imputation-might-be-silly/index.html#scenario-2-i-have-missing-data-that-is-missing-due-to-some-factor-and-other-things-so-still-not-mar",
    "title": "When is complete case analysis unbiased?",
    "section": "Scenario 2: I have missing data that is missing due to some factor (and other things, so still not MAR)",
    "text": "Scenario 2: I have missing data that is missing due to some factor (and other things, so still not MAR)\n\nset.seed(1)\ndata &lt;- tibble(\n  x = rnorm(n),\n  c = rnorm(n),\n  y = x + c + rnorm(n)\n)\n\nLet‚Äôs say we have some data that is not missing at random. The probability that x is missing is dependent both on it‚Äôs own value (MNAR!) and the value of c.\n\ndata &lt;- data |&gt;\n  mutate(x_obs =  ifelse(\n    rbinom(n, 1, 1 / (1 + exp(-(x + c)))),\n    NA,\n    x\n  )\n  )\n\nThis results in missing ~ 50% of the values for x:\n\ndata |&gt;\n  count(miss_x = is.na(x_obs))\n\n# A tibble: 2 √ó 2\n  miss_x      n\n  &lt;lgl&gt;   &lt;int&gt;\n1 FALSE  499277\n2 TRUE   500723\n\n\nSo what happens if we try to predict y? Let‚Äôs try it first with the full data as a benchmark:\n\nright_mod &lt;- lm(y ~ x + c, data = data)\n\n\nright_mod\n\n\nCall:\nlm(formula = y ~ x + c, data = data)\n\nCoefficients:\n(Intercept)            x            c  \n  -0.000162     1.000106     0.999592  \n\n\nBeautiful. we get the correct \\(\\hat\\beta\\) estimates (and thus the correct predictions for y).\nNow let‚Äôs take a look at the complete case analysis:\n\ndata_cc &lt;- na.omit(data)\n\ncc_mod &lt;- lm(y ~ x + c, data = data_cc)\n\n\ncc_mod\n\n\nCall:\nlm(formula = y ~ x + c, data = data_cc)\n\nCoefficients:\n(Intercept)            x            c  \n    0.00128      1.00032      1.00156  \n\n\nWould you look at that. Correct coefficients! How could we break that? If we misspecified the model, for example if we didn‚Äôt adjust for c:\n\nlm(y ~ x, data = data_cc)\n\n\nCall:\nlm(formula = y ~ x, data = data_cc)\n\nCoefficients:\n(Intercept)            x  \n     -0.419        0.849  \n\n\nBut is this a terribly interesting case? If we don‚Äôt have a way to predict the missing x values (MNAR!) we couldn‚Äôt impute it anyways, so we would be in trouble imputation or not!"
  },
  {
    "objectID": "posts/2023-04-28-imputation-might-be-silly/index.html#scenario-3-what-about-inference",
    "href": "posts/2023-04-28-imputation-might-be-silly/index.html#scenario-3-what-about-inference",
    "title": "When is complete case analysis unbiased?",
    "section": "Scenario 3: What about inference?",
    "text": "Scenario 3: What about inference?\nThis is the same as above, but I found it helpful to frame as an inference question. What if I have some treatment, x that is randomly assigned, but is missing based on some factor c that is related to my outcome of interest, y.\n\nset.seed(1)\ndata &lt;- tibble(\n  x = rbinom(n, 1, 0.5), # randomly assigned exposure\n  c = rnorm(n), # problem variable\n  y = x + c + rnorm(n), # true treatment effect is 1\n  x_obs = ifelse(rbinom(n, 1, 1 / (1 + exp(-(x + c)))),\n                 NA, x) # missing based on x and c\n)\n\nFirst of all, in the absense of any missing data we don‚Äôt actually need to adjust for c in order to get an unbiased estimate for x because c is not a confounder (yay!).\n\nlm(y ~ x, data = data)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nCoefficients:\n(Intercept)            x  \n   0.000368     0.998409  \n\n\nBUT! With missing data, we do have a problem:\n\ndata_cc &lt;- na.omit(data)\nlm(y ~ x, data = data_cc)\n\n\nCall:\nlm(formula = y ~ x, data = data_cc)\n\nCoefficients:\n(Intercept)            x  \n     -0.413        0.825  \n\n\nOops! Now we are seeing an effect of 0.825 when the true effect of the exposure is 1 (because we did a bad thing and just deleted all the missing rows). In the past, this is where I would say and therefore you should do imputation! BUT instead, just adjust for c, that is correctly specify your outcome model and all will be well:\n\nlm(y ~ x + c, data = data_cc)\n\n\nCall:\nlm(formula = y ~ x + c, data = data_cc)\n\nCoefficients:\n(Intercept)            x            c  \n   -0.00149      1.00241      1.00161  \n\n\n\nWhat if there is treatment heterogeneity?\n\nset.seed(1)\ndata &lt;- tibble(\n  x = rbinom(n, 1, 0.5), # randomly assigned exposure\n  c = rnorm(n), # problem variable\n  y = x + c + x * c + rnorm(n), # true treatment effect varies by c\n  x_obs = ifelse(rbinom(n, 1, 1 / (1 + exp(-(x + c)))),\n                 NA, x) # missing based on x and c\n)\n\nComplete case analysis is fine as long as the model for the outcome is correctly specified:\n\ndata_cc &lt;- na.omit(data)\nlm(y ~ x + c + x*c, data = data_cc)\n\n\nCall:\nlm(formula = y ~ x + c + x * c, data = data_cc)\n\nCoefficients:\n(Intercept)            x            c          x:c  \n   -0.00122      1.00149      1.00228      0.99824  \n\n\n\n\nDoes the outcome model have to be perfectly specified?\nHere I have an additional factor c2 that just influences y:\n\nset.seed(1)\ndata &lt;- tibble(\n  x = rbinom(n, 1, 0.5), # randomly assigned exposure\n  c = rnorm(n), # problem variable\n  c2 = rnorm(n), # some other thing that only influences y\n  y = x + c + c2 + rnorm(n), # true treatment effect varies by c\n  x_obs = ifelse(rbinom(n, 1, 1 / (1 + exp(-(x + c)))),\n                 NA, x) # missing based on x and c\n)\n\nComplete case analysis is fine as long as the model for the outcome has the things that influence both the exposure and outcome (confounders) and the things that influence the missing data mechanism and outcome (in this case just c) if all we care about is the effect of x on y:\n\ndata_cc &lt;- na.omit(data)\nlm(y ~ x + c, data = data_cc)\n\n\nCall:\nlm(formula = y ~ x + c, data = data_cc)\n\nCoefficients:\n(Intercept)            x            c  \n  -0.000207     1.000115     0.999957"
  },
  {
    "objectID": "posts/2023-04-28-imputation-might-be-silly/index.html#scenario-4-what-if-the-outcome-has-some-missing-data",
    "href": "posts/2023-04-28-imputation-might-be-silly/index.html#scenario-4-what-if-the-outcome-has-some-missing-data",
    "title": "When is complete case analysis unbiased?",
    "section": "Scenario 4: What if the outcome has some missing data?",
    "text": "Scenario 4: What if the outcome has some missing data?\nOk this is where things are a bit trickier (and it is not uncommon to find yourself here, for example loss to follow-up!)\n\ndata &lt;- tibble(\n  x = rnorm(n),\n  c = rnorm(n),\n  y = x + c + rnorm(n),\n  x_obs = ifelse(\n    rbinom(n, 1, 1 / (1 + exp(-(x + c)))), # x is missing not at random (based on value of x and c)\n    NA,\n    x\n  ),\n  y_obs = ifelse(\n    rbinom(n, 1, 1 / (1 + exp(-(x + c)))), # y is missing (conditionally) at random\n    NA, \n    y\n  )\n)\n\nBased on both of these missingness patterns, we are down to ~66% of our data having at least one missing value. AND YET we get unbiased results when we do complete case analysis:\n\ndata_cc &lt;- na.omit(data)\nlm(y ~ x + c, data_cc)\n\n\nCall:\nlm(formula = y ~ x + c, data = data_cc)\n\nCoefficients:\n(Intercept)            x            c  \n    0.00441      1.00213      1.00114  \n\n\nSO when is it a problem? If you are missing y and y is missing not at random then you are in trouble (but, I will note that you would also be in trouble in this case if you wanted to do imputation, so I‚Äôm not sure this is really a case for anything other than yet another example of a case where statistics cannot save you from everything!). Let‚Äôs look at that:\n\ndata &lt;- tibble(\n  x = rnorm(n),\n  c = rnorm(n),\n  y = x + c + rnorm(n),\n  x_obs = ifelse(\n    rbinom(n, 1, 1 / (1 + exp(-(x + c)))), # x is missing not at random (based on value of x and c)\n    NA,\n    x\n  ),\n  y_obs = ifelse(\n    rbinom(n, 1, 1 / (1 + exp(-(x + c + y)))), # y is missing not at random\n    NA, \n    y\n  )\n)\n\nWomp womp, here we have a problem, when we do complete case analysis, even with the correctly specified model, we get the wrong answer:\n\ndata_cc &lt;- na.omit(data)\nlm(y ~ x + c, data_cc)\n\n\nCall:\nlm(formula = y ~ x + c, data = data_cc)\n\nCoefficients:\n(Intercept)            x            c  \n     -0.352        0.849        0.848  \n\n\nLet‚Äôs make another of those cool graphs like they did in that awesome paper explaining missing data in terms of causal inference. Now I am going to make y missing not at random, as opposed to x like above.\n\ndata &lt;- tibble(\n  x = seq(-3, 3, by = 0.1),\n  y = x + rnorm(61),\n  y_miss = ifelse(y &gt; 0, \"yes\", \"no\")\n)\n\ndata_cc &lt;- data[data$y_miss == \"no\", ]\n\nlibrary(geomtextpath)\n\nggplot(data, aes(x = x, y = y, color = y_miss)) +\n  geom_point() + \n  geom_textline(color = \"black\", \n                aes(x = x, y = predict(lm(y ~ x, data = data))), \n                label = \"truth\", hjust = 0.75) +\n  geom_textline(data = data_cc, \n                aes(x = x, y = predict(lm(y ~ x, data = data_cc))), \n                label = \"wrong\") + \n  geom_hline(yintercept = 0, lty = 2) + \n  scale_color_manual(values = c(\"cornflowerblue\", \"orange\")) +\n  geom_label(aes(x = -2, y = 3), label = \"missing data\") + \n  geom_label(aes(x = 2, y = -3), label = \"observed data\", color = \"cornflowerblue\") + \n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/2023-04-28-imputation-might-be-silly/index.html#standard-errors",
    "href": "posts/2023-04-28-imputation-might-be-silly/index.html#standard-errors",
    "title": "When is complete case analysis unbiased?",
    "section": "Standard errors",
    "text": "Standard errors\nThis post is about bias but I would be remiss not to mention the sacrifice in precision that complete case analyses make. It is true that complete case analysis is ‚Äúthrowing away‚Äù data, so the standard errors of these estimates will be larger than they would be had we observed the full data set. BUT these standard errors are out of the box ‚Äúcorrect‚Äù (which is not true if you do something like single imputation, for example!)"
  },
  {
    "objectID": "posts/2017-11-15-secret-sampling/index.html",
    "href": "posts/2017-11-15-secret-sampling/index.html",
    "title": "Secret Sampling",
    "section": "",
    "text": "‚ÄôTis the season for white elephant / Yankee swap / secret santa-ing! There are various rules for this, for our version:\nüè∑ each participant receives the name of someone else to purchase a gift for\nüéÅ gifts are exchanged at a party\nü§î the receiver is tasked with guessing who the gift-giver was!\nWe thought it‚Äôd be particularly fun to do it #rstats style."
  },
  {
    "objectID": "posts/2017-11-15-secret-sampling/index.html#assigning-partners",
    "href": "posts/2017-11-15-secret-sampling/index.html#assigning-partners",
    "title": "Secret Sampling",
    "section": "Assigning partners",
    "text": "Assigning partners\nWe had our interested classmates sign up on a Google Form,‚äï resulting in a Google Sheet with each participant‚Äôs name, email, and interests.\n\nlibrary(\"googlesheets\")\nlibrary(\"dplyr\")\n\n\ndf &lt;- gs_url(\"https://docs.google.com/spreadsheets/d/1NHbsjZVhvIxzuivXWQvDBst0oMN7ZcCjCbI8XJeXxJI/edit#gid=860103774\") %&gt;%\n  gs_read()\n\nWe began by making a tibble that has name in the first column and assigned a random partner in the second.\n\nset.seed(525)\ndat &lt;- tibble(\n  name = df$name,\n  partner = sample(df$name)\n)\n\n‚ÄúBut what if someone was assigned to be their own partner!‚Äù you may ask. ‚äï Have no fear! A while loop is here! We allow a while loop to iterate until every individual is assigned a partner (who isn‚Äôt them!).\n\nwhile (any(dat$name == dat$partner)) {\n  dat &lt;- tibble(\n    name = df$name,\n    partner = sample(df$name)\n  )\n}\n\nThis is likely not the most efficient way, but we only had 23 friends so it‚Äôs üëå ‚Äì if you get excited about efficiency and would like to share a quicker way to do it, please do! We ‚ù§Ô∏è to learn!\nEdit: Looks like our üôè was answered!\n\n\nWhat if you just did sample(names) and everyone was assigned to give a give to the next person in the vector?\n\n‚Äî Hadley Wickham (@hadleywickham) November 15, 2017\n\n\nThis could look something like:\n\ndat &lt;- df %&gt;%\n  select(name) %&gt;%\n  mutate(name = sample(name)) %&gt;%\n  mutate(partner = lag(name))\ndat$partner[1] &lt;- dat$name[nrow(dat)]\n\nOr like this! (Thanks to our #livefreeordichtomize partner in crime + emoji friend Romain üëØ)\n\n\nMaybe tail(.,-1)\n\n‚Äî Romain Fran√ßois (@romain_francois) November 15, 2017\n\n\nWe then do a join to pull the emails and interests back in.\n\ndf_name &lt;- df[ , c(\"name\", \"email\")]\ndf_partner &lt;- df[ , c(\"name\", \"interests\")]\ndat &lt;- dat %&gt;%\n  left_join(df_name, c(\"name\" = \"name\")) %&gt;%\n  left_join(df_partner, c(\"partner\" = \"name\"))"
  },
  {
    "objectID": "posts/2017-11-15-secret-sampling/index.html#sending-the-email",
    "href": "posts/2017-11-15-secret-sampling/index.html#sending-the-email",
    "title": "Secret Sampling",
    "section": "Sending the email",
    "text": "Sending the email\nKarthik and I had a blast at the rOpenSci unconf creating the üê¥ ponyexpress package - what a delightful chance to use it! We didn‚Äôt want the secret to be spoiled (i.e.¬†we didn‚Äôt want to know who would be buying us gifts!), so we wrote a script to send an email to our classmates.\n\n## devtools::install_github(\"ropenscilabs/ponyexpress\")\nlibrary(\"ponyexpress\")\n\n‚äï\n\nbody &lt;- \"Dear {name},\n\nYou have been assigned to surprise &lt;b&gt;{partner}&lt;/b&gt; with a happy gift! {partner} told us they have the following interests:\n\n&lt;b&gt;{interests}&lt;/b&gt;\n\nRemember, the price limit is $20. \n\n&lt;img src = 'https://media.giphy.com/media/zhPXoVIBMtnUs/giphy.gif'&gt; &lt;/img&gt;\n\nHappy white elephant-ing,\nSarah & Lucy\"\n\nour_template &lt;- glue::glue(glitter_template)\n\nparcel &lt;- parcel_create(dat,\n                        sender_name = \"Lucy\",\n                        sender_email = \"lucy@myemail.com\",\n                        subject = \"Secret Santa!\",\n                        template = our_template)\n\nparcel_preview(parcel)     \n\nparcel_send(parcel)\n\nThis will create an email like this:\n\nAnd that‚Äôs it! Happy gift-ing üéÅ!"
  },
  {
    "objectID": "posts/2020-07-02-so-you-want-to-learn-r/index.html",
    "href": "posts/2020-07-02-so-you-want-to-learn-r/index.html",
    "title": "So you want to learn R‚Ä¶",
    "section": "",
    "text": "There have been several twitter threads circulating with (free!) resources for learning R - I wanted to collect some of my favorites here. Thanks to Jessica Lavery, Danielle Navarro, Kelly Bodwin, Jon Harmon, Tyler Morgan-Wall, Rachael Tatman for their awesome threads on these topics!"
  },
  {
    "objectID": "posts/2020-07-02-so-you-want-to-learn-r/index.html#beginnner",
    "href": "posts/2020-07-02-so-you-want-to-learn-r/index.html#beginnner",
    "title": "So you want to learn R‚Ä¶",
    "section": "Beginnner",
    "text": "Beginnner\n1. Teacup Giraffes and Stats [link]\n\nWhat is it?\nThis is a series of fabulous self-paced interactive modules by Hasse Walum and Desir√©e De Leon. They begin by walking through the very basics of R and then teach basic statistics concepts such as measures of central tendency and variability, all with adorable illustrations of giraffes!\n\n\nThis is good for you if you\n\nLearn best with self-paced interactive modules\nAre a beginner interested in getting acquainted with the basics of R and statistics\nLove giraffes\n\n2. Swirl [link]\n\n\nWhat is it?\nThis is an R package that helps you learn R from within R. It‚Äôs an interactive course that runs from your R console!\n\n\nThis is good for you if you\n\nLearn best with self-paced learning-while-doing coding\nAre a beginner interested in getting acquainted with the basics of R\n\n 3. RYouWithMe [link]\n\n\nWhat is it?\nThis is a series of online learning resources for using R geared for beginners by Lisa Williams, Jen Richmond, and Danielle Navarro as part of R-Ladies Sydney.\n\n\nThis is good for you if you\n\nLearn best with self-paced modules\nAre interested in learning data science skills such as data wrangling and data visualization\n\n4. Adventures in R [link]\n\n\nWhat is it?\nThis is a college-level online course taught by Kelly Bodwin focusing on using R for Statistics and Data Science.\n\n\nThis is good for you if you\n\nLearn best with a structured of a full course\nHave some knowledge of basic statistics and basic programming\n\n5. R4DataScience learning community [link] [join]\n\n\nWhat is it?\nThis is a learning community started by Jessie Mostipak that intitially went through Garret Grolemund and Hadley Wickham‚Äôs R For Data Science book. The group has since expanded to broadly welcome folks interested in learning R and / or improving their skills! The community is centered around a Slack group (join here:) and you can join as a learner or a mentor.\n\n\nThis is good for you if you\n\nLearn best in a community\nAre looking for other folks interested in Data Science that use / are learning R\n\n6. Data Science: Foundations using R Specialization [link] \n\n\nWhat is it?\nThis is a Cousera course by the amazing JHU Data Science Lab team: Jeff Leek, Roger Peng, and Brian Caffo. This course will help you ask the right questions, manipulate data sets, and create visualizations to communicate results in R.\n\n\nThis is good for you if you\n\nLearn best with a structured full course\nAre interested in beefing up your data science skills\n\n7. RStudio Primers [link]\n\n\nWhat is it?\nThese are interactive tutorials that teach everything from basic R skills to iterating and writing functions\n\n\nThis is good for you if you\n\nLearn best from self-paced interactive modules\nAre interested in learning everything from the basics in R to more advanced skill\n\n8. Suggested by Giulio Centorame Ready for R [link]\n\n\nWhat is it?\nA course by Ted Laderas meant to be a gentle introduction to using R/Rstudio in your daily work.\n\n\nThis is good for you if you\n\nLearn best from a self-paced online course\nAre interested in learning the basics of R and RStudio"
  },
  {
    "objectID": "posts/2020-07-02-so-you-want-to-learn-r/index.html#a-little-past-beginner",
    "href": "posts/2020-07-02-so-you-want-to-learn-r/index.html#a-little-past-beginner",
    "title": "So you want to learn R‚Ä¶",
    "section": "A little past beginner",
    "text": "A little past beginner\n1. Creating dynamic dashboards with the shinydashboard R package [link]\n\nWhat is it?\nAn online course (by me!) on the shinydashboard package. This expects you know a bit about R and shiny, but aimed mostly at beginner / intermediate users.\n\n\nThis is good for you if you\n\nLearn best with self-paced modules\nAre looking to build a simple, dynamic dashboard in R.\n\n2. Text Mining with R: A Tidy Approach [link]\n\n\nWhat is it?\nThis Rachael Tatman described this as a ‚Äúcode-along‚Äù - I love that description! This book by Julia Silge and David Robinson is a great introduction to text mining the tidy way!\n\n\nThis is good for you if you\n\nLearn best working through a self-pased book\nAre interested in natural language processing\n\n3. MasterClass Featuring Tyler Morgan-Wall: 3D Mapping and Dataviz in R [link]\n\n\nWhat is it?\nA video-lecture by Tyler Morgan-Wall on making 3D maps in R with rayshader.\n\n\nThis is good for you if you\n\nLearn best from video lectures\nAre interested in 3D visualizations in R\n\n4. Suggested by Alireza Akhondi-asl Interactive web-based data visualization with R, plotly, and shiny [link]\n\n\nWhat is it?\nA book by Carson Sievert describing how to create interactive visualizations in R.\n\n\nThis is good for you if you\n\nLearn best working through a self-paced book\nAre interested in creating interactive visualizations\n\n5. Suggested by Lisa Lendway Getting Started with Tidymodels [link] \n\n\nWhat is it?\nAn set of modules by RStudio focused on the tidymodels package.\n\n\nThis is good for you if you\n\nLearn best working through self-paced coding modules\nAre interested in learning advanced statistical modeling in R\n\n6. Suggested by Lisa Lendway and Alex Hayes Tidy machine learning in R [link]\n\n\nWhat is it?\nA tutorial code-along by Rebecca Barter on machine learning focusing on the tidymodels package.\n\n\nThis is good for you if you\n\nLearn best working through self-paced code-alongs\nAre interested in learning advanced statistical modeling in R\n\nDid I miss one of your favorites? Please let me know!"
  },
  {
    "objectID": "posts/2020-04-08-ihme-model-uncertainty-a-quick-explainer/index.html",
    "href": "posts/2020-04-08-ihme-model-uncertainty-a-quick-explainer/index.html",
    "title": "IHME Model Uncertainty: A quick explainer",
    "section": "",
    "text": "There has been a lot of talk about the IHME Covid-19 projection model. Ellie Murray & I have a chat about it on Episode 10 of Casual Inference; here is a quick description of what is going on here with a focus on the uncertainty.\nWhen I look at models, I usually start with two things:\nr emo::ji(\"chart_with_upwards_trend\") What method is being used?\nr emo::ji(\"inbox_tray\") What data is it based on?\nLet‚Äôs start with the methods!\n\nMethods\nr tufte::margin_note(\"in particular it is using [a non-linear mixed effects model](https://ihmeuw-msca.github.io/CurveFit/methods/)\")\nr emo::ji(\"chart_with_upwards_trend\") The IHME model is estimating the log of the cumulative death rate for a given state at a given time\nr emo::ji(\"water_wave\") Using curve fitting\nr emo::ji(\"straight_ruler\") parametrized with info about the state‚Äôs social distancing\nSince the IHME model is trying to estimate a curve there are r emo::ji(\"victory_hand\") two important pieces:\n1Ô∏è‚É£ When will deaths ‚Äúpeak‚Äù\n2Ô∏è‚É£ How many deaths will there be at the ‚Äúpeak‚Äù\nTo estimate when these occur, the IHME model has two sources of info:\n‚è± the current death rate over time for the state\nr emo::ji(\"straight_ruler\") the social distancing measures being implemented\nThis information is combined with some r emo::ji(\"earth_asia\")global info as well\nr emo::ji(\"baby\") In the short run, the model is impacted more by the state‚Äôs data\nr emo::ji(\"older_man\") In the long run, they use info from locations that have seemingly already reached a peak: Wuhan, 5 in Italy, 2 in Spain\n\n\nUncertainty\nOKAY now that we know what the IHME model is doing, let‚Äôs get to the good stuff - where is the uncertainty?\n\nThere is uncertainty that the model itself will accurately predict what will happen (it‚Äôs based on a Gaussian error function - is that right?)\n\nThere is uncertainty in the distributional assumptions of the model\n\nEven if the model is correctly specified, there is uncertainty in the parameter estimation (this is a mixed effects model, so there is uncertainty associated with fixed and the random effects)\n\nThere may be systematic uncertainty in the reported state-by-state death data. Why? Fewer deaths may be reported on weekends, if systems are overrun, COVID-19 related deaths may go unreported, etc r tufte::margin_note(\"[NPR reports that NYC is seeing a spike of deaths at home](https://www.npr.org/sections/coronavirus-live-updates/2020/04/08/829506542/after-deaths-at-home-in-nyc-officials-plan-to-count-many-as-covid-19) that are not originally included in COVID-19 count\")\n\nThere may be random uncertainty in the reported state-by-state death data\nThere is uncertainty in the reported information coming from cities that seem to have already peaked\n\nSo let‚Äôs recap on the uncertainty in the IHME model:\n1Ô∏è‚É£ model choice\n2Ô∏è‚É£ model parameters\n3Ô∏è‚É£ model estimation\n4Ô∏è‚É£ data from the states (systematic)\n5Ô∏è‚É£ data from the states (random)\n6Ô∏è‚É£ data from the ‚Äúpeaked‚Äù locations\nIn the original model (pre-last week) the error bands you saw only accounted for 3Ô∏è‚É£, since then the model was updated so that the uncertainty also accounts for out-of-sample uncertainty, which I believe covers 5Ô∏è‚É£\nThe shaded red region in the model is the uncertainty the model accounts for, just two of the 6:\nr emo::ji(\"cross_mark\")1Ô∏è‚É£ model choice\nr emo::ji(\"cross_mark\")2Ô∏è‚É£ model parameters\nr emo::ji(\"white_heavy_check_mark\")3Ô∏è‚É£ model estimation\nr emo::ji(\"cross_mark\")4Ô∏è‚É£ data from the states (systematic)\nr emo::ji(\"white_heavy_check_mark\")5Ô∏è‚É£ data from the states (random)\nr emo::ji(\"cross_mark\")6Ô∏è‚É£ data from the ‚Äúpeaked‚Äù locations\n\nThis is not unusual or bad! It is just good to keep in mind the uncertainty that these projections carry with them. If all of the uncertainty we‚Äôve talked about today was quantified, it‚Äôs possible we‚Äôd basically have no answers to go off of r emo::ji(\"woman_shrugging\")\n\n\n\"You guys think I don't give you straight answers. You have to talk to these statisticians. They will not give you a direct answer on anything.\" ü§£ https://t.co/4AhCHYaDtz\n\n‚Äî Hilary Parker (@hspter) April 6, 2020\n\n\nThink I missed something important? Please let me know! r emo::ji(\"folded_hands\")"
  },
  {
    "objectID": "posts/2022-02-22-the-peril-of-power-when-prioritizing-a-point-estimate/index.html",
    "href": "posts/2022-02-22-the-peril-of-power-when-prioritizing-a-point-estimate/index.html",
    "title": "The Peril of Power when Prioritizing a Point Estimate",
    "section": "",
    "text": "I recently noticed that the Pfizer immunobridging trials, presumably set up to demonstrate that their COVID-19 vaccines elicit the same antibody response in children as was seen in 16-25 year olds, for whom efficacy has previously been demonstrated, have a strange criteria for ‚Äúsuccess‚Äù.\n\n\nGMT: geometric mean titer. This is a measure of the antibody titers. We use the geometric mean because this data is quite skewed (it is also why you typically see it plotted on the log scale). For those of you who ‚ù§Ô∏è math, the equation for the geometric mean is just \\(\\exp\\{\\frac{\\sum_{i=1}^n\\textrm{log}(x_i)}{n}\\}\\)\nThe primary endpoint of these trials is geomteric mean titer ratio, that is, the ratio between the geometric mean antibody concentration in the younger age groups compared to the 16-25 year olds‚Äô geometric mean antibody concentration.\nAccording to the recent write-up from the 5-11 trial in the New England Journal of Medicine, the trials have been set with two measures of success:\n\nThe lower bound of the GMT ratio must be \\(\\ge 0.67\\)\nThe point estimate of the GMT ratio must be \\(\\ge 1\\)\n\nThe second criteria was originally set by Pfizer to require that the point estimate of the GMT ratio must be \\(\\ge 0.8\\), however after their data lock the FDA requested this to be changed to the higher threshold. While at first glance, this may seem to make sense, after all we often want to make sure that we hold our pediatric trials to a high standard of efficacy, it turns out this change has statistical implications that change the target in ways that are non-standard for non-inferiority trials.\nWhat do I mean? If we believe that the distribution of antibody concentration in children is exactly the same as what we observed in 16-25 year olds, we would expect this second criteria to fail 50% of the time. Why? When doing any trial, we are observing a sample, not the whole population. We expect a certain amount of uncertainty in our estimates. Here is a small example. Below I have generated 10 samples of 250 people from a log normal distribution with a mean of 1142.5 (log mean of 7.04) and a log standard deviation of 0.8. I am comparing this to an observed sample of 253 individuals drawn from the exact same distribution (incidentally, there were 253 16-25 year olds that led to the ‚Äúbenchmark‚Äù of a geometric mean of 1142.5).\n\n\nWant to try it yourself? You can generate samples from a lognormal distribution in R like this rlnorm(253, mean = 7.040974, sd = 0.8). You can then compare the geometric mean ratio across several samples generated from the same distribution.\n\n\n\n\n\nNotice in the above plot that even though the true GMT ratio between these groups should be 1 (they were drawn from the exact same distribution!), when using the point estimate as the threshold for success, we ‚Äúfailed‚Äù 50% of the time. It was a coin toss whether this trial succeeded or failed by this criteria.\n\n\nThe standard error is just the {standard devation} divided by the square-root of the sample size, \\(n\\), \\(\\frac{sd}{\\sqrt{n}}\\)\nThis may not be completely intuitive at first glance, but in fact we can show that the probability of ‚Äúsucceeding‚Äù under this criteria is driven by how much better the antibody concentration is in children compared to the benchmark ‚Äì it is linked to the standard error.\n\n\nThis standard error multiplier is just the critical value derived from a standard normal distribution at a given quantile. You can calculate any success probability in R using the qnorm() function. For example, if we wanted to know the standard error multiplier we‚Äôd need to have at least a 70% chance of succeeding by this criteria, you would run qnorm(0.7), revealing we‚Äôd need to be at least \\(0.52 \\times \\textrm{standard errors}\\) better than the target.\nIn order to have a probability of success greater than 80%, for example, the childrens‚Äô antibody response would need to be at least \\(0.84 \\times\\) standard errors better than the 16-25 year olds‚Äô. To have a probability of success greater than 90%, the childrens‚Äô antibody response would need to be at least \\(1.28 \\times\\) standard errors better than the 16-25 year olds‚Äô.\nAs many have pointed out, it is not uncommon for pediatric trials to be held to a higher standard, often requiring efficacy beyond what is required of adults due to an appropriate caution against intervention in an often vulnerable group. I fully believe that the regulators that requested this had every best intention in doing so. In this particular case, however, this type of threshold can potentially lead to the opposite effect. By requiring the younger children to mount a higher antibody response than the older cohort in order to pass regulatory hurdles, we may be inadvertently pushing towards higher dosing, for example.\nIt is possible that the choice of 1 was made with the full understanding that the truth had to be better than 1 to pass that threshold with any amount of confidence, but it is important that everyone who contributes to decisions about thresholds understands both the math and the rationale for the choice.\nIs this why the 2-4 year old vaccine failed previously? It‚Äôs not totally clear since the data hasn‚Äôt been released, however based on the tid bits we‚Äôve gotten from media reports, I don‚Äôt think so. The New York Times reported, for example, that the 2-4 vaccine only elicited 60% of the response compared to the 16-25 year olds, suggesting that it would have failed by the lower bound criteria alone. So why does this matter? Presumably, these thresholds will be used to compare the post-3rd dose response to the 16-25 year olds as well ‚Äì does it make sense to require the under 5s to have a stronger antibody response than the 16-25 year olds? Especially with no other option for protection via a vaccine for this age group, I would say no."
  },
  {
    "objectID": "media/rmarkdown-libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "href": "media/rmarkdown-libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "title": "Location",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‚Äòold‚Äô relative url protocols and to ‚Äòupgrade‚Äô them at js runtime.\n\n\n\nNotes‚Ä¶\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‚Äò//‚Äô.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  }
]